Unknown Speaker  0:04  
Today is most important. Then I'm

Speaker 1  0:21  
Talking today is and then,

Unknown Speaker  0:33  
after,

Unknown Speaker  0:40  
of the course.

Speaker 1  0:45  
Okay, so basically define the idea of hypothesis testing, and then I use two examples. One example hypothesis was the tavern salesman met. The target was for middle income group. The target for the salesperson was point certified. We estimated point 73 and the question is, should it be filed? Okay? The question is given that some estimated, is it significantly different for the target? And that's the test we did. And luckily, we concluded that we don't need to file sales process. This was a tablet ownership. So this was another hypothesis that we started with, that prior to point 75 and we went through all this testing mechanism and concluded that we cannot reject the null hypothesis, that no hypothesis was that the target was made, and the basis for this decision was that we selected the level of significance, alpha of 5% and so we can also calculate the p value, the p value, and if the p value is greater than the acceptable level of significance, then we can, we cannot check the null hypothesis. So this was the first test. We also showed the confidence interval. So this was the null hypothesis. Was externally specified. It was not part of the data. We did not learn about it from the data. It was a target of point certified. And another hypothesis was a restriction by two equals to point certified for the second restriction, then you said, What is the null hypothesis consists on multiple instructions. So if it's a low composite, and so it's a it's a model with multiple instruction meaning, in our case, the example was, let's assume education. Our hypothesis behind the topic, example is an education. And the other thing hypothesis was education has no effect. So we call it now this hypothesis. We call it the online hypothesis. So it's age zero, is a model which now says our general model in one model, impostor models, but multiple restrictions. In the first example, there was one restriction, pi two equal to point 05, now the example was pi one equals pi two equal to i sigma. So it's multiple restriction, and for the single restriction. The convenient test was, we call it t test, although using a normal distribution, so it's often a photos and Z test, t test. Z test was a single restriction, and basically we had the critical value, etc, the mechanism last time, for a multiple restriction convenient test, and the test of desirable poverty is a likelihood ratio test. And the likelihood ratio means a log likelihood difference. So it takes a difference between log likelihood of two models. The model under each one. We call it the restricted model. It's like the model that is created by imposing the restriction and a new is the likelihood of the data in the model for a situation, the general situation under each one, no restriction is imposed. So that's the test. In this case, we are making two restrictions, pi, one equal to pi, two by one equal to pi, three equal to equality, signs two restrictions and this test statistic in the single hypothesis case under normality, approximation, assuming we have a test statistic which is normally distributed, and we calculate the probability, the p value, alpha, using the normal assumption. In this case, this test statistic, it relates to the normality under the normality assumption, this is chi square distributed. So how do you get chi square? You take your square normal, normally distributed variable. There is a map you get the chi square and and the chi square has degrees of freedom. I showed you, we passed by the growth of distribution for different values of m, and so we do a cash flow test, and excuse me, and the conclusion was that we can. We cannot accept the null hypothesis. So if we reject it, the p value is 10 to the L to the minus five. So it's very small. And so we can sort of inject the hypothesis. So and then part I can cover is this. So, so far, the two examples, the hypothesis was externally specified. It came from a marketing target or from an alternative hypothesis. Education doesn't matter and and this led to what we call the null hypothesis. And what we have done, we have looked only so far in type 1l and we selected the alpha, which was the level of significance. And the question is, would be always like alpha to be as small as possible. Why not make it practically zero? And the answer is that we cannot do that, because if you make alpha too small, then there's another L, type 2l so what is alpha? Alpha was we often write it like this, which is kind of not, not really correct, because if we say type 1l probability alpha, if you say alpha is a probability of a Type 1l is a mistake. It's a three question that we're going to ask you. Why is it a mistake? Why is this what sort of mistake? I mean the rigorous language, type 1l probability alpha. Is alpha, the probability of a type one error, yes or no. Who thinks it Yes, three questions,

Unknown Speaker  8:52  
nobody, who thinks it's no

Speaker 1  8:58  
why? Because it is, if you remember, I emphasize it, and when we calculate this alpha, I said this alpha is calculated under this distribution, and this distribution is a distribution of the test statistic under the null hypothesis, meaning that the correct language here is correct language. Maybe that alpha is a conditional probability. It's not the probability. It's a type one. Type One is the type 1l is a type 1l type 1l means objective two. Type 2l is acceptable, false. So that's the definition of type 1l type Well, the question was, what is alpha? So alpha is not the probability of a Type 1l it's a conditional probability of a Type 1l

Unknown Speaker  10:08  
it's a probability of a Type 1l if hypothesis is two, condition on The line of this being two. So when we,

Speaker 1  10:23  
when we draw this distribution and calculate alpha, it's an area. It's like the critical value the left here. Therefore the one time test for the test that we did last time. This alpha is a conditional probability. So if you ask yourself, then what is the probability of a Type 1l and also, we'll have the same issue with beta. Beta, the probability of the type 2l is a conditional probability of a Type 2l type two error, meaning acceptance, false. Don't think about that, but, but it's the probability. Beta is the probability of a Type 2l, given that h1 is two. Okay, so, so, how do we I mean, so, what is the probability of a Type 1l but you know that in order to get if you have a conditional probability, you want to get the probability, you have to multiply it by the marginal probability of the conditioning event. So you have a probability of a Type 1l given H zero is two, then the probability of that one will be this conditional probability, which we call alpha times the marginal probability of H zero being two. What is it? You don't specify, but that's why you need to think we need a distance. And when you do this test, and you ask yourself, should I have alpha small? Is alpha bigger? Or we need to think about this marginal probability? What's the marginal probability of Hmong of H zero being two, is our upper yielding belief in H zero? Do we think that? Do we have a strong belief in H zero? And then, in order to So, we need to think about that. What is our up on your belief that H zero is two, and then what is the cost of committing type one zero? So, so if you, if you want to calculate that, what's the expected cost of making a decision here, you have to think, okay, in the case of the marketing director, time on your own, is we are firing, we are going to fire a good Marketing Director. And if there are very few marketing director, good marketing director, mistake so, but often we don't know what's the cost of type 1l. Was the cost of type two, error. But we have that. We usually have some a priori notion about unlikely is H zero to be two. So if it's very likely, then you multiply it by alpha, then you will have to be small. If it is unlikely, then the margin probability of H zero being two is small, then maybe we should pay attention to type 2l so that's what we're going to calculate. Now the problem is, it's difficult to calculate type 2l because type 1l we say this distribution under H zero, H zero is specific. It says by two equal point 75 or it has an equality sign in it. The the h1 is often an inequality. And if it is an inequality, what should it be? So if that if h1 move to the right, then beta will be small, and to the left, then beta becomes small. As we move this to the right, beta becomes very large. So that's why mechanism of hypothesizing is focused on, on the null hypothesis, but it's important to think about, how much should we worry about beta? And so that's for our example. By the way, this often, instead of calling it beta, which is the conditional probability of a Type Two, error, given that h1 is two, we refer to a power. So one minus beta is called power and and this is usually difficult to determine, as I said, because h1 same, just as you call it beta of power, to say just one minus the other and then so it's usually different than the term in beta power because they find it the composite hypothesis for given sample size and angle between alpha and beta, meaning, Usually we have this fact that if we this pattern, that if we reduce, I mean distribute the picture, right, if we reduce alpha, we will increase beta for a given h1 so there is a trade off between alpha and beta. So that's why we don't want to have alpha to be too small, and especially if if the a priori probability of alpha is not very loud, so if the apoyo probability of alpha is high, like it's a no hypothesis, which we believe is two, then we want alpha to be as small as possible. But if alpha is, I mean, I was talking to my colleague, and he said, Well, why don't you try modern Well, education has no effect, but I'm not I don't feel very strongly that education has no effect. The opposite is true. I feel very strongly that education does have an effect. Then my alpha for the null hypothesis is very small, and my boy. So anyhow, let's so for giving sample sizes and data between alpha data. So we don't want to, often we don't want to induce alpha, but sometimes we may want to reduce alpha. Why? Because we want to. By reducing alpha, we reduce beta. So we often want to increase alpha, because by increasing alpha, we reduce beta. To reduce both type one and type two, these two conditional probabilities, and we just need another sample size. Okay. So next is, let's calculate beta in our example. So the so this was an example right time equals point 75 and calculated the Delta, and we made the null hypothesis, pi two equal to point 75 the alternate hypothesis is that pi two is less than point 75 where the test statistics And the test statistic was this ratio, and it's like a z test. And for probability of point five, the critical value is 1.65 we get the value which is less than that. And that's what we did. Now, in order to calculate beta, we need to make an assumption about the response. So So let's make an assumption that h1 is 2% below the target. What does it mean? It means that, later on, we talk about sampling. I refer to it as effect size. Effect size is a difference is it is a different form. From the null hypothesis that we want to make sure to detect. So we said, Okay, we want to calculate what's the probability of committing a type two error. If we are 2% below point 75 and so the alternate hypothesis is that the pi is equal to point 73 so as we calculate, we just calculate this, this area, right? So we move, we position this distribution same variance. It will be, it will look the same as this distribution just shifted. But here it became. It changed variance a little bit, but so basically we shift, take this distribution, shift it to 2% below So the null hypothesis is a delta. The difference is zero, meaning point 35 the alternate hypothesis is that, the fact is that the knowledge, the the h1 is point 33 so if h1 is point 73 and we make the decision that we made, we make the decision here, then we position the

Speaker 1  20:13  
distribution of the statistic under h1 of our deal. We shifted it 2% to the left, and our critical value remain as it was before, and we now can calculate beta, and that's the formula to calculate beta, and it's point eight, two. So before we conclude this without the name, nobody argued about this that we had a 5% Alpha was 5% and we

Speaker 1  20:50  
decided that hypothesis that the target was made. Now you can see that if under the probability of

Speaker 1  21:07  
saying it correctly after I talked about it. So if the effect of the alternate hypothesis is 2% less meaning, I want to detect if the target was not made by at least 2% and the probability of doing that is 82%

Speaker 1  21:35  
if indeed the salesperson was 2% under target. And and, and that's the probability of accepting when it's 2% or under target is 83% so maybe, maybe we selected the alpha which was too small. Maybe we should select it. We selected the big if this beta is too high, or we need the round for sample to reduce beta. So that's that's beta now, so you notice that we can calculate beta for given an assumption on on the distribution under h1 and this difference, this shift, is called effect size. And so when you calculate where they say, What effect do I want to able to detect and and so in this example, we calculated it for 2% but we couldn't calculate it for 1% or 6% it depends on the application. What is relevant to the employer, what kind of what difference makes a difference as far as hiring or filing, as far as meeting the target, or some targets. Okay, so let me just say that one more concept that is often used in this case is a power flow. So if you calculate beta, we can calculate power which is by minus beta, which is point dt, and the power of beta depends on this effect size. We calculated it for 2% suppose that they change. They change it from 1% to 2% to 6% I get the kill. I get the kill. That shows how the power changes, right? So the larger, there's this shift effect size, the greater the power So, so we are we can so the power kill tells us, you know, what kind of effects can be defined detect, and what kind of probability. So that's, that's idea of the power kill, yes.

Speaker 2  24:00  
So even though the power of the test is very small, if the pH one is very big, is it okay, our prior conception of h1 being true? Could that be the case? Like, even though power is very low, like, exactly

Unknown Speaker  24:18  
so, if new point,

Unknown Speaker  24:29  
because, if you say, Okay, so the other Reasons is two or 4 million probability of h1 is so

Speaker 1  24:52  
the probability of h1 is one minus zero, right? So the probability of committing an L is the probability of H zero plus one minus this absolute probability times beta, that's the probability of making an L, whether it is L, type one and type two and L, any L, that would be the probability So, so yeah, the probability of committing a type 2l is a one minus the probability of HCl being two times better. So if the angular probability of type 1l, of HQ is very high. So

Speaker 1  25:45  
so if the absolute probability of H zero is high, meaning that the absolute probability of factors of h1 is small, so we can then the large beta is not the problem for us. But usually, usually, a lot of these psychologist tests are done for knowing positive learners. That's a mistake that often is made when you just make this test as, kind of, the algorithm, kind of, I mean, just every t has to be like the two. That's a mistake, because for if the upper unit probability is low, then using such a low alpha is a good idea. So this the fact that the choice of alpha dependent this A Priori probability tells you that it cannot be used as an algorithm. You have to use judgment, but in other words, just do the test when there is a strong reason to make the null hypothesis. If a priori, you know that the non hypothesis is wrong. I know education. Is it possible that education is unlikely, but that's not the good case. The good case is price effects. We know that price has an effect, so to make a test of whether price is zero and no effect is millions, these tests are millions because the upon your probability of the no hypothesis is zero. Okay, anyway, so, so that's basically what this is said. We illustrated t test C, test value ratio, test for given sample size of the trade off between alpha and beta and using small alpha, reasonable for any more concern with this type 1l final probability of non hypothesis is close to one and constant committing type one high. That's the scenario in which you want to go test the small alpha classical. So this is a classical scenario. So you ask yourself, Am I in the classical hypothesis in scenario? Meaning, do I have a high probability that the null hypothesis is close to one, hypothesis is two is close to one, and the constant permitting time on L is okay? So that's an appendix, so any question is going slightly fine, so the step one

Unknown Speaker  28:55  
arrow is conditional, conditional

Speaker 1  29:04  
conditional probability of making a type 1l given that the knowledge COVID received two is alpha.

Unknown Speaker  29:12  
Okay, so there's also another

Unknown Speaker  29:15  
radical, the d value. The d value is the probability of a Type, 1l,

Speaker 1  29:22  
yeah, the same. Let me just show you the difference picture. So if that's a distribution, and I selected the critical value, it was based on alpha. But now, if I have the statistic, if I have the test statistic, it has some value along this axis. And if I calculate this area from the test statistic, that's a p value. So it's a probability of a time point error. If I set the critical region at the value that I of the test statistic. Okay, so on this axis, I have this, the Delta head divided by the standard. And all these are the value that we use to define the critical value. I have an estimated value that they compare to the critical value, it will be somewhere to the right or to the left, somewhere and wherever it is. If I calculate an area like alpha, that's a p value, that's it. So by comparing P value to a significance level alpha. Then if it is, if the p value is greater, it means that to the right of the critical p value is small. So if p value is small, then we can accept if p value is large, and we can reject that. I'm sorry if p value is more. Religion is large you can accept. So

Speaker 1  31:15  
it's the same, it's the same, it's just the p value. The alpha value is what we use to calculate the critical value, the critical region, and the p value is just whatever. So you don't calculate the critical region, you just calculate the p value, and then by comparing it against the level of significance that you want, you can reject or accept without defining a critical value. So as I said, if the p value is greater than alpha, then then you are building right and you can accept

Speaker 1  31:58  
alpha is an upper yo we specify acceptable.

Speaker 3  32:07  
Yes, that's if we present it as a specific value. So how do we replace this when we increase?

Speaker 1  32:12  
Oh, I mean, so if you're testing hypothesis that has low upper yield probability and maybe low cost, you may use an alpha equal to point two. I mean, there's nothing religious about alpha being 2.05 it's just customary. But there's no I mean, if you want to calculate alpha precisely, then then you have to write, then you have to know the cost of committing type 1l, the cost of committing type 2l write the objective function right and calculate for given sample size. And you need to calculate the value of the optimal value of alpha to minimize the total cost. Every value of alpha describes also a value of beta, and so you can select the optimal value. But this is a problem that we never solve because it's very hard to quantify the probability of a Type, 1l so how to quantify the probability and the cost and but, yeah, it is a simple optimization problem, except it's impossible to Do it because we don't have information.

Speaker 1  33:42  
Again, I'm going question about hypothesis testing? Okay, testing as if it is an algorithm, just when you have an alpha, you only reason to make the null hypothesis like two examples that they gave you, the null hypothesis loaded algorithm like testing every PI or every just because it's statistically tested against zero. Now that's not too okay, do

Speaker 1  34:32  
Okay, now the last part of this review is what's happening, and so I so I talk about some basic concepts, then I will introduce just notation. I'm not going to review again, statistical influence on essentially this part is an introduce a notation that I'm going to use to number. Angie is to show you some elementary program for sample size determination. How do you determine sample size? So until now, you might example, we had sample size is 600 we're going to relax this today, but, and the question is, what should be the sample size? So that's a big question. What should be the sample size? How could it help in sample size? But I'm going to show you some so you have examples. And study by random sampling and concluding practical consideration. So again, now we assume that there exists a random sample of decisions. Again, people make a decision, and this idea of a random sample is really the case. Usually see some weights like summary data weights, because the sample rates are different. And so this basic concept of summary definition, what is a random sample, talk about other types of sample and talk about sampling size determination. So why are we sampling? If you can have a large, big data, data set, the transaction. But we often wish to estimate the listing of a large population, and census meaning those list just examples a census to measure the characteristic of a population. This looks expensive, not physical. Expensive, not feasible, but it's also unnecessary, because we can make influence, for example, and that's what people do. So sample saves money and save time and reduce the problems of conducting a survey, require administration and intuition. People nowadays refuse to participate in surveys a lot. So what are the definitions of sampling terms? The language of sampling population, we define the population already and to all entities for which we wish to estimate. The non statistic population element is obvious, sampling units sometimes and the population elements and the sampling unit are different. So for example, if I instead of summary individuals, so we have all possible, it's sending me sample buildings and interview everybody in the building as an example. So that's going to be a sample unit. And actually there are alternative methods of collecting data, collecting samples that are working stage. So you sample an area, sample addresses, etc, and then once you identify cluster, then you try to collect data from everyone to minimize cost. This idea of a sampling unit, sampling frame when you draw a sample, and the question is, how do you draw the sample? So the image of a sample does not exist, is a list. So it's a list of all the sampling units. So if we used to have telephone books, it's a list of people, so list of telephone numbers. And so if you have a list all sorts of information, so it doesn't have to be actually. So, for example, if you are trying to serve your people in the mall, and and you stand at the entrance to the mall, and so the sampling frame of the people coming into entering the mall, and then you stop every 10th person and try to interview them, they will be there, and then the sampling frame would be the stream of customers coming into the morning. Sampling strategy is a word of selecting sampling units from the sampling frame, so I just described one such strategy, which was,

Unknown Speaker  40:05  
interview every text, but I can, I can have a different strategy. I call according to time, wait every 10 minutes, stop somebody or something. So these are the

Speaker 1  40:31  
terms that are used to talk about something. So we said that sampling means an error because we don't have everybody census, if you have a sense that we don't health. So the L due to sampling is sampling L, and everything we've done so far when we talked about the fact that the sampling distribution is representation of sampling L, meaning that every time we sample the distribution. So so far, we only talked about this kind of in an estimate of population characteristic which is based on the sum of our census. So everything we calculated until now, all the standard variances the vulnerability when you go on distribution, these are liability due to something and this, they all disappear when the samples become bigger, bigger and bigger. But if you conduct the survey, you have to worry a lot about non response bias and response bias. So non response bias will help you do inability touristic information from some responses in the sample, often caused by diffuser so reach or decline and why is this a bias? It's a bias if the population that does not respond is different from the population that does respond in terms of the variables that you are trying the distribution of the variable that you are trying to estimate. So so for example, if you are in transportation, if you are some households or individuals, and some individuals that travel a lot difficult to reach, then the non response is, is a function of what you're trying to estimate, which is the degree of response formation. So if you're connected with a non respondent, how people are talking a lot that there is a non response bias because we are going to underestimate demand for transportation. Response bias is distortion, meaning respondent does not provide proof of information and all kinds of situations, all kinds of theories about how people distort the responses to the survey. They respond in a way that in transportation, they said that you use public transportation when they never use it because they think it's a good idea to use a procedure seeking and post purchase certification. Post purchase certification is like people try to justify what they've done before. So if I've done something, I will report that I'm doing it over and over again, even if I'm not, because I'm trying to justify what they've done exactly. But then, yeah, we'll talk more about these kind of situations later in the semester, when we talk about specific kind of surveys, which are called stated preferences surveys, when we are interested in people who sponsor big people preferences, and not trying to estimate the preferences for different products or services. So so all the analytics we talked about are designed to deal with sampling at all. But this doesn't mean that you can ignore this. They have to especially normal sports and few words to say about this later on the last section of this lecture. So the summary process, you know, define a corporation, identify a sampling frame, and select the sampling strategy and determine sample size and then collect the data. So the rest of this lecture, we focus next, focus on three and four. So I talk about different sampling strategies, and then talk about determination of sampling studies. So in terms of sampling strategy, first, the distinction between probability sampling and non probability sampling, that's very important distinction, because only statistical influence method that I talked about require a probability sample. What's the probability sample is any sampling method in which the chance of any population element inclusion in the sample is known and titles and field. So if you have a population, you're trying to make influence about the population, then the sampling method that can be used to make influence require that it will be a probability sampling, meaning different elements are selected into the sample with different probability, we need to know what it is, and it has to be there cannot be elements of the population in which the probability of inclusion is new, then You're not making inference about the population, maybe about some sub population. So, so why? Why are we doing other than probability something? Why do you use non probability sampling, but you may use it for contingent sometimes or judgment, instead of you saying, I cannot. I don't have the budget to go out and collect a probability sample from Cloud operation. I need some judgment and say, you know, the prototype elements of the population that they want to have in my sample, and that's method sampling, convenience. And, you know, I'm interested in English, but I'm dispute with MIT, so I just separate MIT. So that's, that's an example. So very often, for host reasons, convenience, using no probability, something can be done. Yeah. So the advantage of COVID sampling, I already said, it can be used to get statistically valid estimate the population using we talked about that. What we just talked about statistical influence, and about calculation of magnitude or something, or sampling of sampling distribution. Okay. So what are types of probability sampling, sampling strategies, which are probability sampling? So the first one is simple random sampling, often referred to as random sampling. All of these have some randomness in it. When we talk about the random sample, which means, we mean that each element has equal chance of being chosen. This means the sample which you don't need weights. So actually, that's what I can tell, the end sample, the samples, in sample, I said it's random sample, and you didn't use any weights, systematic random sampling is also random sampling, but like the example that I've been selecting every tense visitor to a mall, it's a random sample. Every person has an equal chance of being included, stratified random sampling. We'll talk more about that later. Is we define the population. The population is stratified into mutually exclusive groups. So the groups, every element of the population belongs to one and only one group. So exhaustive and exclusive kind of certification and and then a simple random sample is then chosen independently for each group. A group is a startup, so the example does a calculation of certified random sampling in a few minutes. And for the tablet example, and I define startups, and the starting can be defined by geographical area, can be defined by income group, or defined by on any kind of consideration. So that's ideal. And but from once you define the startup, then you sample from each other, but maybe the different types more so usually, I mean, I'll show you a formula for them. Cluster sampling is sometimes I mentioned convenient to define the cluster and sample everybody in the cluster

Speaker 1  50:26  
like a group, and everybody's in a sample, so I sample groups and then include everybody in the group. Multitude stage sampling, briefly talked about it, and consists of several sampling methods, sequentially, the select group of sample units. So these we begin, say, for the US, sample states, sample counties, and then sample local areas and then collect the random sample in the selected local area. Study. This sequential sampling is different. Sequential sampling, I mean, the multi stage sampling. It's like this hierarchy of going, getting to the population. In the way we do the sampling, sequential sampling is, you say, let me first get a small sample, maybe a lot of sampling. So this emission, small sample is taken in our eyes based on the analysis of this initial sample, then I design about the subsequent sample, and because, as you see later on, the sampling decision about sampling, what's the best way to spread the sample may depend on assumptions about the population. I mean, if you think about the cluster, certified sampling, the whole idea of certified sampling is to define the population into books which are relatively homogeneous, because if you have a group that everybody is the same and you only need one observation, the more vulnerability is in the sample, the larger the sample is a little quiet. So this is why this sequential sampling is often recommended, but this often

Unknown Speaker  52:21  
doesn't work. And doesn't work. Why? Right? I mean, it looks like an obvious thing to do.

Speaker 1  52:32  
I want to make a decision about what software to collect. The decision depends on my ability within a goal or all kind of information that they need about the population in order to design an optimal sample. So the idea of, how do I optimize the sample? I can just make assumptions about the population, then optimize a negative or I may say, let me select the initial small sample and then optimize. And I said, this often doesn't work because of the work small. What do you know about Small? Small sample? Big L, small, big L. So, so if you optimize on the basis of a small sample, and again, as the same order that making assumptions. So this is why. But if you are doing a huge volume, and the initial small sample is not very small, if it's based, this idea does work anyhow. So this is sequential sampling. Okay, now I come with the basic concepts. I think it's now I'm going to do a quick go through some statistical influence notation that I'm going to use for sample size. So we talked about statistical influence, they're making inference about the population characteristic. And so we want to estimate tests and do hypothesis about varieties of population characteristics. So let's, let's use this notation, u protein, so that's the average in the population. Sigma square would be the variance. So these are a characteristic, just the one characteristic of the population, keeping it simple, and the mean of that characteristic is mu and the variance the distribution of that characteristic in the population is sigma squared, and sigma is standard deviation. And now we take a sample. Let's assume random sample, and suppose that we have n s observation on this one variable, x, so we have this was our observations, and we calculate an average. The average is x bar, and we calculated it like that. And this is an estimator of mu, right? So x bar is an estimator of mu, so we make inference based on x bar on the value of mu. The sample variance is a definition of the sample variance you've seen this in Formula before. This s squared is an estimate of sigma squared, and the reason we do your minus one is to get this estimator to be unbiased. This is like some lesson in statistics that you've said before. You have a question, just assuming that all the populations are normally distributed. No, so far, I made no assumption, no assumption about normality. This is just the definition of variance. This is some variance. And so this is an estimate of sigma squared, no matter, because I did not make a normality assumption here. So the distribution of the characteristic in the population, I just said it has a mean u and it has a variance, sigma squared. That's all. Didn't say anything about the distribution. And I calculate an average on the sample, and I calculate this sample variance from the sample, and it's our estimator up, excuse me. So this x bar would be an estimate of mu and S square would be an estimate of sigma square. And the reason it is divided by NS minus one is because this leads to an unbiased estimate of sigma square. If you if you didn't put the minus one here, then S squared will be a consistent estimator of sigma square, but not unbiased, and it just the samples under deviations. So this also you would calculate for the data the single sample on the single characteristic. So now we're talking about estimator. So x bar is an estimator and S squared is an estimator. And so the question is, how does what's the sampling distribution of x bar, right? So, what does the sampling distribution of x bar depends on? It depends on the probability of X. It depends

Speaker 1  58:15  
on the variability of X in the population. It depends on sigma squared, and it depends on the sample size. And so if the sample is independent, and then the standard of x bar is given by this formula, right? So if you think about what is the distribution from theoretical you can send the sum in this condition of x bar has a mean which is equal to mu unbiased, and it has a spread which is equal by this given by this formula. So how do we calculate this spread? How do we calculate the standard level of x bar? We calculate S squared from the data, calculate S and divide it by the square root of n, s. So this is a standard error of the average. So we know in this case, summing the solution, X, bar and mean u and variance sigma squared estimated by s, I'm sorry, as estimated by that, as the NS becomes large, the summing the solution of x bar approaches normal distribution, with mean u and variance equal to sigma squared divided by ns. So, so that's a that's the center limit theory, asymptotic normality. So for round enough sample, we can assume it is necessary is normal, and that's the first time that nobody's coming into this, the only assumptions we make is that the sample is independent and it's independent of the form of the distribution of x in the population. And we also can use a central limit theory, asymptotic normality to now define confidence. So we've been told this before, if, if they based on the central limit theory, we can say that x bar is distributed. I put the node here to be asymptotic. I forgot asymptotic, no deal. And for this should be scripted two ends, this end and this end. And are no different. I apologize. So this is the N for the should be scripted for normal and this is NS for the sample side. So we have n, and n is sample size of this normal distribution. And so central limit theorem as x bar, it's a summary distribution of x bar is given. Is normal. It should be adult yield, and we can standardize it by dividing x bar minus mu divided by cn s, and this is normal zero. So this is normal 01 because the standardized number one and we use it to calculate confidence interval to do tests. So for standardized no one, we can calculate the C values. And that's how we created this confidence interval. This is a two tailed confidence interval. Alpha is significant, minus alpha confidence and this value you've seen before 195 six for 95% confidence, meaning that the area under the distribution is one minus alpha is 95 and for one minus alpha equal to 90 then the z value is plus or minus 1.65 we are going to use this variation. So am I going to use it for sample size determination? So I pretend that I use this case of just calculating the average, so that's why I introduced it. So what was happening with the term sample size, given a sampling strategy? And so usually, the most important factor is the cost of something and available budget. That's usually budget available for daily collection, and that's usually the main factor.

Speaker 1  1:03:16  
Another consideration is number of groups, some books in the sample that need to be underlined market segments. So if I want to sample it, I need an estimate for every insulity. Then I have many groups, so I need to have many estimates, the variability within the population, the required accuracy or effect size, the terminal sample size, and the selective level of confidence and power and analysis of this and the previous two factors in the following slides. So I'm going to look how these things affect sample size of excellent examples. You know, some samples that lead to simple formula. In practice, often these kind of formulas cannot be used. So using another error confidence, so I have one, there's one characteristic, everything, single variable, and I'm going to estimate x bar, right? So x bar is my estimate. But they want to estimate x bar with confidence interval of size d plus or minus d, and I want this the design confidence to be one minus alpha. So how can obtain an estimate of x bar within d units of the total value mu with one minus alpha confidence, meaning I want to have a confidence interval that is plus or minus D. So the question is, what should be the sample size? So d, we know is like c times the standard L, right and Z divides on alpha, and sigma divided by school, then is the standard L. And so if we know sigma, we don't specify alpha. We know sigma, we specify D. You sort it for N and S. So to solve this equation, we get the required sample size is equal to this expression. So we specified alpha, so we know z, we specify D, so we know d square. But how do we know sigma so in order to determine what sample size we require, or given confidence interval, we need to know sigma square. The Greater sigma square, the greater the sample size, the more as a viability in the population is increasing the amount of the sample size required to achieve a given level of faculty. So d represents the level of faculty that you require and the sample size. So we need some estimates of sigma as well. What do we get? Remember, sequential summary, or maybe for produce something or make an assumption? Oh, suppose, suppose D is specified as a percentage of algo L. So instead of saying I want the confidence interval which is plus or minus x d units of x, suppose that they say, I want plus or minus 10% for x bar to be estimated by the continents interval, plus or minus 10% so plus or minus So, if D is a percentage, say, 10% times mu, the mean should be equal to this the size of the interval. So you see that sigma over square of the fineness. So same idea as before. We have an equation d is given, z is given. We solve for ns, but it depends on sigma and mu, so we get it. We get the solution, but the solution depends on sigma and mu. So before, it just depended on sigma. Now it depends on sigma and mu, but it depends on the ratio of sigma over mu, and this is known as coefficient of variation. And so when you sample some characteristic over and over again in different places, you develop knowledge about COVID. And so it's often used to it's a unit free measure, because within the measure x in kilos or pounds or feet, the COVID variation is unit free. So so it's easier to remember, so when you look to compare different locations, and so you know confusion, variation is jewel and long distribution, if it's less than one, maybe it's not so for any given segment in transportation, for example, this solution is very important in transportation is a dissolution of the willingness to pay for time savings. The transportation is about time saving, and so knowing what the value of time is very important. So it's distributed for all of us. We are depending from day to day value of person to person. It has a great deal of variability. So to say, Okay, what? What can I say about the distribution of value of time? Often we talk about this in terms of coefficient of variation. So there's a lot of vulnerability. This is two that means a lot of vulnerability of willingness to pay, okay, so uniquely easier to approximate. That's why this formula is easier to apply, because it's easier to guess apoyu, what this is, as opposed to guessing. Often we estimate propulsion. So this will bring us to our tablets. We estimated the propulsion tablet on site, so the characteristic was one or zero. Those one variable, two variables, also education. But if you forget education for a moment, you ask a person you own a tablet or not. So one, if yes, zero, if not, and then we calculated this pi, which was an average, average of this x, it's a proportion, but because this x is a dummy variable, 01, then the average is the shell, right? So itself, x bar, we now have pi hat, which is given by this expression. And the variance in the population, the viability in the population we have for each one, that is a probability. If you have a population with the same PI, the same probability, then the variance is equal to pi times one minus pi, and therefore the standard error is equal to this expression. So for a particular situation in which we estimate the probability, say, of time and ownership, we use this formula initially. And so this is based on knowing that this is a Bernoulli random variable, and we're calculating an average and no weights. And so we know the formula, and so for that situation, which is situation? So we want to determine sample size, and the D, the L will be shell points, meaning point 02, means point 72, three plus or minus point 02, so this an interval. It's a SharePoint that will be the PI between zero and one. So SharePoint terminology. But anyhow, so what is d is equal to z times the standard L. That's the standard L by N. So we solving for ns, you get this expression. So this expression, and this is, by the way, often used expression z, we know the Specify share points, let's say, confidence interval that we want to get, and it depends on pi, so we need to collecting the data to estimate pi. But so, but we need to assume what that pi will be. That's very easy, because we can the variability of Ns pi is very clear. If pi is equal

Speaker 1  1:13:09  
to zero, sample size is zero. If pi is equal to one, sample size is one, is what zero? Because we know. So what's the worst situation with pi equal to point five goes up and down with Pi is like a parabola, and so, so, for example, so, so we can calculate it. If you calculate it for point five, we know we can meet the accuracy requirements. So it's like, that's what it says. It's a conservative estimate using 5.5 conservative estimate. So So for, for example, for 105 chill, for 5% SharePoint, we get this is 4.5 we get 400 that's for alpha equal to 5% for alpha equal to 10% we get 300 now this is pretty standard. You know that you see all the time in the newspaper, The elections are before election, a lot of summers being born of potential Future voters. And people ask you, who will you vote? You nobody's first mutual, and typically you said the sample size of 400 Well, this only sample 400 because the estimated, you know, with the confidence of

Unknown Speaker  1:14:55  
95% confidence.

Speaker 1  1:15:00  
Oh, well, yeah. So they get, by using 400 they get a proportion that they know as a confidence interval of 5% plus or minus 5% is 95% confidence. That's a typical sample size that you see in newspapers before actions. Now,

Unknown Speaker  1:15:28  
let's go back to our samples, our target example, assume, let's assume that we want to have 5% with confidence of 10% then what should be the sample size, 300 that we could just calculate the below. So for alpha equal five 10% for D equal to 5% share points, meaning 173 plus or minus.

Speaker 1  1:16:05  
We need 200 but we estimated same proportion without income, right? We had resume proportion for low income, medium income and high income. And this was a distribution, in our sample, 600 This is a distribution. And if you are going to have a stratified sample, three different samples, it's different. But if you don't, if you just select a random sample to guarantee that we have 300 in each in every in the minority group, which is either group one or group three, we need 500 observation, right? Because if we selected this, the distribution of education in the population, and if I collected a sample of 1200 I expected 300-600-3000 because that's the distribution in the population. So, so that's really a very large sample. And so what you we can do better for certified sampling, certified the population by level of education and those three random samples, then we have just 300 300 300 Why have 600 units? Not necessary. So we are down to 900 but we can do better than that. Why? Because, if we are based on apoyo in order to assume that the tablet ownership for education is between five and 10% cannot exceed 10% and for medium income is somewhere around 50. So the most conservative yield, the most conservative would be 10 yield, the most conservative would be 50. And for higher education, the most conservative again, would be 50. So if you apply it for 10, we get only 100 so we can only select, we select 100 for the low income group. These two groups, the conservative will be 50 and 300 so we are down to 700 so kilo random sample was 1200 then we did 300 for each group, and like the point five conservative. But then, if it's taking account down some knowledge. So that's how certification can be yes. In that previous example, the critical value the outcome

Unknown Speaker  1:18:49  
was 10% is that what you're using, 1.65

Speaker 1  1:18:57  
Yes, yeah, or 90% confidence. No, okay, so I have one minute left and then, but so far a confidence interval that, you know, give me the probability with the confidence of fun. But the question is, what about detecting size effect so effect size, and I will not do it today. I will continue this next time effect size is smaller, substantive difference between h1 and h2 that we want to detect so very often, like let's talk. Take the example of marketing. What should be the sample size to make a decision whether to fire in place or not in place? Marketing Director, what should be the sample size? Well, what's the Detect side effects? So that's very often. Now, sample size may be selected on the basis of this confidence interval, but the probability. So we'll do this next time completed just the beginning of next time.

Unknown Speaker  1:20:22  
What's

Transcribed by https://otter.ai
