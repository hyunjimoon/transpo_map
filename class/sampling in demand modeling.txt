Speaker 1  0:00  
Do it again, and then I use a template example, using this formula for estimating a proportion, but estimating three proportions. For this. And so assuming a confidence interval, 90% confidence interval for and the size is based on 5% we want to have a confidence interval or 90% confidence interval that includes plus or minus 5% the proposal and the required sample size that we calculated using the cases was so hard, so if you want to determine the proportion, the confidence was 90% plus or minus 5% then you you use, you need 300 that was the formula that we use last time, just to remind you, here it is. So it was sample size determination. This formula for proportion, assuming alpha, 10% and also size concerns. And so the zine for that is one point 65 and we get the 300 and so the required sample size for each proportion is 300 since you're estimating three proportions, if you're doing a random sample, in order to have at least 300 small category, we need 1200 sample size. But this gives us redundancy, because we have 600 from a minimum, level, which is more position than we want. So we can do stratified sampling, meaning we have view it as not a single random sample, but three different random samples, the random sample of 300 for low education, 300 for medium and 300 for high education, so we're down to 900 then if you remember the formula, the formula applied here takes into account the variance, and the variance is a function of the proportion, And if you make an assumption about the proportion for the 300 we use the conservative value of the proportion of 50% the one I told you that the variance value is a parabola and it maxima is maximum when the proportion is equal to point five. So the conservative value for this group is 10% 50% for this group and 50% for this group. In other words, if we calculate the required sample size with the proportion the conservative estimate is 10% we get it only 100 as required. In other words, we need 100 for Rogue patient level, and then 300 so down to 700 and forget what you want. And then we ask, we had a test about the the target. You know how we is our marketing person needed the target and and so in that case, we may want to look at the type, 2l, and the question is, within what range can we detect that whether the target is met or not met. And so this is I talked about, the fact size and the type two test. And when we wrote about data, we calculate data. So we calculated data last time, this value here and which is the probability, conditional probability of accepting when false. So, given that the null hypothesis, sorry, let's say that the null hypothesis is one, alternate hypothesis h1, is two, then this is the probability of accepting, because we are making a decision on the basis of alpha. So if you make a decision on the basis of alpha and determine that this is our critical value, so in this region, so we are accepting when it is false in if h1 is two, that is the probability of a Type two arrow. And the question is, you know, we calculated what it is, and it was so high. If you remember, last time, the probability of a Type two arrow was about 80% and that's like. Be too much, because accepting your thoughts, meaning, in this case, if the authorized hypothesis is true, meaning the person is not a good marketing person, and we are accepting so we are keeping that marketing person. So we should be concerned about that. Do it all. And then, so the sample size determination is, let's specify what beta is acceptable to us. And then we calculate, we calculate the sample size that is required for some level of data. So this equation is explained in the appendix. I love them to go through the detail of how you get this equation, but it's very simple. Basically, is not calculating this area here. So what's this area? It's an area under this distribution, the distribution of h1, for this value to this value, if it was a 130 this goes to infinity. So, so you calculate an area. So that's simple to calculate. So that's a trivial equation. I mean, this looks like you've not seen it before, but this is a standardized cumulative load, like the kind of distribution I showed you, the table format earlier in previous lecture. So this is just a simple equation, and that's for one thing, it looks like that. That's it. So what the ES? ES is a Fixed sign.

Unknown Speaker  7:01  
Fixed sign

Speaker 1  7:04  
is what's the difference between the null hypothesis versus the automated hypothesis? That's effect sign. And so now hypothesis, this example was the target was met, and the alternate hypothesis the target was not met and it was not met by with the final met by this effect size. So, so we need to solve we need to specify beta now, not alpha beta, and calculate what the required sample size will be. And so in our example, the known hypothesis was we met the target by people to point certified. The alternate hypothesis is by two, is less than point certified. It's a one thing test. The test statistic that we use was this ratio and and so, you know, this was the estimated value. And then we selected the given alpha and made the test. But now, instead of selecting alpha and we are now calculating beta. So this is the equation. We substitute it for that for proportion. And depending on the alpha that we select, we selected alpha the 10% so Z, alpha is 1.65 the PI, N. We know what it is. It's point 33 so we just calculate, we calculate beta effect size. It will assume 2% or 5% or 10% for different effect size, and calculate beta for different effect size. So for 2% it's equal to point eight. For 5% it's equal to so that's what we calculated before. For 5% we get this right, and for 10% we get this but now we can specify so what you can see that as the effect size increase, the power of the test is also increased, it's easier to detect larger effect size, of course. So the type, the conditional probability for type two error, it goes down right. So we calculated it for 2% earlier. But if we say, well, I mean the required that you see for market share is maybe only 5% then the beta is point 2.4 and if within 10% I can beta is okay, but now let's assume that the n is not this was for n equals 300 Now suppose we say, let's, let's decide that we want beta to be equal to point one. Then and we selected beta. Out, we selected an effect size, say, to 2% or 5% to 10% and let's solve this equation now for n, so we get an equation that looks like that, and the solution is given here. So for the effect size, 2% if I want to be able to detect so the phenot meeting the target is a range of 2% effect size. I need almost 5000 observations. So often that's this kind of calculation of the power to determine the sample size is often used depending on what the purpose of the data collection. But that's another way to just go ahead. So

Unknown Speaker  11:11  
that operator is the CDF operator,

Unknown Speaker  11:17  
and so but there's a table that they showed you that's in the previous lecture. It's cumulative. Is it cumulative? Standardized, normal.

Speaker 2  11:35  
So, so phi, phi itself is different.

Speaker 1  11:40  
Yeah, phi of x is a CVS, standardized, normal CDs. And so this is the probability like. This is some, let's call it beta x.

Speaker 2  12:13  
Beta is the conditional probability of the type two error. It's that in the graph you had, it is just kind of that right tail.

Speaker 1  12:26  
Yeah, definitely. I mean all this distribution standardized robot, because we divided by the standard error, right? So we get asymptotically normal distribution. We are using a standardized normal. So these, all these are standardized normal. And so to calculate any of these areas, we need to calculate, we have the horizontal axis, and we just calculate the probability, so alpha, to determine what this area we say under the standard that is normal, what value will give me a cumulus Q of alpha here, calculate beta i. It's an area under this distribution. And so this is solution. Goes up down, goes down here. I calculate the q up to this point. Calculate the Q up to this point and take the difference. That's how I got the difference. So if you have, if you have, if you have a distribution, what would that amount? And you want yourself, but I want to calculate some area. So you calculate the CDF at this point. This is x1 and x2 so it's the CDF. This probability is equal to the CDF at x2 minus the CDF at x1 okay.

Speaker 1  14:11  
So let's basically conclude the section of sample size in termination, we chose not to do it with starting with an allowable confidence interval, and then concluded now with effect size when we are trying to detect something and we are worried about type two, meaning we are concerned with keeping a bad marketing director, and so we want to focus on type 2l, and that's basically a different approach to calculate the required sample size. Next section of the lecture is about certified sampling. So we already introduced the idea of certified sampling is a double example by saying, let's, let's divide the population by income group and by education group, and sample that is education that will separate them. Let's call the stratified sample, and so the population is divided. Into starter, suddenly second from each and it is worthwhile when it is worthwhile when the population variance differ by starter, and all the cost of data collection differ by starter. So what I've shown you is this first point, because if we know that the proportion in the low income group is at most 10. It means it has a lower variance than in the other group. So I can it's enough to collect only 100 in that group. So I use the information about the fact that the population variance differ by slata. So the intuition is always, if I can identify a starter that has high variance, big circle, you have another struggle with lower variance, all the same, then one person is not in one observation, jokingly, of course. But of course, but a small sample is required. So, so that this is implementation of the simple idea that you have a budget to spend on data collection, you allocate it to the places where you have the least information, the greatest heterogeneity. If you can identify an area of the goal for which you know genius, you allocate few resources then the collection. So allocate your data collection budget through segments data and segments for which you have

Unknown Speaker  17:00  
sort of less knowledge.

Speaker 1  17:04  
So the knowledge is about resilience. If you go with a high variability, you need more observation, okay, you can even this study by sampling. Is even the ease of benefit, even beneficial if you allocate your sample sizes, suppose that NSC is your sample size, and suppose that you know in the population. So in our target example, let's assume that we know that the education group are provided. 2515 25% suppose that we know it from survival source, and we have budget for, say, 600 observations working as the best way to allocate it. 150 301 50. And that's called proportionate allocation. So we allocated our sample size according to known propulsion is the propulsion G denotes a statue or group. And so this is a sample size for group G. So these, these are the proportion of the goods in the population. They add up to one, and that's a proportional allocation, meaning we have information about opening my area, different area, and I know the population niche area, and I will go different samples in different areas, but I will do proportionate allocation, because I'm trying to do a lot of sample, so that's you can say, you can show, and I'm not going to do it even for a proportionate allocation is beneficial. Why it is beneficial? Because I'm using some knowledge about the pi G is the way I allocate my sample, so it's better than just a simple amount of sample. So by controlling and making sure that in each group I have, I have its proportion in the population introduces the variance overall. So that's a good thing to do, but you can do better, and you can do better by calculating an optimal rotation. So if you say, if you want to apply with the concept that we talked about already, if you want to use information about the different standard, different variants, you can say, I can come up with, if I have a fixed budget, say a sample size of N is 600 and they now allocated to the groups, and I have Capital G books, then we get this formula and personal innovation in the human This is a relationship. So, nothing sophisticated. So, so if you want to reduce the overall. Variance of estimating x bar the average of all goods, then that's the best way to allocate it. And I'm showing you this formula because it has some intuition that is, in general useful to the allocation of sample is in proportion to the share of the starter in starting in the population by g is proportional to the standard deviation. Scope of the variance, the bone variation is in the group, the greater the sample size. So, so it's proportional to the sharing the population and to the variance. That's the intuition, and it's inversely proportional to the square root of the cost, the unit cost of data collection from observation. So assume that there is a budget constraint, and if I have a budget constraint, then I will also take this into account, usually, very often, the cost may be the same, but if, anyhow, but it's inversely proportional to the cost, it's expensive. You collect less so and here, but you say, in order to allocate it optimally. I need to know the G, I may not even know the PI, I may not want to know the PI, I may not know the G, and I may not even know the C. So what I'll do? I do a two step forces. I do a small, simple, random sample to learn, I will find out what it cost me to collect the data in different for different area. For example, if I start out different geographically, I will find out in full sample, what are the variants The within covariance. I will find out also maybe an estimate of the shells. And so once I now have all these values of the small sample, then I can upgrade spend the rest of my budget on using this optimal location. And I told you I did. It's okay if you really collected a lot of data, you know, you're spending step one. And one small is not really very small. It's large enough to give estimates. If the first step is too small to give you good estimates of this value, then this doesn't work. And then, of course, this was only an allocation and but it was also you can. You know if you calculate an overall mean, you can decide what should be the sample size using the same formula as before, excepted in in calculating rebellions, you have the location so you can assume of your location. But usually this is not the case. The case is. We're not just interested in just one characteristic, by the average, by new X bar we are interested in, you know, my area, my group, like in the case of the average example, we had three proportions. So the certification is necessary, because each one requires different sample size, etc. So this formula for case, which is usually not really relevant, but it gives you some intuition. Okay, so this is slightly and finally, very important practical consideration. I am not going to go through it because she landed behind. And just to tell you that there was a forum, they showed you of very special cases, the most sample size determination sampling strategy is based on this particular consideration. And so. So there is no kind of cookbook approach that you can use, except if you are selecting, just conducting services, selecting a voting outcome, are you voting for this person just in one proportion? Then yeah, the formula can be used. So you have to use a lot of common sense. So what I'm going to ask you is to read the slides, as I mentioned here, of responsible and non responsible, and we have not talked about it so far so so that we've talked only about something else. So the responsible one of the way we're going to discuss it later is something called incentive alignment. We have to think when we give we interview respondent, we want to give an incentive to respondent to tell us the truth. How to do it is very difficult. Like, for example, in in transportation, sometimes we just tell the person, this is your city duty. I mean, because this will determine what transportation will be provided to everyone, like voting. So your duty is to tell the truth. In commercial application for surveys, you often have to designing a payment for the point of the service, which depends on how you answer the servant. So if you say, if you ask, like, your opinion about soft drinks, and you give an opinion about soft drinks, and then you get the soft drinks that you said you like to talk for me, otherwise you'll get links. But I mean, just to give you what I mean hereby incentive. So the response, oh, that's what you need to

Speaker 1  26:53  
worry about many surveys, the incentive for the respondent is to do quickly and get, like a payment. You get the fixed payment for during the survey, and so you're sending it to those quickly, those information. So anyhow, so that's what you need to worry about and for normal Spotify, there's a slide here on that I'm going to talk about boyfriend. So how do we deal with normal Spotify? It's a major issue.

Speaker 1  27:35  
That's why surveys are very problematic, and you often, anytime you do the survey, you want to find also some some sort of actual data sets, like a transaction database, register of some sort that you can compare against your cell and so, of course, the first word is obvious, but that's what you really need to do. So you need to have some external data that allow you to compare the distribution in your survey versus distribution. And if you have such external data, one technique that I'll teach you later in this next book, not the next book, next, next, next. There'll be a lecture about a technique called is IPF, which is a very easy to apply method to reweight the sample based on external data. It has many other applications, but the idea is to do what's called post notification, meaning you collect yourself whichever method you use, and then the question is, what weight should they give to different groups or different formation? And you base it on external data. And so this degree then allows you to calculate what should be the weights. Okay, that's it. So any questions about something? One, two, golden,

Unknown Speaker  29:20  
okay. I

Speaker 3  30:08  
i Otherwise can press the button next to the Zoom device. So

Unknown Speaker  30:22  
you concluded

Speaker 1  30:28  
now the film's broken material from first block of material from birth review

Unknown Speaker  30:44  
statistics

Speaker 1  30:47  
company and now is most of things you need to know to validation. And all my lectures will be linear. The expression is non linear. We'll talk about this a little bit. So the first lecture is just to say, when we say linear, what do we mean by linear? Then talk about three squares, Y v squares, as opposed to maximum black unit. We're not going to do maximum likelihood v square, and why? Because it will not require us to make an assumption about the probability distribution. You'll see it's later. Then once you apply these squares, the as if it is gall fitting, we talk about the mathematical properties of the fit and and we are going to see that in order for the least squares solution to exist, we have to make some assumptions and some properties about the data. I'll call it assumption one, and then we'll then talk about the sample distribution.

Unknown Speaker  31:57  
So that's

Speaker 1  31:59  
often when you talk about regulation. It ends here, right? Because you do regression. What's the button? You get results. In order to get results, we have to satisfy assumption one, otherwise you don't get those thoughts. But then we will emphasize here the properties of the sampling distribution. What are they? Biases, consistency, efficiency, normality. That's the thing that we need to know, right? And in order to look at these properties of least squares, we will make some assumption, and then we will see that this assumption is very critical to get unbiased estimates. You'll find out what it is, and then this assumption allows us to get efficient. And this assumption is to give us normality. If you don't want to have a loud sample, you don't rely on asymptotic normality. You want to have normality for any sample, then it requires an assumption. Okay, so this is what this assumption is all about. Doesn't seem interesting. Assumption, what to sleep over there consequences that we'll talk about. I have a single example that they took from a long time ago. I found out in some book was actually a data TSC, stands for Classic Analysis zones. So Chicago was divided sometimes during the 60s to 57 zones, and then they estimate the data on averages by zone. So this was the number of trips to occupied body units, average co ownership in the zone, average household size and some social ministries of the zone. So the idea of the model that we have in mind here, that they had in mind, is to predict the threat making. The whole concept is that areas that generate slips, slips going out from coming in and the generation of sleeps depends on auto ownership depends on House of size, depends on these social rank or standardization index, like this measure of density and segregation index. I'm just using it as an example, you always when you have data you plotted as two dimensional plots. This is just the dependent variable that we are interested is trips, right? So this is trips, the Occupy boarding unit as a function of carbon sheet as a function of what's this, I would

Unknown Speaker  35:05  
go, maybe I would have some time i. So these are the social index and some other index. And so you see some effect. Here you see like a cloud of point, and this is maybe yoga is no effect. And so this is just an exploratory look at the data. And then this is dependent as a function of the last element,

Speaker 1  35:41  
organization index. So as density increase, the number of trips decrease. Why? Because these are all vehicles. So they did not count the 60s. Walking did not come anyhow. So this is a linear model, right? Simple linear model fixed the Occupy going unit is equal to an intercept. And these are five and an error term, meaning that this fit will not be exact. So this is a simple example to find out the values of beta. Use the beta and estimate the best values of beta. So the linear model, when you say linear model. We think that model like that, with alpha and beta the unknown parameters. This is just a bivariate, meaning only two variables, meaning only one exponential influence will extend its course to both variables. But all these models are linear as well, because the word linear in linear regression refers to linearity in the unknown parameters, alpha, beta, so the variable, it could be any non linear transformation of the variables. And and so these, all these models, are linear. And so when you write it like that, it's alpha plus beta X. X can be a function, a known function for variables, as we just estimate the betas and alpha disabled is coefficients. Now, if you have this kind of equation, this is not linear. Why? Because it's non linear in beta. So this is a model. Could be a regression model. Is an additive algorithm. So that's important, that the discrepancy between what we fit and what we observe is additive, right? So we have this is like the fitted values will be dysfunction. And so here the same, the fitted value will be this function, to be function of alpha and beta, and it will be an additive. Sometimes it's called the disturbance. So it has different names in once we fit the model, after we fit the model, and you get fitted value for alpha and beta, the discrepancy between of cell and its fitted value is called the residual

Speaker 4  38:41  
Question, okay,

Speaker 1  38:47  
is this model linear? No, let's talk. Is this model linear? No, the answer is, well, it's written as non linear, but it is a linearizing transformation. By taking the log of the two side, this is equivalent to this the log is a monotonic transformation. It doesn't change the model. And so the dependent variable becomes the logo for original y. The independent variable is a logo for that it was originally and now it is linear. So, so this is not linear, but this one is by having a multiplicative L, then e to the epsilon, by taking the load, we get this expression. So the the this is different. So this is a logarithmic transformation, and it will be different from linear I mean, if we have the linear model, or if we have this, these two models are different. Error term is multiplicative, and here is additive. This model, again, is normally. Here. But if I again, if I put the arrow there inside this exponent is exponent equal zero, alpha plus beta X is now alpha plus beta X plus epsilon. There is a long mean. There is a linearizing transformation, which is shown here. This is, by the way, logic model, binary logic model, or sometimes called logistic regression. And so this, this transformation. And of course, here, why cannot be negative, here, why cannot be one or zero. So l.

Unknown Speaker  40:54  
So

Speaker 1  40:56  
you have to have Y interior to what zero to one. But if you have integral oscillation y damage over one or zero, then this transformation cannot work so often. When this transformation is applied, it's applied to group data. So if you have data with a Y is done variable, you can create books in which you have some y. You can type with the proportion, and then you replace y of the proportion between zero and one. But anyhow, let's assume that now y is number between zero and one, then we have a linearizing transformation.

Speaker 5  41:43  
So good question. So for linear transformation, it's important to preserve the error distribution right as like plus epsilon, we need that not as

Speaker 1  41:58  
epsilon is epsilon, whatever epsilon goes here, the same epsilon is zero. So if this epsilon was normally distributed, then this epsilon is normally distributed. Now a multiplicative level here, if we said epsilon is normal, then the e to the epsilon is normal. So

Unknown Speaker  42:22  
we can

Speaker 1  42:24  
it's a multiplicative and here is an additive, whatever, whatever assumptions you make on epsilon, here remains whatever you so it's not the idea is that the definition of all the terms remains the same. Okay. Kill fitting this squares. Why are these squares? So we have, let's assume again, that we have only two variables x and y. Y is a dependent variable, x is explanatory variable, and there is a straight line that is given by alpha plus beta. X. This is a straight line, the intercept is alpha, the slope is beta. And the question is, what's a good line? We have the points, and we might want to find the line. And the idea is to treat epsilon as a discrepancy between the fit design right and and y, and you want to somehow minimize epsilon. Want to make epsilon as small as possible, but we have many epsilon right for every point we have an epsilon. So how about just summing up all the epsilon and minimizing all these epsilon? Well, this will not give us a unique line, right? So if you say, Let's minimize the sum of epsilon, the minimum is equal to zero, because we can just make the line goes through the x bar, y bar, this one. So it's a line. If x bar and y bar is a point on the line, then the sum is zero by definition. So the minimum we know is zero. But there is no unique line, any line that goes to this point, x, bar, Y, bar will bring this to the minimum. So there's no unique line. So we say, okay, so is a sign of the money, because you can by shifting the line. The line is all rotated on the point of mean. So some epsilon become more positive, other epsilon become more negative, and they cancel out. So let's just sum up epsilon, the absolute value of epsilon, it's equal to sign and so this, this gives a unique line. So this is the minimum the sum of absolute residuals. And so what are the property of this line? There are, there is one property, as it makes this line not as attractive. Supposedly had 3.123

Speaker 1  45:35  
turns out, in this situation, if you try to minimize this, the line will connect the two extreme points and doesn't matter. Well, this this point will not have an effect. You can move this point to the basement or put it on the roof, this line will still remain this it will not follow this observation. It may be an outlier. So if you have situations where you have an outlier, this model, this criteria, is more robust, but but in general, if the points or the points are correct, and if this goes up or down the line is to somehow follow it, it will not be a property of this. So this later on, we will prove that the least square method of sufficiency, this is a sample, an indication that there will be an efficiency problem. Why? Because this data is not effective. It's not throwing away data. So the least coils is we square, the absolute sense that you have to evaluate the square, then we get rid of the sign problem. But now large volume and in because of that, if any mistake, and there is no solution which is kind of way out of the cloud, it can have a significant effect on the results. And if this is not justified, this observation is called an outlier. And so by changing the real squares, and there is possibility of the motion for vastness is less mistakes in the data, mistakes in the data at least two hours. And so that's why data is the same we do for maximum because if an observation has a very normal ability, because mistake, you can have large and so the characteristic of u squared micro One is that they there's too much weight to observations. Which are different from the from the cloud, from the cloud, the cloud, and they're efficient. So this world will show we have some efficiencies here for maximum likelihood. And told you about the efficiency property of maximum likelihood, comprised of to pay is in robustness, and therefore analysis of the data, careful checking of the data before you do an estimation is required. Just a warning, okay, next mathematical properties. So let's do it. First of all, the least squares. The idea is, you want to minimize the sum of the squares of this epsilon. Epsilon, you know, is equal to y minus alpha minus beta x, right? So this epsilon square just to substitute the model for epsilon, and then just calculate the first order conditions, second order conditions. So we have, we have two unknown alpha, beta, so we differentiate this with respect to alpha, and then we differentiate with respect to beta and set it equal to zero. And the solution of this system of equation, we have two equations will be called alpha n and beta n. So we have two equations, one and two. This minus two here doesn't necessarily cancel out, because it's equal to zero. So we have this sum over n of this expression, and then the sum of the equation. This is this first equation. Essentially, it's an interpretation, and the second one will have an interpretation as well. This often refer to as a normal equation. This is just the first order condition. So. So what is the interpretation equation? One means the defeated line goes through the averages, the point of averages. Let me show you. If you minimize the sum, then the epsilon goes to x, bar, Y, bar. So that's what the first equation does. First equation has this probability that the sums are residuals. Now, now notice that with theoretical model, you said the model is y is equal to some function of x plus a negative L term. I call it an algorithm or disturbance. But once the assigned values to alpha and beta, called the alpha n, beta cells, will call it the residual. So this residual add up to zero, which means the line goes fitted. Line goes to the point of means. So y bound is equal to alpha hat plus beta hat. X bound. That's the meaning of equation one. That was equation one. This equation,

Unknown Speaker  51:22  
equation two

Speaker 1  51:24  
can written in the short form, the short way, like that. What's equation two? Equation two says this is a residual inside the parenthesis. So residual times X adds up to zero.

Unknown Speaker  51:41  
So what's this?

Speaker 1  51:43  
X is a factor y, x, n will be vital for sample size is n and they have n values of x, and I have n values of this epsilon net. So this is just a scalable of two vectors, the vectors of epsilon and the vector of x. We'll come to linear algebra, maybe next lecture will only use any algebra. So if you feel that you need an impression in any algebra, now it's a time and but no high level mini algebra. I just want to multiply with the scalar multiplayer, doing filter matrix and single that later. That's about it, maybe a little bit more, but

Speaker 1  52:38  
I mean to teach linear regression without basis. Make the equations too long, too messy, and so that's really a linear addition. That's for linear algebra

Unknown Speaker  52:52  
shines.

Speaker 1  52:54  
It's really so useful. Okay, so the second equation is just a scalar product of these two vectors of vector for the epsilon and the vector of all the x's. And so this means that the residuals are orthogonal to the x's. When you multiply them, we get zero. So this is orthogonality. It doesn't mean that the original epsilon and the x were orthogonal, but it means it's a residual that we produce. Is using this square, we produce a residual of x dot n, so d is orthogonal to the x. This, this sometimes, I mean, you can think about this equation one as also an orthogonality condition. Why? Because you can think about, if you go back to the model, right? Well, both

Unknown Speaker  53:52  
of you like the model.

Speaker 1  53:57  
When you write a model like this, I can pretend as if I have an x here, alpha times x, but it is an X that always has a value of 1111, it's always one. So this, I can write it as alpha times one, and then alpha and beta both looks like coefficients. So, So anyhow, so when i So, when I look at this condition, I can also view it as residuals times one, right? So do I to vector with 1111, okay. So sometimes this one and two together, we can refer to them as an orthogonality condition. And if, if we have in the model variables, this thing that is constant, then we get this definite angles and parameters. And this is residuals are orthogonal to the X so when you take regression, if you do this CO regression, and you obtain residuals, and you check it, it will have to match to be exactly the opposite of constant definition. Please. Square. There's just another way you can say, if I want to produce those engines that are orthogonal to x, do I get these squares? Yes, and it's equivalent. One is equivalent to the other. Some people's going to ask me a question. Okay? And so let's solve this equation. Fine. So we have equation one and two, we now can solve it. And if we add one and two, we get this equation, and then you can essentially get rid of alpha. And by getting rid of alpha, you can solve for beta. If you know beta, then you can calculate alpha, point of view, the averages. So, so here we get some equation for beta. That's a solution. We take equation one and two, and we obtain a solution for beta. But for the solution to exist, we have to divide by this thing. So this one cannot be zero. So the question, What's this? What's this? What's this? What do we have here? And so you know, the Y, bar, x, bar, et cetera. And if you calculate this sum, and if I divide this by n or n minus one, if you want, let's just divide by n, that's not going on the minus one minus two. So if I divide this by n, it is a consistent estimate of the variance of x. So that's the variance of x. What's this? Divide this by n? This is a consistent estimate of the covariance of y and x. So, so beta hat is essentially this ratio of the covariance of y and x divided by covariance. So the variance is always positive. Covariance can be positive or negative. So that's, will be the sine of beta, will be the sine of the covariance of y and x. So that's, that's the equation for the time being, to remember, it will become simpler when you use linear algebra. So the equation that we got beta is we got this expression right. So let's, let's investigate a little bit this expression. I can put a sum like take it outside and put it in this curly pockets here, and then in it, I have, I divided this to these two terms, y n and y n, and I get this expression. And it turns out that the second part cancels out zero. And if it is zero, we get that this expression here can also be written like that. So what does this look like? This was just an equation for beta which is equal to some variance divided by covariance. Now we get this is equal to some coefficients which is a function of x multiplied by y. So, so it's the estimator is linear in y. So it's a property that we're going to use it later. But another, another property that we are interested in is what happened to the second derivative. So we have two unknown alpha and beta. So we had a first derivative. Just differentiate them again, including the cost derivative. And so this objective, the second derivative of the objective function three respect to alpha so it's a beta and it's a cost derivative. So this is the hessian matrix, and, and this matrix is, we can show, is positive definite. And so these are the values, and the determinant of this is given here, and it's not negative, and which means that you have a minimum, and minimum if, if the minimum exists, right? Well, for the minimum to exist, we made assumption one.

Speaker 4  59:48  
So if assumption one is false, then

Speaker 1  59:57  
the solution is the global optimum is unique. There's no other possibilities. And because the Hessian is positive definite for any value to fix this, that's basically least squares. And these were the mathematical properties. So what are the mathematical properties that you need to remember a that it is, we have this assumption, one, assumption, one. What does it mean? When is it this? These are all positive value, right? It cannot be negative, right? So it's can only be non negative. Can be zero. The problem is, when is this zero? The same? When there's no variation? Well, the x's are all the same, no variability. This, this assumption needed to this variability in x so that's what you have to remember. For the least consolution to exist, there has to be variability in X, and solution, whatever it is, is linear in y, and it is, oh, excuse me, and it is unique. If it exists, it's unique. And that's these are the mathematical properties, and I forgot to mention the residuals orthogonal to the x. Could

Speaker 5  1:01:40  
you explain again, how assumption one applies to the Haitian positive definite assumption one and the Haitian positive definite. How does that apply?

Unknown Speaker  1:01:53  
This one? Because

Unknown Speaker  1:01:58  
that not not being zero.

Unknown Speaker  1:02:04  
Sorry, I'm a little confused. No,

Unknown Speaker  1:02:12  
absolutely

Speaker 5  1:02:22  
think that is always just positive, right? Yeah, yeah.

Speaker 1  1:02:46  
So the Hessian will the determinant will be zero, if the assumption does not hold. So it says so the Hessian is always greater or equal to zero, but it's equal to zero. Okay. Next is statistical property. So in order to move statistical property, remember, we have to use this operator, E, for expectation. So the definition with me, we need to define you something called the conditional expectation. So what's an expectation is we have a variable, say Y. Y is a distribution f of y, and if you calculate the expected value of y, this is called the expected the E of y, expected value of 1y is equal to the solution of this integral. A conditional expectation is when you integrate calculate the expected value of Y, conditional on the value of x. So instead of integrating it over the distribution of Y, we integrate it over the conditional distribution of Y, given x, what's, what's this conditional distribution? It's a joint distribution divided by marginal you know that. So if we have such a distribution, and if you calculate the distribution and the mean of y given x, you get this off label, so it's expected value of another value y, given that another variable has a specific value, x, so the linear regression model. Therefore, the intuition behind the linear regression model, or regression model, anytime in the regression model, is an additive epsilon, the expected. Value of y given x is equal to this part, the systematic part, if the expected value of epsilon given X is zero. So that's what we want to the best way to interpret the linear model will be this whole So the expected value of X, Y, given x is given by the model, by the fitted line, and in order for this to be two, then the expected value of epsilon goes into y as well. The expected value of epsilon given x has to be zero. Now what does it mean? If this is not true, then what's the problem? The problem is, x affects the epsilon, then beta doesn't capture the effect of X. I mean, when we write a model like that, we say here we have an explanatory

Unknown Speaker  1:06:08  
education, something like

Speaker 1  1:06:11  
that. And while we have this ownership, we want to find out what they feel, and if the expert also depends on education, then we can't capture the model doesn't allow to fit. The model doesn't capture the sense of education, because some of it is in the absolute so when this is violated, we will come back to talk about endogeneity, that's there's a disease that you always have to worry about. It's often neglected. People use it's often neglected. And when it is neglected, it means that the model doesn't fit in analysis, analysis on analysis, meaning you will be intervened in the system. Intervening system, meaning we want to change x and predict what we added y. If changing x is also going to affect the epsilon, then we don't, then it should be part of the model. It shouldn't be just a disturbance. The disturbance we want it to be just noise that we don't care about because we want to explain our education, affect our methodology, and so we don't care about the noise we care about education, which is x, and what the full effect of x to be captured by the physical value. So, so this is a this will need to become an important part of the regression model in when it is validated with saying we have a function. Angie was talking about the and we find out that this is that will become an assumption. So pretty soon we come to an assumption that will be necessary in order to have this interpretation of the linear formula. Ok. So what do we want? Both of the designers, we have some sample distribution. We focus on beta two, unknown, alpha and beta. And the statistics are alpha head and beta head. Let's focus on beta head. So we're going to do all this analysis, conditional next. So we collected data on x and y given x, but we pretend as if x is given and we observe y. So we say, Let's for the given some size that we collected the x and we obtained the estimate beta hat, how different would it be if we do it again and then also have to be some distribution. So for us, we want the summary distribution to be centered on the total value, which is beta. In other words, we want the expected value of beta given x to be equal to the correct value beta hat to be equal to beta and to be efficient, we want the variance of this beta hat and beta hat just to remind you, beta hat is this expression right? All this expression one of the two are equal. So given this beta head, we need to prove that its expectation, conditional expectation, is equal to beta and the variance, which is also the. Will be defined as some expectations of differences, square, etc, that the variance of weight that is less than the variance of some other potential estimator, beta tilde. Do we have some efficiency result that we can state? So first of all, let's look at the unbiasedness. So I showed you that the beta head can also be written like that. I do remember that was two slides again. So we can just for y, we can just substitute the model,

Unknown Speaker  1:10:39  
and then we can,

Speaker 1  1:10:43  
turns out that this alpha will disappear. We get this expression. And after some calculation, we'll have some additional calculations. Here we will get an expression. It looks like that. We find out that beta hat is equal to the two value. Because when you want the model, when we substitute for y the model, we use the two value alpha and beta, no hats and epsilon. And so we get that the beta hat is equal to beta plus an expression here. Well, that looks like that. See this cancels out this one, and it's beta. And here we have the sum over n of these values that we've seen before. You look like the coefficients of this linear equation. It's only a function of x, so C n a function of x multiplied by this epsilon. So the question is, when we take the expected value of beta hat, what happened to this term? That's a discrepancy between the two value and the estimated value, but it's a function of the disturbances. So suppose that we take an expectation of beta hat given x, then it's mu equal to beta times the expected value given x of all these things. But if x is given, then c is a constant, right? So, so we can move in the expectation into the sum, and it is equal to Cn, this coefficient and the expected value of epsilon n given x, when is this equal to beta? If these are all zero. So if we can conclude the expected value of epsilon, every epsilon, expected value of every epsilon, every n kind of entire vector of epsilon given x is equal to zero. Now remember, I gave you like an intuition for what this means when I talked about the interpretation of the regression minus so this is something that should be expected, because it says this will call this assumption too. And the consequence of assumption two is that beta hat, the riskimeter, is unbiased, which is clearly

Unknown Speaker  1:13:43  
what we desire.

Speaker 1  1:13:46  
So this is called so assumption one was that there is variability. The X is value, there's vulnerability in our data. And assumption two is that when you write the model, you have this epsilon there, which does not depend on x. All the explanatory power of the model is the factor x is capitalized, captured by the fitted line and and the consequence is that when we apply a squared we get an unbiased estimator of beta.

Speaker 1  1:14:30  
What does it mean? Graphically, it means, remember this point that we had, we found the line and for the epsilon vertical disturbances, right? It's additive. It's y equals the fitted line pass epsilon. So for every value of x, there is a distribution of epsilon, the distribution vertically moving up and down for every value of x. And we are saying, we're saying that the expected value of. Absolute N, given x is equal to zero, it means that all these distributions are centered on the line, right? So wherever we are, wherever value of x, it may change the variance. The variance could be different, the shape could be different, but the expected value of the distribution, the mean of this distribution, is on the line,

Unknown Speaker  1:15:31  
on the two line.

Speaker 1  1:15:34  
So that's another way to interpret this condition, this assumption, too. So

Speaker 1  1:15:47  
next, we want to do the efficiency. So we want to calculate the variance of beta hat. So what's the definition? We will condition it on x again, that's the definition of variance expected value of the difference between beta hat and its expectation zero under assumption two, when we make assumption one for it to exist, we now make assumption two, then this expectation is equal to beta and So the variance is equal to this expression. So now we need to we know that beta hat minus beta we calculated it. You know, beta hat minus beta is equal to this expression. So this to calculate the variance. We just put this expression, but we now have to square it, and then take the expectation, when we square it, we get two kinds of sums, the sums of this c squared epsilon square, and we get cross products. And so we divided into these two separate sums. And then when we take expectation, we have an expected value of the sum given x. We can move the expected operator into this thing, and we can do the same here. So you have the expected value of epsilon square. And here we have an expected value of S, 1n, and x1, and prime any other so this is just epsilon n times epsilon, and this epsilon n times epsilon n prime, and the summation is over n and n prime. So now we have to look at these terms, and we're going to make some additional assumption. So, so what can we assume about the variance of epsilon? And what can we so this because the expected value of epsilon is zero, then this is a variance of epsilon. And because the expected value of epsilon n and epsilon are both zero, this is a covariance of epsilon. So what does it mean? It means epsilon is a vector one give epsilon. For every observation, give n epsilon. And until now, we said that the expected value of every epsilon given X is zero. Now it's saying, OK, so it has a distribution. So that's what's the variance of this distribution? So remember, we plotted this distribution, we had this distribution. Now they may have different variance. These are the variances and these are the covariances between different and so we will make an assumption. This transfer is equal to definition is so if you expected value of epsilon, given x equal to zero, then this is variance of epsilon, and this is the covariance. Can be one minute late finish a sentence. It is a covariance. So I like the other variance and covariance this expression for the variant to raise their hands, and we're going to make assumption three. And assumption three is two parts. One is called the homoscedasticity, and the other one is called the autocorrelation. This basically homoscedasticity means that the variance is constant, that we have this epsilon. For this observation, they all have the same value. And this would say that the epsilon for this observation are not correlated. The observation are independent. So without them, the epsilon are not correlated between observations. So that's when we authorize your function. You make this assumption. Now could. That formation are independent. We'll relax it later. We'll realize this one later as well. Okay, but for the Angie, you make assumptions three, and if you make assumptions three, we get we say that these distributions have the same variance and they're uncorrelated. These are the assumptions about this vertical distribution. And if we make this assumption, we get the formula that tells us that the variance of data has is equal to this expression. So page 27 will continue here with this expression. Now we got to this expression, and now we get done with the interpretation and We continue. Okay,

Speaker 4  1:21:00  
you. Thank you.

Unknown Speaker  1:21:15  
So yes, 200

Unknown Speaker  1:21:21  
observations,

Speaker 6  1:21:38  
so what do you think I didn't make this problem. Let's infer the nature so perhaps population histograms generate

Unknown Speaker  1:22:00  
the population

Unknown Speaker  1:22:04  
rate. Is it what we created?

Unknown Speaker  1:22:13  
Yeah, yeah,

Speaker 6  1:22:16  
yes, I will. So 70% I think that's what we need. Yes, I will. So 70% I think that's so I think

Speaker 2  1:22:40  
you said that, because, for example, the first

Unknown Speaker  1:22:47  
analysis annual income versus, like the tenant

Speaker 7  1:22:55  
variable. But if they go out at night to make popular, leaders, that was the first thing I did looking at the modes Now, so that was the context of my question. For

Unknown Speaker  1:23:20  
next block.

Speaker 8  1:23:36  
Okay, given up there putting

Unknown Speaker  1:24:03  
the tape non motorized car tax.

Unknown Speaker  1:24:14  
How many interprets?

Unknown Speaker  1:24:20  
Understanding. How can you interpret odds?

Transcribed by https://otter.ai
