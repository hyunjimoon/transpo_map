Speaker 1  
This was assumption. Now we're talking about violation of assumptions. So what we talked about last time we said, suppose that this assumption is violated, and we can write this to be equal to some scalar times a matrix. So this is just convenient to keep it the same. And this V can be square. It's an n by n matrix. It has to be positive definite, to be pops, covads matrix, symmetric and positive definite So anyhow, but v can be anything that will satisfy the requirement that it's a variance covariance matrix. And what they said, suppose that you don't have V, or you don't know when what V is, you suspect maybe you're still applied or less estimator what happened. It's still unbiased or consistent and but it's not, does not satisfy the Gauss Margo. That's efficiencies you're in. So it's not the most efficient. But if you have large enough data, they said, Okay, so if you have millions of observations, who cares about efficiency? We care about the biasness. So suppose you want to, you don't know. And also you may, you may be afraid of using a V because you don't have good information about what V is, you can apply feasible GLS, which means, do OLS. Get the residual from the residual, get some estimates as best as you can of V, and then use estimated V in GL, I suppose that you're not trusting the assumption about you have to in order to do that, you make some assumption about the structure of v, because it's not possible to estimate every element in V. It has n times n plus one divided by two element, and you have only n observation, and so suppose you still apply overs, so it's not efficient. No big deal. But how to calculate the standard levels? So this says, Yeah, I said double stopping. You know how to do it? Just put double strapping and calculate the standard error so you don't need any equation, because very often the software that you use assumes that the standard error is sigma squared x, prime x, inverse. But it's not true, because to derive this formula, we assume that the expected value of epsilon epsilon y have forgotten given x here, I assume that this expected value is sigma square i, if it's sigma square I, you can see if this is sigma square i, sigma squared goes out i vanishes and and the results will be so, so that. So if v is equal to i, if v is equal to i, then this will be single square x, prime x in those because this X, prime x and x, 1x in those cancel out. So we're left on here is this part, right? So, so v is equal to y, then the variance of beta hat given x is equal to sigma square x, prime x, inverse. So, so that's what the software often use. Of course, replace sigma square with s square. You know how to calculate S squared just some constituents. So it's the sum squared o divided by n minus k. And so Kenny is an equation to calculate what the standard totals are. Here I did it. So I said, let's just go back to the basics. We know that if I apply OLS, the difference between beta hat and beta is this. That's a C. We derived it many times and and so the variance of beta hat is just equal to the is different squared expected value. So I did it. I want to play this by itself and and I get this correct expression it was expected by the first one, epsilon, prime substitute V square and sigma square v, you get this expression. So that's if you say, how do I calculate the correct standard errors? If I apply OLS, but the assumption is violated is that you have to use this formula. Now, what about this formula? You need to know v. So do boots. I mean, you need to know v. That's a difference. That's why the formula, the standard formula that used, is incorrect, because it assumes that feeling good dry. So this is the formula that I did not have in any of the lecture. I hope I did not make any circular drawing here. This is just time. I didn't, kind of check anywhere, just thought about these otherwise I forgot to do that. Okay, so that's it. Any questions? Yeah, taking pictures quickly, I think.

Unknown Speaker  
Okay.

Speaker 1  
So today. So what I did last time I derived the GLS formula, or the GS estimate. And then applied it to a special case, the case of heteroscedasticity, or violation of homoscedasticity. And he found out that this matrix in the case of homosexuality, this matrix V is not an identity matrix, but it is a diagonal matrix, meaning it has only elements on the diagonal and all the off diagonal elements are zero. And so that's a violation only of homoscedasticity, because the epsilon for different observations still have zero covariance. So because of this, off the covariances between the epsilon from different observations. So so for the case of V diagonal, we found out that the estimator is essentially just weights to the sum squared arrow, and we called it weighted least squares. So weighted least squares is a special case of least squares when the V matrix is not I, but it is diagonal, and we get weighted least squares, and it's a violation of the homoscedasticity. You know, assumption is assumption of spherical ill structure. We divided it into two parts, the homoscedasticity and no autocorrelation. So we said no autocorrelation still exists, but there is no homoscedasticity. Heteroscedasticity. This V is a diagonal matrix in use weighted V squares. Why weighted? Because observation has nothing to do with something. It has to do with the variance of the error term, and we talked about different examples, like the data is based on some size. If it's a function of size, you have observation from small state, big state, etc, and or when you combine different data sets and the data, everything is independent, but different method of collection a different point in time. If you assume that the variance is the same, then v will be diagonal, but it will have different values along the diagonal depending on what the data comes from. So weighted least squares is something useful, and we talked about applying feasible weighted risk by starting with OLS, etc. Today, the purpose is to talk about off diagonal elements, so violation of the node correlation, so situation in which the epsilon for different oscillation are correlated. And again, what I will do so that will be, I will assume the homeless characteristicities Hold. I just use kind of focus only on the violation of the node the correlation, and assume that the diagonal element of V are still the same, are still constant, but the off diagonal element will now be zero. So that's a case of autocorrelation. And so I talked about, primarily the type of the lecture is time series, primarily when the observations are in time, like when you periodically observe and then so what they will come up will come up with a special cases again. And so I talk about autocorrelation, then talk about different world of autocorrelation may occur in practice when you have time series data, which will be later, my example, panel data, which I'll explain, and we will encounter again, panel data later in the semester, and then cross sectional data, which you have a data set at the given point in time, is called the cross section. Then I will talk about time series. And then for time series, I will talk about the photocorrelation, because epsilon at time t is correlated with the epsilon and t minus one, because things persist over time, right? And maybe you're not in the model, so now in the epsilon. And so I'll talk about the stage, the assumption about the autocorrelation. And one assumption about the autocorrelation is called stationarity, so I'll explain what it is. And then I will talk about one particular kind of autocorrelation structure called AR for auto aggressive and and then talk about working let's take the simplest possible model, auto aggressive model is called AR one, and then talk about, you know, how to how to test for it, and if You have it after estimate the model, it will be a special case that we can actually apply, even without, without having to calculate the V and invert the V you'll see later on in the lecture. So it's easy, it's a special case, but the general case is GLS, so that we are still talking about GLS. But what do we do when we have autocorrelation? So this is just a review now, right? But the glitter doesn't work. Why? Imagined that that

Speaker 1  
would be so addictive. And it so the this just a notation from the multivariate like Russian model. You've seen this. This notation explained before, and you've seen this the exogeneity assumption, this strict exogeneity assumption. We talked about, also weak exogeneity And next lecture. Next lecture, very important, don't miss the next lecture, because it's about violation of this assumption. So and the assumption about the second, second second assumption about epsilon, or what we call assumption three. And these two parts homoscedasticity, and this part now we have all the correlation. So we'll assume that homoscedasticity is fine, but there is of the correlation this the covariances are not zero. Okay? And so again, assuming that this still holds, the model is unbiased, consistent, if we have weak exogeneity, but it's inefficient, so we have these unbiased and consistent and we are fine, but we want to get the efficiency. So we need to use, want to use GLS, or we need to use fgls. We talked about it. It's we develop a consistent estimate of the valence covariance matrix, and then we achieve efficiency for some specific case of autocorrelation that we'll talk about today, the problem can be solved in a simplified form you'll see today. So now, when does autocorrelation occur? It occurs in these cases that are described and so when you have time series data, so I replaced, just to make it concrete, maybe replace the N with T, and the sample is t goes from one to capital T. So t are periods of time like I measure something, say monthly and then monthly data, and I have many, many months, I hope. And the number of months for which I have data is called capital T. In that kind of situation, we expect that from T to be correlated all the time. Why? Because always some omitted fractals, things that are not in the water. That's why we have an error term. And it's likely that there are factors like this also factor that consist of a function. So that's just example of where you can get time series data. Panel data is very important. And because panel data, when you do studies of the run and use like a transaction databases, or you can conduct multiple surveys over time, you all you have panel data. And also, if you do something that we call stated preferences, which are measuring subsequent slide. So in the planning data, we have n person or n entities for which we measure things over time, and so it doesn't it could be n states or n business establishment or M, whatever. So n is the usual sample size. How many of these behavioral entities, each one of them over time, t period. So, so the number of observation is total number of observation is t times n. We have a lot more data. And one kind of naive approach is to say, so I just have more data. I estimate the model. I use the ordinary least square like this. But then the question is, what about this epsilon? It's likely to be correlated over time for given value, for given n. So, so if I look at the particular entity, N, I observe the sequence over time and and then I have, I will have auto correlation, and one the most common model that to capture this auto correlation. Sometimes it's, it has different names. It's, it's called agent effect or unobservant originating. I'll explain what these names come from, or sometimes it's just called the panel effect, but it's a simple model. You just said this epsilon. When you have a panel data, this epsilon has two parts, one part, which persists over time, remain constant. It goes with N, with the entity. So if n are states, and t are ones, and then, so there are something about the state of Massachusetts that consists of a time, for example, this will be me and and then eta, and then will be like white noise, like kind of voice that we like like that M is homoscedastic correlation, or the assumption that we like to make about epsilon, and we can make about eta, but we cannot make it about me. So why? Why we cannot make it about me? Well, we can say that we have the same variance, but the error term consists for the same for the same n, if you can compare two periods of time, they will be correlated. What will be the correlation will be the variance of this so the variance of this persistent effect, or it's called agent effect, or called unobserved heterogeneity. Why it's called unobserved heterogeneity, because it explains differences between N and our behavioral meanings, right? So they are heterogeneous meaning. They behave differently. They have different epsilon. And the difference between meaning behavioral unit is explained by me, not by just by me, because it will make constant over time for each individual. So this leads to a particular estimation method, which is called fixed effects. So when you look at the reasons you're capturing autocorrelation, there'll be a discussion, usually, whether it is a fixed effect estimator or random effect. What's a fixed effect? Look at this knee. It's like it's the same for given individual. Can we estimate an intercept which is individual specific? Because for each individual we have T observation. So if, if T, capital T is big, suppose that this is like weekly data for many years. So we have hundreds. T is a hundreds and N, whatever n, n, hopefully is not too big. So n is not millions, but maybe a few hundreds or hundreds, maybe even less. So if n is small and T is large, then fixed effect is a solution, because you just estimate the intercept for each n, so it will be a lot of intercepts, and it's essentially, yeah, individual specific intercept in This model. And this is called the fixed effect. And the ADA need the assumption about the epsilon because the ADA has consumed the spherical structure, because this one is uncorrelated in same variant. So so by having an individual in this case, or an N specific intercept, V is now part of the model. This the value of is part of the model. And it's like, you say, this alpha n, or something like this, and, and we need to estimate a lot of intercepts. That's, you know, again, if any, if capital N is just maybe 100 and capital T is few hundreds, and that may be possible. And so when you observe a small number of entities over a long period of time, then this fixed effect estimator is simple to do. You apply ordinary squares.

Speaker 1  
But if t, if n is very large, or T is small, is okay, and this may not be feasible approach. Then you have to take this into account. It will be build a correlation. So we have to model the autocorrelation. We have to, again, the same thing that I just thought you apply. I mean, you may be able to use OLS, and if you want to calculate the standard errors you may have to appear to be or do good starting, but that's called the random effects, meaning V is not included in the error term, and it will may have different variance. We may have to estimate the variable if you want to do if you want to calculate the V we will have to, you know, maybe apply the feasible approach can be done as well. We take some assumption, but I will not get it right now. Okay, so, so that's in that case, we model this random term, and it's called the random effects estimator. And so we can do or less, we can do a fixed effect, or we can do random effect, which means we will model the variance, covariance structure the V, take the V into consideration. Okay, that's a panel data. We'll come to it back in in the case of display choice, of course, these are just examples of panel data, so repeated survey, transaction databases, where you, by the way, the panels can be balanced or unbalanced. When you do a survey, you have attrition response, so you cannot maintain every observation for the same level period and but if you have a transaction database, the customers come into this transaction database at different points in time. So they it's not as rectangular data like I described it here. You know, because you have n, goes from n to n, but then t, t goes from one to t, sub n or a different set of information for different individual and that's called, where was it? And this is like an unbalanced and so if you even, even if you start with n and you try to interview people over time, there is always attrition. People live, people die, people disappear, move to another area. Refused to reply. So often, what is serving the dog down repeatedly over time? They had to sort of free, enrich this panel over time, because there is always attrition, and so you end up with an unbalanced one less everything I said still apply. It's just for the simplification, to simplify his notation. I assume that it is like a balanced panel in my notation. Okay, so this is about findings. And finally, cross section, call section, suppose, if we have units at a given point in time, can you have correlation? So you see units is like states or zones. You remember, in my example I had was from Chicago, we have this TAC traffic analysis zones, and then so the observation come from different zones. And the question is, are epsilons from zones that are adjacent, correlated or not? Is it all correlated? This is called spatial or the correlation, so that the units and observation N come from different point in space to the geographical space, or for temporal space, we could be calling it Stein, right, but it could be also geographic space and then there could be correlation between epsilon and adjacent units that are affected by environmental situation or economic situation or market situation that affects everybody in an area in A similar way. And then there is one more situation in cross section, which we'll definitely be encountered in this class. We'll have lectures later on in the semester about something called stated preferences, both on stated preferences. I mentioned it in lecture one. I mentioned the difference between the field preferences and stated preferences. Do you remember I

Speaker 1  
they got of surveys in which, excuse me, in which we create hypothetical scenario, and in this hypothetical scenario, we ask those bonded to make a choice, to express how they would vote. But it's all hypothetical, right? And then, and so. So it's like it's done in a given instant of time, but it's like a panel, because if you do multiple experiments in the same survey, we have multiple observations for the same respondent, and so it's like a panel done in the single instance of time and subject that goes into different experiments. And we definitely expect some sort of an agent effect, because the same person applying, you know, a model for different different choice experiments, same instant of time, same context. There may be, you know, some tastes that don't change preferences anything. So it's an epsilon like this. So that's where a typical assumption for this kind of panel is this ancient effect model of call it autocorrelation, that they talk about. So, so you assume that what I said before, this kind of agent effect model, you assume that the epsilon can be divided into this agent effect, and like white noise or spherical noise assumption. So these are situations where you expect to have violation of no autocorrelation assumption. And so now we're talking about, okay, so what do we do? And so, because we have, because we assume that we have homoscedasticity, the variance covad is v we are going to write it like this. We are going to write it with ones on the diagonal. Why? Because it's multiplied by sigma squared. So these all are on the diagonal. So we assume homoscedasticity. But this is just this is a V where only the low autocorrelation is violated. So all the terms, it's a symmetric matrix along the main diagonal, and we call these covariances. We call them as rho, because to get the covariance, we need to multiply everything by sigma squared. So, so if you multiply it by sigma squared, and only epsilon is the same sigma, then these are correlations. Correlation is a covariance divided by the square root of the product of the variances. So the variances are all the same. And so these are correlation. It's covariance divided by sigma single square. So all these are correlations, and we know that correlations are number between minus one and plus one and and symmetric, or one two or one two, you see symmetric along the main diagonal to be properly this. Oh, I'm sorry, I was talking about it when it's all here. And so this is a definition, or is equal to the covariate correlation, covariance divided by the product of the standard deviations. So it's sigma square. And this expected value of this is a covariance, because all these epsilon f zero mean, it just equal to this definition of the covariance. So anyhow, so this is the violation of the no autocorrelation. So this is a V, the way v is going to look like, Okay? And now we are going to make assumptions. Assumption of such an idea. We make some assumption about this. Why? If we don't make assumption, you know, we have a lot of flows, right? It's equal to, well, we will have a lot of these correlations that's not necessarily count them, and so we're going to make assumption. So the assumption of certainty means that the only proximity in time between observation affect the meaning that at any given point in time, t we have a separation from other periods of time, t minus 1t, plus 1t, minus 2t, plus two. And there is a distance in time or proximity in time, proximity in time, between observations and and if the correlation is just a function of proximity in time, not of the exact time period. So doesn't mean that at time t if I look one minute next one minute before I proximity of one, I have a correlation, and the same correlation will apply if I look at T by T plus 15, whatever future point in time. And so that the correlation, this correlation only a function of proximity and time. So therefore the only function if we have the correlation between total time period t and t prime, then it's only the difference in time periods that affect the correlation, which leads to the correlation matrix having these scenarios. Because if you have a diagonal, the diagonal mean, it's a pair t, t prime, the same t equal to t prime, right? So that's the same time period. This is one. This layer is one time period difference, etc, etc. So if you assume that the correlations are only function of proximity and time, then the correlation matrix at these layers and and correlation only depends on distance and time and and what more we expect the correlation to decrease this time. So we expect that these layers will decrease as we move away, move away from the diagonal. And so this is the assumption of functionality. And so this is like a typical assumption, and because otherwise it's all time dependent. So it makes, you know, this matrix where, anyhow, it becomes time dependent, and then it's there are too many parameters. So this essentially is a reduction in the number of parameters, because how many parameters you have depending on the number of layers, how many layers you have is t minus one. Okay, so now still you have to say what the rows are, right? So we know we have layers, so we have now T minus one parameters in this matrix, right? So we reduce it by making the assumption of homoscedasticity, making the assumption of sectionality we are we now have this kind of matrix where we have this layers, but since we have t minus one layers, we have t minus one parameter and and then the question is, okay, still t minus One is a lot of parameters. And so the different proposed models of the relationship between layers, and the first simplest model is called autoregressive. It says that the epsilon at time t depends on the previous epsilon and up to p, this is like an autoregressive of order p, so it means that epsilon t depends on the previous P epsilon. So you start this with t minus 1t, minus 2t, minus three, etc, and plus white noise like this, independent of a time, etc. And so this is called an auto inversive model. So in this model, we need to estimate, in order to estimate the matrix we need to we have a variance of this thing, and we have the laws. So if p is small, then you only have a few parameters. That's idea. So the idea is,

Speaker 1  
instead of having t minus one layer and t minus one parameter, which can be a lot of parameters, and we can simplify it by using this autoregressive model and like, if P is one, that's the simplest model that we're going to work with later. Then there's only one parameter, because p is one, then just epsilon t is a roar times epsilon, t minus one, plus some other noise that is independent over time and at the same variance, etc, moving average is given here. I'm not going to talk about this if you take a course in terms of time series forecasting with time series data, or there's really not much more than where the focus is more on the epsilon, as opposed to the model, then you find a lot about moving averages and a combination of AR and MA, which is called Arma. And then there's all kind of you take differences in epsilon, and you get something called Arima and all kinds of other models of outer model using a fewer number of parameters of the variance covariance matrix, or the correlations. And so I will not talk about this. And I mean, I'm just going to give you a simple model for the correlation to show how it can be done, and it can always be done with GLS, but also a simple way of dealing with it. So that's what we're going to do next. We're going to look at the AR one.

Speaker 1  
So what does it mean? AR one, it mean? So let's look at this modeling detail. So assume whatever we can have multiple X when I write it, write it here, simple, just 1x but any number of x is here, but we have an epsilon. And on this, for this epsilon, you make an AR, one assumption, meaning that epsilon t is equal to some unknown parameter, rho times epsilon t minus one plus b. So we just, we now have only two parameters, right? We have sigma squared. So the variance of V is sigma squared, sigma v squared we need to calculate the variance of epsilon. We know that the correlation epsilon will be correlated because of volume, and we assume that that V and E and epsilon are independent. So because me is just like a white noise. So, so what's the variance of the epsilon t? We call it sigma square. It's the AR model means that this that's not D, depends on the previous one, so we can write it as it's expected this square, right? The expected of this square is equal to rho square times the variance of epsilon t minus one, we assume almost elasticity. So it will be sigma squared plus sigma v squared. And so sigma square is equal to a function of sigma square. So we can solve it. And we solve for sigma square it's equal to this. In other words, assuming that this is a model, and assuming that V is is assuming that V of V of, I should call it V, let me and epsilon are independent, and assuming that the variance of me is equal to sigma d squared, then we can say that sigma squared is equal to this expression, but it's constant, because This is a constant, and rho is a constant. And so this is a variance. So that's fine. So we have a sigma squared multiplied by a matrix that looks the V matrix. So what can we say about the V matrix? It says, as we said before one and then we have layers. And the layers are going to be just a function of one parameter. So the first layer is just equal to O times sigma squared, because if you multiply epsilon times epsilon t minus one, if you multiply this the equation for epsilon, you get this equal to all sigma squared. So this one is sigma square. And this one, I'm sorry, not sigma square. Sigma square then wall. Sigma square is outside. So just all. And if you have any number of distance s, the s layer. It just all to the power of s. So this is one real difference. Is one, two, the three, et cetera, to T minus one. So all these V majors under the assumption of a or one, we only have one parameter v. Well, V is a function of just one single parameter. It has a sigma square where it depends covads matrix as a sigma square outside which is a constant, same as standard, the basic or less model, right? Because that assumption so. But now we know v, we can V is just a function of form, but we still need to estimate all. So if you will to use GLS, step one, estimate all, find that all, and then apply GLS using this matrix. So this will be a GLS approach this problem. And in step one, you need to estimate visa at all, somehow. And how do you do that? You apply OLS. And so you apply OLS, and it's considered. Consistent, right? Or less is okay, and the residuals are consistent. So to this just says that the residuals are consistent. So I can just run a regression like this. Can you have a regression? Can I regress? I have T observation of epsilon t? I just under regression. When I regress every epsilon t on the previous one, in order to one is regression? How many observation will I have? Can I will I have copies of T observations? Think about doing it so, so the one over the ordinary square. You know how to do that. Ignoring, ignoring the autocorrelation, you get residuals. How many residuals do you get? T minus 1t? By T minus one residuals, I have T observations. I

Unknown Speaker  
this is my bonus, right?

Speaker 1  
Isn't there each observation in the previous? Russell from the Bucha, you're singing ahead. I'm going to come here for the moment. You're singing about having loud variables. I don't have any loud variables. So often walking through a discussion this time since later among the x's, you included log variables. And there are two kinds of log variables, log independent variables and log independent variable, log L Angie, lag variables, I did not take it into account. Now. Account and but so what's, what's if I had a log t minus one, then I want to run this regression for the observation one. I don't have data for period zero, so I'm losing an observation. So that's the T minus one line. Yeah, so for epsilon t, though, if you're looking at the equation for epsilon t minus one, I did not ask about this regression. I said, When you listen this one, how many epsilons do you have? But then when I want to, want to take this estimated episode and I want to run this regression, D minus one oscillation. Okay. So now, now we are in agreement. So, so I will run the oil less step one two, and oil less, and you and, and I get usually t epsilon if we don't have lagged variables. And we're going to come back to we're not going to finish this lecture before talking again about log variables, because if there is a lagged independent variable, like x, t minus one, then we will lose an observation, and that's it. But sometimes you have the dependent variable, and that's at the complexity. So large dependent variable means I say, I have y period T and I think it depends on y t minus one, because the way demand is evolved from period to period is by adjustment. You don't start from scratch. Every time period you have a demand at time t minus one, then it is Y t minus one, and then y t is some sort of an adjustment to Y t minus one. So this will make a case for including when we write a time series model, when we write a G model, then the explanatory variable could have log independent variables and log dependent variable, meaning y from the previous period that's will come to in this case. So if it is previous time period, no additional complexity, it just lose an observation. That's it, the same way that we will lose an observation to our dislocation. Because If, on the other hand, if you already lose one observation, we would lose another observation here, where we run this regression. So if we had a long independent variable that's X t minus one in the equation, it says, Well, suppose the price takes a y to have an effect, so the price at time t minus one also still have an effect at time period t so that's a case to include a large independent variable. And so anyhow, so I may not have for this regression, I may not have all the observation, but T should be sufficiently large so we can afford to lose one or two few observations, and we write this regression, and then we write this regression, and this gives us an estimate of four. So from this regression, it's a regression where these are the estimated epsilon, not the two epsilon, right? So and the knee is not the original knee, because we replace the epsilon with estimated epsilon. So this one, we call it me tilde, but then, yeah, we can rather irrational that, and you can get an estimate of four, and you can do a T test on the estimate of four. So null hypothesis is that or is equal to zero, or the null hypothesis could be that rho is equal to one perfect correlation between anyhow. So we can do a T test, and this will be, will give us an estimate of four we can say, is it significantly different from zero, if significant equal to zero, it's an indication that we do have of the correlation. So overhead from this regression is consistent. The standard error is problematic. Residuals of different type cuts are correlated because they use the same data which is a function of the same data. So, so it's, it's, you have to do bootstrapping if you want to calculate. So there is also a test called the building Watson test, and you test. This is not critical. If you are with people that are using a lot of time series data, mostly economic time series, then they will often perform tests. It's called tube in Watson. I will skip it. I will. I will not. It's all in the slides, and that's a way to calculate it and and then there is a way of calculating the significance the new language is, and it's designed to circumvent the problem of the the standard error, you know, like doing a details for this regulation, is problematic because the standard error is not estimated correctly, but you can do it with bootstrapping if you don't. So the day when bootstrapping was not as easy. There was this test called the COVID Watson. So I'm not going to go through so, so that's Wait a minute. Where am I? Okay? So, so far, we know how to do GLS, right you Oh, fgls. You do fgls by, by running oils, then estimating oil, and then you can do the GLS because and construct the V matrix using this formula, this thing V matrix, you can do GLS. Okay, now, what's the problem? The problem is, when you have a large dependent variable. So if you have a large dependent variable, and you have serial correlation like that, and you have an AR, even a single border like this, and then we have endogeneity. Why? Why do we? Why do I say exogeneity? So when I say endogeneity, it means that the assumption that epsilon is exogenous cannot be true. Why the assumption that the expected value of epsilon given x or that epsilon and X, unquote and y, these not two well, because the x now include y minus 1y, minus one. This equation here, you can also write it for y minus one, in y minus one, what the L term called epsilon, t minus one and epsilon t and epsilon t minus one are correlated. Therefore, epsilon t and Y t minus one are correlated, right? So epsilon t minus one, which is correlated with epsilon t affect Y t minus one directly, and therefore Y t minus one and epsilon t cannot be uncorrelated, so we don't have not even weak exogeneity. So strict is clearly violated, but also weak. So running a regression on this gives her ears on the correlation, and you want all this on this, you get the estimates are not consistent. So you don't get consistent epsilon. So what I described before cannot be done

Speaker 1  
when you have yt minus 1e we'll have to do something else, and something else will be I'll teach you next time. So that's a situation where this kind of two stage start with OLS and then do GLS cannot be done because the OLS is inconsistent, so the residuals that you get out of it are not consistent, so you cannot estimate V's consistently. And so there is, I mean, there is, again, an estimator, a test procedure, extension of dog in one so that you can use to test, in this case, to see either correlation, either auto correlation. But then there's a simple way of dealing with it that I will describe it. So first I'll talk about not doing fgms And and then I will have to come back to the Y t minus one, next one, and maybe we have time for them. So assuming that there are no log deep and available, there's no y d minus one, OK, so we talked about how to do an fg of less in this model. But there is something simpler. It's known as a COVID or COVID procedure and but it's a generalized the idea here motivate the use of differences. So instead of working with YT and Y t minus one, we're going to work with differences. So why not? Why model this y that's more than yield to yield, period to period differences in y. But if we know, oh, then we can have sort of generalized differences that can be even more useful, as you'll see. So suppose that I give I know rho. I can multiply the same I write the model for what period T. I write the model for period t minus one, I multiply it by rho and take the difference. So on the left hand side, I have Y t minus rho times Y t minus one. Can I do that? Yes. And so I get on the on the left hand side, I take the difference, I get this, y star, Y t minus rho, Y t minus one for alpha, I get alpha right, minus alpha times rho, which is alpha times one, minus rho for beta. I will end with x star, again, X t minus r, x t minus one. And for the elder I get epsilon t minus O times epsilon t minus one. But epsilon t minus o, epsilon t minus one is equal to this knee. So this equal to knee t. So by taking the difference between periods, I get an expression. Look at this equation. I have, I have an error term. I mean the x's are exogenous, so we have exogeneity. The me now satisfy the assumption on epsilon. It has all the assumptions that we made on me. Now applying me is now the error term of this model, and so we can do ordinary least squares so we don't have autocorrelation. So we got rid of the autocorrelation. But in order to do that, we need to calculate y star and x star, and in order to do that, we need to know one. So very often, if you say Roy is very high, close to one, then let me just make this difference work with differences. And then I think this differences is always equal to one, but is not equal to one.

Speaker 1  
So, so that's the idea is to this procedure, let's say it's known as coconut organ, for the angle, not one. We start with oil s. We know that oil s gives us consistent estimate in this game. So if we had a dependent that deepened variable, that deepened for everyone that we couldn't do that, but so we start with OLS. That's step one, calculate the residuals, epsilon star, then run this regression. Right in this regression, we will use at least one observation and for which we estimate wall head. Now we can take the differences, calculate y star, calculate x star, and on this regression, and from this regression, we can estimate the alpha hat and beta hat, I mean, and then we can iterate. We can go back that's we know alpha and beta. We can calculate new residuals, because we have new estimate of alpha and beta and new residuals, we can calculate new world and new or we can calculate step three, and so we can repeat step one, two and three until convergence. It's really not so long, not so essential. So we lose one observation, which we don't if we do fg of us, we lose only one observation to estimate the law. But once we have the law, we use all our T observations, right? Because we have this matrix V, which is t by t. But this is like you can see how we just use ordinary squares. You estimate the log here, so step one and two is the same as in fg of less, but step three is different. It's just we calculate these generalized differences, y, star, x, and now we can just do again OLS. So we don't have to do GLS. We don't have to calculate V in V etc. You just do OLS. Simple case and and the simple case is based, well, it's based on this idea of calculating these differences. And so sometimes it may when we kind of find out, we want to know what to explain the period to period differences. So working with these differences is very useful sometimes. So anyhow, so that's the coconut organ procedure, and this conclusion. So other correlation kills in several cases. Under Other correlation, the model is still a bias, consistent, but inefficient. One can use the consistent estimator to gain efficiency and under meaning to do it fgms With AR one, it's easy to test for this autocorrelation, because you just for any kind of AR, it doesn't have to be AR one, because for AR process, you just get apply or less, get residuals, and then address epsilon t on so t hat on, T minus one, epsilon t minus Two, if you think it's l2 so you can so so under year one is very simple. You can easily test if the correlation exists, and then you can also correct for it using either fgls or using this simple coconut organ procedure. The other possible assumption about the serial correlation, but much more complicated. So Al one is kind of standard model that is like a starting point for modeling of the correlation in time series. But you can do al two, Al three, etc. You can extend it. You can let the data tell you what it should be, because once you have this epsilon, you can then you can regress so T on s and t minus one, so t minus two, so t minus 3c, was significant. That's for better data, tell you what should be the degree of the autocorrelation. So that's why this auto industry process is very useful. That's all I have to say and any questions that we have time For questions, yes, can you go

Unknown Speaker  
back to Slide? 22, slide 22 sine, 2222

Speaker 2  
So here, if we were interested in t minus five, we would just put in time

Unknown Speaker  
minus 5t. T minus five.

Speaker 2  
Yeah, we would just plug that in here. We wouldn't do like multiple equations,

Speaker 1  
subtracting from the person, right? I'm sorry, we're interested in D minus 5t, minus 10. It doesn't matter. I'm sorry. Why are you interested in D minus five? Just hypothetically. Oh, hypothetically. So, the model applies to any t right? So I can write it for one to capital T, and I can get the fitted value right. So it's a regression model. So 5t equal to five is just the fifth observation. So I have T observations for every observation, you can write the fitted value which is given by this as in standard reduction. There is no difference. The only difference is that now, after applying O and S and we have y d minus y t, y d times minus y t at is equal to epsilon d hat the regression. Where was it? When you run this regression, you see, we run it on epsilon t, epsilon hat. So I calculate this epsilon i add for every t. I mean, this is maybe not well written, because I should put a comma and say t equal to one, all the way to capital T. So I calculate this for every t. And when I run this regression, I have t minus one of the T minus one observation. So observation number one drops out because for one, I don't have epsilon Z. Did that answer the question? I don't think so. For AR to what would factor two. That's not complicated yet. Before l do I have rho one and rho two? So I multiply this t minus one by rho, 1t minus two by rho two. Let's subtract the y t minus r 1y, t minus one minus one to Y t minus two if you want to do like a COVID procedure, but you can always do GMs. But I think there's still an answer which keeps Oh. Oh, so you wanted to go all the way to five. Yes, he went step by step. Yeah, but that's what you will do in practice. You will say, I'm going to test for the AR process, and you keep increasing the grade says, Does it still make sense? You know, he's a modern and find out what is AR. The only AR pauses in this that wasn't till later.

Unknown Speaker  
Usually AR one is enough applications minute by minute, then maybe five is not enough. Right?

Speaker 1  
Depends on it depends on the context. Any other questions, hopefully easier, easier for me to understand, still

Unknown Speaker  
trying to understand what this B matrix actually is. V

Speaker 1  
this is what you represent. I left my slides and came here, until which hopefully I thought was helpful. V is just a replacement for I. Until now the assumption, the assumption of spherical l variance, said v is equal to i, so just generalize it. What no longer violation, violation. So the variance, covariance of the epsilon definition here, the epsilons may have different variances. Can have covariances, anything goes. V has to be a proper symmetric, positive definite, but, but many, infinitely many possible vs. So I was just a very restrictive special case generalization

Speaker 3  
of that matrix, because it can't always it might not always be. I Is that

Speaker 1  
correct? That's the idea. The idea was that the standard OLS assumption that gave you regards mark of Theo, and give you this equation here is equal to i. So this way, in this class, we could have done violations, because it's not such a violation of it's not like but we started with the assumption, and therefore it's a violation of the assumption, but it's a more general case, clearly. So the epsilon can have any variance, covariance matrix, legitimate first like is V and I gave you Now two important examples of v, v diagonal, the heteroskedastic one, and V for A r1 so that's

Unknown Speaker  
but it depends on the application.

Speaker 3  
Slide nine, I think you spoke a lot about The this agent effect,

Unknown Speaker  
yeah, I did.

Unknown Speaker  
Yeah.

Speaker 3  
I think I'm trying to understand how that's linked to this comment you made about this preferences versus revealed preferences.

Speaker 1  
Oh, you want to know this, which one? Well, I think you talked about this, the expected agent effect. I talked about the agent effect. That's why it said and then I said that stated preferences is like panel data, okay, okay, it's likewise statement. Why stated preferences is like panel data? Because it's panel data, you have an entity observed multiple times, stated references, you record span and then you conduct multiple choice experiments for the same approach. So it's like, it's like a panel

Unknown Speaker  
COVID

Speaker 1  
statistical point of view, it's A pattern data I see, okay. I

Speaker 1  
so quiet. So, okay, so what? What comes next? So the next lecture is, is about the violation of exogeneity. So that's a case where we have endogeneity. We encounter a important example of endogeneity today,

Unknown Speaker  
the black people let us know. I So this case, this model,

Speaker 1  
this model is the epsilon auto correlation. Then this means the correlation between epsilon, theta minus one, that's a violation of exogeneity, strong or strict or weak, and the only support you starting with OLS, does not work. So the fgls, you cannot apply it, because you have to first deal with endogeneity, because you don't get consistent estimates. If you apply orders to this equation, the results are not consistent. So the next lecture is Martin Luther and similar case where we have this correlation between the epsilon and right hand side random. So that's why it's a critical lecture. And make sure you understand it, because it's often ignored, yet it is so important, okay? And by the way, then next week, we start with discrete choice, and then there is a midterm exam, and then you get a spring break. You arrange the midterm before the spring break because you don't need to think about this class. The midterm is out of class. Out of class. The

Speaker 3  
are due on the same day. Based on this, one of those, is that correct? Yes, yes, yes, it's due Monday. I think that was on the syllabus. But ask Moshe

Unknown Speaker  
to change if you want my

Speaker 3  
face. Yeah, I think I texted you about it. So one thing I want to talk to you, I did study like limited stuff you mentioned.

Transcribed by https://otter.ai
