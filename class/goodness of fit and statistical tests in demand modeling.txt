Unknown Speaker  0:00  
It's also in this question

Speaker 1  0:07  
review session, and I want to make sure and statistical

Unknown Speaker  0:19  
tests so my voice, I covered the basic what I didn't do last Time is go back review

Speaker 1  0:39  
the assumption for assumption, but for the multivariate case. So statistical terms, these three items first and then in the time, we go back to the previous lecture and talk about the assumption. Why? Because what comes next? Our violations. Starting next week, we will have a sequence of four lectures, four lectures dedicated to when these assumptions, remember the full assumption. This assumption is going to start off and some feedback is key for

Speaker 1  1:40  
and some feedback. Okay, so I'm going to this review. I may not get to it, particularly the slides of them. Please review specific questions available to answer it in the beginning of next time, today, anytime.

Unknown Speaker  2:13  
This is for you. So this is a review.

Speaker 2  2:21  
Yes, go ahead, just for the recording. Oh,

Unknown Speaker  2:26  
there's a mic.

Unknown Speaker  2:28  
Mic is on your desk.

Speaker 1  2:37  
We need the microphone. I

Speaker 1  3:02  
Excellent. Easy for me, easier. So goodness of fit. This review, the medical properties, statistical properties, and so we know the estimator, the assumption, and now, how do you evaluate the regression? Is it good? Is it bad? So that's the out of model building. How do you know that you have a good model? Because there are alternatives. Right? You can include different sets of experimental variable. Include them non linearly, linearly, all kinds of ways in which you can specify the model. So how to spell, how to choose between different model formulation, what variable should be included? Should include interactions between variables, like is effective for price depends on income. How does it depend on income? Etc? Should we use a logarithmic transform? Some logarithmic, or sometimes exponential transforms are very helpful to capture more linearities. So the basic answer to this is the reason. The most important is your knowledge, common sense domain knowledge that you need to decide whether the model is acceptable or not, because the estimation results the model, the parameters that you estimate have meanings, and these meanings need to be trustworthy. You have to be able to trust the model we talked about it. So this is a top criteria, but there are three tools that can have. One is goodness of faith and statistical tests of specific hypothesis. Maybe there is some suggestions. We talked about hypothesis testing. So first, goodness of fit, the standard measure used is our square and and the the best way to think about our square is using this formula, it's a correlation squared between between Y and Y hat. So you observe y then the model. Once you insert in the model beta hat, you can calculate y hat, which is x times beta hat, and so you have y hat, and the correlation between y and y n is the best way of thinking about fit. Do they do match how they're related to each other? And clearly you expect it to be positive. If it's not if it's negative, then something is completely wrong. So you expect it to be positive, this correlation, and you square it as well, and and that's why it's called the r square. So it's a correlation square. Sometimes it is called multiple correlation, but because it's a correlation of y against a linear combination of multiple x's, right? And so, and this is a formula for the correlation. And correlation is equal to the variance divided by the product of the square of the square root of the product of covariances, and I'm sorry, of the variances. So this is the variance of y along the mean of y this is, of course, variance is being divided by n, but and this is the variance of y hat, and when you subtract the mean of y n, now the mean of y n is the same as y8 in the case when you have an intercept, if there is no intercept, then these two are different. So this is the mean of y n, and that's a variance of y n, the mean of y n, and there's a covariance between the two, between the Y and y8 and so by normalizing it, by taking the covariance, normalizing it like this, we get, I mentioned square root, but there is no square root because this is square and this is square of the covariance, so it's a Square correlation. But anyhow, the so this is a covariance square, and this measure is unit free, and it can be shown to be between zero and one. It's equal to one, if you have a perfect fit, if y is equal to y for every observation and clearly we would like to maximize it. That's and, and the least square idea is essentially it maximizes this L square. It's the best L square. It's a least square solution. Okay, now however, elsewhere is often computed differently, so the world of software where elsewhere is computed differently. And I gave you this definition because it's easy to compute. You may need to compute it yourself sometimes, because in the software, alpha square is calculated differently. And the idea of a calculation of R square is to take the total sum of squares. This is a total variation in y, y divided by y bar square. So this before there was any model. If I look at the distribution of Y and calculate the variance of y, that's how I would calculate the variance of y in my data. This can be divided into these two sums, and and the sums. The way to prove it is similar to the you'll never add an equation showing that mean squared L is equal to pious square plus the billions. And it's the same kind of proof you basically add and subtract y hat, and then you proceed so the dimension. And at some point, you make an assumption that y bar is equal to y hat. If you make this assumption, come to Y hat bound. And if you make this assumption that y bar is equal to y hat, essentially you see assuming I have a model with intercept. So there is an intercept in the model. So, so if there is an intercept in the model, what we this equation is two. And so the see the model with the intercept. And so a lot of software assume that you have to include an intercept. I mean y eliminated intercept. And therefore this equation usually hold in most cases. And so this is the variability of the fitted value on y bar. And this is viability of Y Y hat. So the difference between Y and Y hat is the residual. So this is the sum square residual. This is what we are minimizing. So when you calculate the least contribution. This was our objective function. We brought this to a minimum. And so it turns out that I can take the total probability that I have an intercept, and I can write it as a sum of two. One is the fitted value around the average. And so therefore, so this is sometimes called sum SS, for sum of square of regression. Or sometimes, as it says here, unfortunately, explain that. Fortunately, in a minute, it's called this explain sum of square. So it's starting with the sum of square of y around its mean, then I'm replacing y with y hat and the variability around the mean. The same mean is explained, and this is unexplained of the error. So some so the name, so I will use the convention top line SST equal to SSR plus SSE. But in some textbooks, if you use textbook, you may raise viability. And some people, they would call this SST, but this will be SSC, this will be SSL, y, E for, explain why our for C join. So maybe confusion. So, so when you see these SSDs, there is no confusion. When you see SSL and SSC, it may be the other way along.

Speaker 1  12:40  
You can figure it out. And so once I have this decomposition R square is then sometimes refer to as a coefficient of the termination, because it says the proportion explained. I start with the total probability which is explained, and this equal to one minus. And you know that we are the objective function is SSE. Therefore, you know R square is the best you can get. What you get is based on is you achieved. If you solve for the minimum of some square level, you get the maximum possible R square. And so that's the way, very often our square is calculated, because the program calculates the sum of squared L. I mean, that's the objective function, and this is the initial sum of squared divides of y. So if the model is an intercept, this calculation is the same as the one I gave you before we call it. So it's easier to calculate. You don't have to calculate some covariance. You just have to you have the sum of squared L you have the SST, and that's the equation that is used in host software to calculate Oscar. And so there is one kind of situation that can often tell you that if you just take the intercept out then, and if you use this formula elsewhere, can become negative. So that's just when you calculate it as a correlation. It's never negative a correlation square. Okay, so what does the limitation of this measure of fit? It's essentially the objective function. So it's a value, let's say, but now if you compare alternative models, you said we compare the objective function and but it has limitations. It has a limitation. The first one is R, square is monotonic in KK is a number of angle parameters, the number of coefficients, the number of betas. And so if you just add a variable, Oscar using cannot decrease, it can only, only increase. There may be a real patient where it will stay the same, but its probability is essentially zero. It's usually this increases. And if you saw a way observation, suppose that you have data, and you know already sawing away observation is not a good idea. You lose efficiency. But if you saw observation, how negative is actually, as a matter of fact, if you saw a way, n minus k. Observation, you leave only K observation, then ask, square would be one. So O square is one. You have perfect fit in. The line goes through all the points, but it's clearly can be nonsense. You just you have some epsilon. It fits to the epsilon. So now another thing that you have to worry about, what will happen with us? Well, if we transform it in the planet variable. So for example, if the dependent variable was y, and I replace it with log y. Another regression, again, the alpha square is not comparable. Why? Because we changed the epsilon. We don't form the epsilon as well, and SSD is now SSD of log y. So So, if you use a regression problem, and you modify the dependent variable and you want to calculate our square what you have to do, you have to get estimate of the model, go back to the same dependent variable, right? If you take the load and you have to exponent, exponent, the right hand side, then you get Y again, equal to something, you get y hat, and you calculate the correlation. So that's what would happen if, as well, if we transform the dependent variable, Oh, that. Oh, sorry I misspoke. I write it too quickly. This is independent so if we, if you change independent variable, it's fine. Nothing changes what I said, I misspoke about this one. What would happen if, as well, if we transform the dependent variable, dependent y, left hand side. I should have seen left hand side, anyhow. So if we change the variable on the right hand side, meaning explanatory level, that's a good idea. He said, should the variable be transformed with a low exponential changes? Maybe I included linearly plus log, that's fine. And of course, the more these you include, the better will be out. So, so. But if I take a variable that was linear and I put a log of it, then ask, well, I can use it. Tell me which one fits better, that's okay. But if I include both of them, then ask for will be greater. But it's possible something shouldn't be done. That's the kind of things you may want to do in comparing different models, but you have to be careful that ask for will always increase when you add more parameters so what can you do about that? So this is the one about the dependent variable. There's something to worry about it, because then it's not compounding. So if the one person estimates the dependent variable is y, and the other person estimates long y and they compare the output. R square is not comparable. You have to calculate it. Okay. Next, so this was a mutation. Next, what do we do? How do we address this problem, that R square is monotonic in k this one, we know how to address. You just don't throw away data, except if there was mistakes, if you can clean data, because you know the mistakes, you know itself and and this, I will tell you what to do about it, but the first one to the way to address it is to use adjusted elsewhere, and we do the same in a maximum likelihood, we have a measure of fit, and we have an adjusted measure of many ways in which this adjustment can be done is basically penalize the fit for the loss of degrees of freedom. Degrees of freedom is the difference between the number of observation n and the number of parameters to be estimated. So you see a lot of time, you'll see this n minus k, n minus k. So before we end the model, if we just said y and we estimate y bar, then the degrees of freedom are n minus one. Once we estimate the model, we use the data to estimate k parameters, and the remaining, so to speak, degrees of freedom is n minus k. And so the menu, so the fact that as we increase k, our square is increasing to go into it, we penalize the the fit for as k is increasing, and that's the penalty. The penalty is when we calculate our squares, one equals one minus SSE divided by SSD, we divide SSE. SSE is calculated from the regulator by n minus k, and divide SST by n minus one. So, so this gives you a different measure that is not monotonic in K, because at some point if, if you increase k and the gain in SSE is small, then the fit can actually begin to decrease. So if you start you're adding variable to your regression, it will made out square bar will reach a top, and then it may begin to increase when you when you add variables that provide more explanatory power. So that's our square bar. So it may decrease when you add a regressor.

Speaker 1  21:52  
It always increases if a regressor with a T star t ratio is greater than one is added. And so that's the way you know if it's going to in this case, if you're going to help on the track from this measure. And it's useful to compare models estimated with the same data. So if you have the same data right and and the same dependent variable, same data, same dependent variable, same number of observation. So that means the same data, same data means same number of observation, same dependent variable. Then R squared is difficult to use because we know adding more parameters will increase r square. But then you can use r squared, and in that case it's useful to compare to say, then you can say one model fits better than the other. So it's useful to compare models estimated with the same data and the same dependent. Okay, that's basically what any questions about goodness of fit. I

Speaker 3  23:03  
Yes, can you just say again, one more time? Kind of, what is the logic behind for the regular r squared? You know why this comment about the perfect R squared with n equals k with meaningless? Like, what do you mean by meaningless? What do you mean by signal? Meaningless?

Speaker 1  23:21  
Yeah, meaning that it just fits to epsilon. You get the line that goes through some a bunch of points, and the points we know you need to fluctuate around the line, right? There's an epsilon. And if you take two points and just connect them, you're connecting to an observation to epsilon. So it's valid only for these two observations by by having a cloud of points and finding the kind of a central tendencies to the cloud. Then it is can be generalized to more points if I have more observation in the future. But if you fit the line only for two points, you put a straight line. It goes for these two observation, but everybody else will be off the line. Okay,

Unknown Speaker  24:10  
yeah, it's kind of similar idea of like overfitting

Unknown Speaker  24:13  
a model. Yeah, absolutely.

Speaker 4  24:17  
So n is the sample. N is the size of the sample. K is the size of the population.

Speaker 1  24:24  
N is the size of the sample. K is the number of coefficients. Oh, thank

Speaker 2  24:36  
you. Yep. Point three or point four. What does it mean? And in the last year, last semester, class is a useless parameter. Can you explain it? Okay?

Speaker 1  25:04  
It's useless for often, for the way people use it, but it is,

Speaker 1  25:16  
I'll tell you when it is useful. R square is useful, but it has the same data, the same dependent variable and the same number of unknown parameter, then you can use R square to say one model feels better than the other. Relatively adjusted R square is good to compare model with the same data, the same dependent level, but the number of parameters can be can be different. But to compare models, what? What else square or bar square is not useful to is to say this model has a good fit or bad fit. There is no scale here that says when R square becomes good and when it is bad. So if it is I can have a very good model with R square equal to point one. So suppose that you have a million observations, and you fit a model, there's a cloud of points, and you get a nice model, and ask, why is point one standard errors as parameters are small, relatively small. It could be an excellent model. And then you say, Okay, let me slow. Let me keep only 10,000 parameters every observation and estimate the same model, and our score would be great, but likely to be better, but you have a worse model in terms of applying the model, generalizing any applying the model to predict, to analyze. So so our square is useful to compare models which are same. To compare models, same data, same dependent variable, same k. Our bar square is useful same data, same infinite variable, different level, final parameter, but only to compare models. To say there is no scalar between zero and one. When one is perfect, zero is clearly useless. But between to say we have a good fit or a bad fit, there is no such thing.

Speaker 2  27:41  
So in papers, some papers, writing such a model, explain 70% of the variation. It's a very good

Speaker 1  27:52  
model. It explains 70% of the variation. Is right. But to say, therefore it is good, like that's that's meaningless. To say

Speaker 1  28:09  
the EXPLAIN 70% of the variation is on the definition, so you just repeat the definition of our square. But to say good? Why 70% is good and more than 60% is bad. There's no such thing. So when I use the word useless, I was afraid to that.

Speaker 1  28:42  
Any other questions about goodness of fit? Good scope to get questions. Okay, confidence in the vial. I mean, we talked about confidence in the vial. And kind of this section, I don't really want to spend too much time on it. It's the same thing about confidence interval that I mentioned previously when I talked about it in the review, but in order to create the confidence interval, we have to assume we need to know the shape of the sample distribution so we often if you assume that the epsilons are normally distributed, then, as I said before, beta hat is also known with mean, if it's if it's if assumption two is only strictly exogenated And it's unbiased. So it's nowhere on the turbine, and it has this variance covariance formula. Now I've not shown you this variance covariance formula so far right? I did not. So you're saying this for the first time, correct? If you don't know whether you saw it or didn't, then it's okay to you didn't. It's it is really an appendix of a previous lecture and but I did show it to you in this case, when I went to regression in the scalar case in the first lecture, regression, I did show you this formula. This is just the extension to multivariate case. So if you remember, in the bivariate case, we calculated S squared divided by the sum over n x minus x bar square. That was a formula. This is this formula. So this is a here. Beta is a vector. Beta hat is a vector, beta is a vector. Sigma Square is a scalar. And x prime x is a k by k matrix that you can invent and, and this is if you make assumption, remember, in order to derive between an assumption one of course two and assumption three, and, and so this equation I told you at that time that it's two only if assumption three is two, assumption two and assumption three, assumption about the distribution of epsilon. The assumption two is that the expected value of epsilon given x is zero, meaning that epsilon is independent of x, the mean of epsilon is independent of x, and assumption three says that the variance of epsilon is constant over epsilon and the epsilon are uncorrelated. Under this assumption, we can then we have a formula that they showed it to you in the bivariate case. For the multivariate case, it's in the appendix in that lecture, and I'll show it to you later. And so, so this is for the powers in a specific parameter, parameter k. Then this assumption means that this parameter, minus its store value, divided by its standard deviation, under the assumption of normality, will be standardized over nor 01 so this it says that the zillions of beta k is the KK element of this matrix, square root multiply. So this means the diagonal. This is a matrix, right? It's a K by K matrix. The diagonal element of that matrix are variances. So that's what let's explain this equation. So, so so we know the distribution now of element normality. Assumption is assumption for and we can calculate the confidence settlement. So this means and, and we replace single square here with s square, and S square is estimated in this form. Okay, so once we know, once we have normality, we can then, I mean this slide that I can skip it, it just about the difference between the t distribution and the normal distribution. So because when we replace sigma with S, then this variable itself being normal, 01, it's t n minus k, but T n minus k approaches normal as n goes to infinity. But N doesn't have to be very far for this to reach normal to be well approximated by no one for n equal to 30, it's already there. So, so therefore we never really use the t distribution. It goes back to the time when it was relevant. But anyhow, so instead of having z, it says he decides, well, we should do it with T and, but this is just a minor, okay, so, so you know how to construct tests and test, which is called a T test, for the same reason, and as I you already talked about it. In the case of the tablet for null hypothesis, it has like a single section like beta gain equal to some value, externally specified value, Q. So q is often 01, and then the test statistic for to test this is given by this expression. And you know, we select Alpha and comparative value, we can calculate p value because, under the null assumption, under the null hypothesis, I should say this is solution. Is normal, 01, that's how we do the test. And so, same as we've done before, and we can do it now for coefficient by coefficient. And so maybe just for you to have the complex one on regression, and it was actually six in the course. I had this example and taken from Chicago for many, many years ago. And so these are the estimation results you get, the coefficient estimate. So this is a column. This column with beta hat. DC is a standard L. So how is this calculated? We calculate from SSE, you can calculate sigma or s right, and from the data, you have the variance covariance matrix, which is s squared times X, prime x inference. So from this matrix, you take the diagonal, and you take the square root of the diagonal, and you get this standard so this standard error, again, calculated by this formula, derived for you, for the case of the bivariate model. And I told you that for the multivariate model, under assumption 230, is equal to s squared times X, prime x inverse this matrix, this matrix, X, prime x, plays a significant role, as you can see. And so the S squared times finite symbols that the angular of that matrix, square root is the standard level. T is just the ratio this divided by this. That's all, and it will be the statistics that you may want to use, if you will, to use a test. To do a test on the null hypothesis that a given parameter is equal to zero. But as you know, there's no you can do this test with small alpha only in situations where you have stronger for your belief that the null hypothesis is true. Otherwise you have to worry about type 2l, etc. So, so the idea, the idea of pre hacking, is called P hacking, is to try to look for specification such that all these T's are greater than two. That's wrong. It's not justified. Okay, now and here, this is calculate the p value. So the p value will be what would be the type one, the conditional probability of the type one error if the critical value were to be exactly this estimated value was a critical value, then this will be the authority to calculate. So anyway, so you get the p value. Now, what's the most important, as I said before, is to look at this and say, does it make sense, but intercept positive meaning? If everything else is zero, then the T about three times. But this is this average car ownership. If people have no cows, then and this everything else is zero, then this will be 2.8

Speaker 1  38:40  
if they have cows, they talk about more. I think this was cows. These are on household size, more people, more trips. Positive makes sense. And these indices, I have forgotten anybody. What was indentured density. What was the density? Urban index life? Yeah, but this is interesting. You see this, the one that ever the natural tea. This is this. This was a natural density. So urban areas are then higher the nature density. You have opportunities nearby you walk, and this is walking dreams and this display is negative. Makes sense? Why these are positive or, I don't know, okay, now maybe the interpretation is a positive or negative. These are the estimation results. In addition, you get auxiliary statistics and like this, SSE, SSD, r square point seven, adjusted R square. Again, I if this has no interpretation whatsoever, except if I will compare it against another model. So if I don't care about I don't know this is like segregation index. And I said, is segregation relevant or not? And I take it out and re estimate the model, then I can use adjusted out square and compare. Okay? F coming up observation, number of observation. So this is N. N is 57k

Unknown Speaker  40:33  
is six, and this f will explain it now. Okay, so the t test an example. We can do a T test, I guess, just as some of the things that we talked about before and I'm talking to,

Speaker 1  41:07  
so I said, Because what did they said many times we need to worry about the cost of type 2l type 2l is when we incorrect. In this case, the suppose that the ones is variable, and we take it out because it doesn't have a T equal to two, we can make a significant type doable. That's why we don't want to do so. So, like, I don't know what's a good example here, on the T this one average household size. Let's take average household size to say that more people in the household, there are more trips, is zero. The effect of the zero makes no sense. So the fact that it's that it has a T of lesson, Lesson One, you cannot take it out. You cannot, you cannot. You just cannot do the test you want to do the test. Test it with a very large alpha, alpha equal to one, alpha equal to one, says it no matter what you Yes,

Speaker 1  42:28  
because the household can never be zero, I can say that the household size cannot be zero. Cannot be 00.

Speaker 1  42:41  
If you want to do a test, if you use it for a test of the new hypothesis that the COVID is equal to zero, the test should not be done, or it should be done with a very large that's a case where you need to worry about type 12, because if you take it out of the problem, you are sort of accepting a no hypothesis when it is false. Because the hypothesis is false to say that household size is no effect, it should be positive effect, maybe when the household becomes, when the household size goes to infinity, it becomes a city. But for no one household size effect should be positive. Okay, also, I mean, there's a problem. If you exclude a variable that belongs it clearly is a type 2l, but it can, it can lead to other problems. It can lead to a detergent, and it can lead to a violation of assumption one, assumption two, the endogeneity and and so you're not saying is, of course, the feet will decrease. You're losing efficiency, but you can also get virus everywhere. So for example, in this in this case, if you good example, if I take, suppose that we look at the average household size and say, Oh, T is point eight. It's not, I can slow it down. But suppose that in the data, average household size and average car ownership are correlated, positively correlated, and we take this variable out, then even the effect of car ownership will now be biased. So that's the correct will now be in the arrow term will include in it household size which is correlated with an included variable, which is governorship. More about endogeneity later, but anyhow, so you have to worry about this. Endogeneity means violation of assumption too. We can't do it. Okay. Now, the f test was an F level. What the hell was left doing. Now, what tests are we doing? So the f test is good for the test multiple restrictions. So you remember this four and six. This example, four and six. This was four, oh and this was six. So these two some indices, which I remember, what they are. This was a natural density. One was segregation. The other one was some social, social status. So whatever it was, and one can say, I mean, there is no debate about COVID, household size and density, they affect sleep, making frequency, no matter what the data would say. But these two indices maybe hypothesis that for which our upper yellow infinity is not one, it's not, we're not. We can, we can accept the situation and the model will still be trustworthy if these two indices were not in the model. So, so this is an example of a specification testing. We said we suppose that the more hypothesis is that this variable or not in the model. I don't feel very strongly about these two indices, like apoyo, 5050, I can include them, not include them. So to do a classical test, it will be wrong and but anyway, I want to do a test. I want to do a test, and we're happy to decide whether to include them or not. And so in order to this is a hypothesis with two restrictions, because I think two two variables out, I will have four parameters to estimate, instead of six, because imposed two restrictions. And so I have essentially compared two models. I have a restricted model where these variables are out and an unrestricted model for these variables are in, and the test statistic is follow an F distribution. The test statistics is given by that. So what is it? This is SSE of the restricted model which is greater than it SSE of the unrestricted model. This is the smallest we can get right. And so this is the loss in explanatory explain viability, and I but I gained two parameters. So by giving up two parameters, I lost some fit. So this is the loss of fit per parameters, and this is normalized by the sample in the unrestricted model, n minus three, so I can calculate this, this test statistic, and this is under the null hypothesis that this variable vow that the restricted model is two, and the null hypothesis second module. This follows a distribution called F. It has two parameters, and the parameters are called d1, d2 and they are called degrees of freedom. And this is like these. The device on here, KU minus r and n minus k. So the following slides will take the total over this distribution. So this is a centralized F distribution, and there's these two degrees of freedom. And from different d1 and d2 you get the shape of that. And so, but the picture here is what you want to see. So you calculate the test statistics, and the question is, what is alpha here? What's the area? Or what for a calculated f this area is a p value. And so the p value.

Speaker 1  49:49  
So just more about the statistic, SSE minus SSE is always positive since the closing instruction, fit you minus KL is again in degrees of freedom. By reducing the normal parameter, we get a normalized measure of philosophy, as I explained. So. Procedure is usually select the level of alpha and the critical value from the F table, and then to get the distribution and you inject if f is greater than the value of the F critical so you inject in the critical value the boundary band Between that's, that's the critical value for given alpha. So if s is very large, you can reject the num hypothesis.

Speaker 1  50:58  
So this was a statistic. You had the SS you, this was an unrestricted model, and we get the SSE, use this and SSD, and now we just need to add another regression. So realize the model screwing Sri SSD is the model that you estimate, and you get this estimation right. So, see our square. Where was l square? Change the little stuff adjusted our square. Here it's pointed six, eight. What was it before adjusted our square? Was point six, seven, so, so from an adjusted L square, this one fits better. So this model, we took out two variable, and it tells us it fits better. So this you can say, and then you want, we want to calculate this F test. So use this SSE restricted is 29.8 relative to what 29.1 So anyhow, we can calculate we have for the restricted model. We have this value. We have the case we have the n. We can calculate the F. The F is this for 5% we get the critical value of three. So we cannot reject the null hypothesis, meaning for 5% whether, if it was like, I don't know what the p value did we calculate the p value? We can calculate the p value. So you have to go to the distribution and to see what's the p value for. For this is, and it will be smaller than 5% right? Okay, so, so this is the f test. The F is mechanism to test for with migration, with this CO regression to test multiple restrictions. So the interpretation of

Unknown Speaker  53:12  
this is saying that even though

Speaker 1  53:22  
we will not show with an LCL or not, but the data do not support including so the data.

Speaker 1  53:36  
With this test, we cannot reject a no hypothesis, but the normal hypothesis is 00, we cannot reject it, meaning we can accept it,

Unknown Speaker  53:49  
but we still, it's still better to,

Unknown Speaker  53:53  
like, move the model,

Speaker 1  53:55  
if We, I don't know. I mean, it's like it seems like in this case, I would be different, but the evidence is that you cannot reject the null hypothesis, meaning the null hypothesis is zero. So if somebody believes that it's zero, then we cannot object. It

Unknown Speaker  54:22  
the time to evaluate the possibility as well. But I will say, given that the P value is going to be very small, and given that I don't feel strongly a priori about this variable is not

Speaker 1  54:44  
impossible. Could be told, then I would say restricted model could be accepted. He says, If we cannot object, it mean we can accept but want to be careful that you are not accepting something which is false, but the chance that is false is Gone is small. Okay, so then there was any output. We saw this F pointed out, and the question is, what is this F? It's a different F, and it's a very specific null hypothesis. So this f is given by this formula here, and it is a test that all the slope coefficient except the intercept of zero. So every time you get the regression output, you get an F statistic. It's a test that you would calculate if you were to make this test, because the restricted model is all the slopes of zero, meaning Y equal to Y bar that's what. So it therefore referred to as a test of the regression, not to say that's a regression. Is a regression is a whole significant or not. And so that's, that's a this isn't the way you calculated it, and it's basically tells you whether the model as a whole is significant. The slopes of the model anywhere is significantly different from zero. So choose you printed out. It's usually meaningless, because it's a test that we don't really need to do. So Do.

Speaker 1  1:00:39  
X inverse. It will be large variance, because this matrix x by Max will give 30 you divide it by, you know, it's inversely related to the determinant of a matrix. So, so if your matrix is close to singularity, make the determinant will be very small. So this x prime x inverse becomes large. So, so when you have this is coming up in a lecture that I'm coming to. So you're you're an advanced student, I guess you're asking me about things that I'm going to cover. So when you have multiple linearity, you get big variances. And so if you don't like to see big variances, then you take variable out. So if two variables are highly correlated, you take one of them out, and you get better standard the t value for me. So you had two variables with a very low T value. So one out, one kicked, one in, and now it has a large T value. So that's what people do. What did they do? They now violated assumption two point. Now you have another name, and you may get the t value to be larger, but the coefficient is now biased. So in transportation, which I know you are in transportation, if you look at time and distance, for example, time because I highly correlated, so they take one of them so on right? Just because drivers are correlated doesn't mean that you can take take one of them out, you don't seem to afford what they said. Anybody wants to disagree with me or go ahead? No. I mean, it's okay. It's a very interesting, I think

Speaker 3  1:02:38  
it's a very interesting, like very applied problem. So I think what I want to ask is just, can you clarify what the best practice is? Let's say you have a model and you have a new hypothesis that these 10 variables are correlated, right? Are informative, right for predicting y, and then you know, if you have, let's say one that has the low significance, right? Is the first thing to do to look at multicollinearity or to try the model without it, and have really addressed our squares, something

Speaker 1  1:03:16  
else entirely. Okay, give me an option. What choice question so the reason it's now that because the first thing you should ask yourself is, does the estimated value make sense? Why is household size has a positive coefficient, meaning greater household size, meaning more power. Makes sense, right? Yeah, and, and maybe I have some other evidence. So you, first of all, you compare the estimated value against you apply your knowledge, and you decide whether it is trustworthy before you do any statistics, right? So the statistics are useful. When you said, Well, I have nine variable that I know need to be there, but I have this my office might suggested variable 10, and I'm going to try it, because I have the data and I can flat out, then you do a test, and then you have to option, you have a restricted model and another restricted model, and you can let the statistics make them help you make a decision, but to Take it out, because it is correlated with a long thing to do. Go ahead so and

Speaker 5  1:04:45  
perhaps we'll get discussed, discuss the assumptions. But when we're considering a model in regards to the end of the set, when we're when we're asking ourselves a question, should we should we be asking ourselves, have we captured all all the parameters? I mean, you know, if you're examining population, there's millions of parameters you can consider about about a person or individual, but there's some, some degree of we talked about budget and but the base question is, do we have all the relevant parameters is that, does that make the bias

Speaker 1  1:05:24  
as small as possible Exactly, exactly? I mean, you want to catch a vehicle. I mean, there is a lot of techniques that are available where you just trying to maximize fit. You might want to put the data because you only are trying to interpret or or or, you know, match existing data, because it's interpreting some sensors or activity or machine vision, things of that nature, then the objective is different. And I beginning because lecture one is the purpose of this course is to do analysis, to look at the develop the model, and then manipulate and see how the behavior of the dependent change. So that's why I emphasize endogenic unbiasedness, if you are doing machine learning and some image recognition. You're not trying to manipulate anybody. You're just trying to detect and it's very different consideration. So you don't even find discussion of endogeneity, and here I mentioned it every lecture. So it's just different emphasis. I mean, if you're trying to, like, a typical application of demand money is to calculate willingness to pay, willingness to pay, distribution, calculate price elasticity, calculate you know, people reacting to you know, more frequent banks want to increase invest money you know, electric busses. And then the question is, what do you think more types things of that nature? This is why endogeneity is important by unbiased results or capturing their faith. Okay, any other questions? Okay, so I'm going back to the assumptions. So going back to the previous lecture. Do

Unknown Speaker  1:07:49  
so and I, that's what I spoke last time.

Speaker 1  1:08:02  
So I'm going to talk about this as an introduction to whatever is coming up next in the following lecture, which is violation of this assumption. So this was the assumption. Am I saying something wrong or something wrong

Speaker 1  1:08:33  
with multicollinearity. So something we call it no belt and multicollinearity. One is, we call it multicollinearity assumption to strict exogeneity, spherical variants and normality. And I'm just going, I talked through this assumption before, and I'm not just doing it for the multivariate case. So I'm in it's kind of repetitive. So assumption one, assumption learning is that the X has full length. What does it mean that there is no perfect linear correlation among independent variables? So when we started, when we talked about the bivariate model, we have an intercept, and we had one variable x, and I said, if all, if x is exactly the same for observation. It means that you cannot distinguish it from the intercept. The intercept is a column of one, and if you only have a column which is a constant, then it's a linear relationship between the variable is just a constant times the intercept, the one. So this, in the general case, we have this x is a matrix. You remember, X is a matrix the column of X of the variables. So if the columns so is a linear relationship between different columns in this matrix x, then this is what reason we are saying that assumption one is that there is no such perfect linear correlation among the independent variable. There is no perfect linear function that explain one variable, one column as a function of the other column. So So when one of the regressor is highly correlated with with, but not equal to linear combination of the other vessel, then we say that the regression suffers from multicollinearity. So it's like it's another one of those disease that you try to avoid it as much as they can. We'll talk about it. It's not as bad as an endogeneity, but it is although the multicollinearity is not perfect. So if multicollinearity is perfect, then assumption one is violated. So multicollinearity cannot can be no can be imperfect, meaning we there is some relationship between variable, but it doesn't fit exactly. It's not and so if multicollinearity is not perfect, in such a case that design matrix is full length, right? Meaning that there is correlation between variables, but it's not perfect. And then what happened in that case, when I answer your question before I have felt the situation? So what do collinearity mean? Variables are correlated, and the consequence will be the least square estimate will exist, but it has larger boundary variance. It just cause the variances become that's important.

Speaker 1  1:11:56  
Sorry, just example of violation here, like suppose that you have dumfr gender, and you live in a world with only two male and female then, then the male dummy plus the female dummy is equal To the constant one. So that's a perfect, perfect collinearity, so you cannot include these two. So in this case, it's okay to take one, because it's just a categorical variable. It has two categories, and you need to select the base when you have an intercept. If there was no intercept, then it would be okay with the interpretation of if you if you want to include dummy variable for male and dummy variable for female, if you take the intercept, that's okay, because that beta two is essentially the intercept for male and beta three is the intercept for female. If you take female out, then the intercept for female is beta one and the intercept for male is beta one plus beta two. You get it. So let's it doesn't change the model. This model has perfect colinearity assumption one is violated, and there is no unique solution. So you take one of them out, or one of the layout. In this case, okay? And it doesn't matter which one you take them, it doesn't change the model. Another example in the hill is when you suppose that Summertime is divided into two components, in vehicle and out of vehicle. And suppose it's like to estimate the model which have all three, three of those, this variable, this variable, this will be x4 is equal to x2 plus x3 that's an exact linear relationship. So that's, again, the violation of assumption one. So this is a kind of violation that happened when you just don't realize because if you estimate the model, you cannot include all three, because there's exactly a relationship between them. You can include all the two. If you have a dummy variable, if you have a categorical variable and it has two categories, you can include only one dummy variable if you have an intercept. So this is the assumption one, meaning no perfect, multiple reality. So sometimes people make a mistake. I mean, these are mistakes that you can typically make, and the regression problem, two kinds are some that the matrix will be singular. You get the message. The other themselves, there's a single matrix here. Let me use generalized influence, meaning, what does it mean that they use generalized influence? It means that they will get a solution when it's arbitrary. There's no unique solution. We need to apply least squares. And the problem that looks like this, the beta two, beta three and beta four have a relationship between them because, because total time is equal to the sum of these two. So, so the effect of in vehicle time will be beta two plus beta four, the effect of out of vehicle time will be beta three plus beta four, if I was able to estimate it. So it means that the sum beta two plus beta four and beta three plus beta four could be determined, but only plus beta four. So you can beta four. You can specify whatever you want, because if you put some beta four, it will reduce beta two, such as the sum will remain the same. That's, I'm not sure if it was totally clear what I just said. It doesn't matter. The two kind of computational response to this one is detect singularity and inform the user singularity that you check the model. Another one is to say, like detect singularity. I mean that I can, in terms of minimizing least square, I can find the minimum. But the minimum is not unique, and and then you select arbitrary values algo in the breast, you get some the model is usable, but in terms of interpretability is questionable. I don't know how to interpret it because you're not okay. So that's the first assumption. Second assumption was strict exogeneity. So the mathematical notation is this an assumption one and two, the expected value of beta has given x is equal to beta, and this is just the same proof that I gave to earlier from the five variant case. This is just a proof of unbiasedness in the multivariate case. So beta is like no and it's equal to we know that least squares, and if we substitute the model in the least square estimator, we get this formula. So these are the matrix of, sorry, yeah, it's matrix. And this is a k by k times k by n. So this is a matrix. It's a k by n matrix multiplied by N by N by one. So we get an n, k by one. So these are the C, the C coefficient that we add in the bivalent case. And so. So by insulting the model, we get this expression. So the expected value of this given x is this a constant. And then if x is given it's expected value of X, not given x and and so if this assumption is two, we call it strict exogeneity, then the expected value, beta net, so it's unbiased. So let's just improve that I showed you before in the multivariate, in the bivariate case, and it's a disturbance here. And so suppose that the regulator has an intercept that this has no meaning. It does not imply exogenating, dis exogenating, no dependence of the expected value of epsilon on x. Okay. So mid assumption one, existence, assumption two, to get that minus this amount of time. But then we make assumption three, and it has two pounds, and this is just the multivariate extension of it, and the variance the same for every observation. The conditional variance is equal to sigma squared, because expected value because of assumption one, assumption one, then this is a variance, or conditional variance, and this, this is a variance, a conditional covariance. This just shows this conditional covariance. And so this remember assumption two and assumption three, assumption about the dissolution of epsilon. So epsilon is an n by one vector. So we have an n by n variance covariance matrix. So essentially assumption one epsilon two and three together. Assumption three means the variance covariance matrix of epsilon is a scalar matrix I is an identity matrix. If you have one on an 11 multiplied by a constant, you get the matrix that looks like that. So in general, the variance covariance matrix, what it means? It means that your variance has an 11 and it's a square, symmetric, positive definite matrix to the of the Angular will be zero. And so this is s squared. How do we estimate sigma? And finally, there is an extension of the Gauss Markov theorem applies to the multivariate case so on, remind me talk about the control. And then there's normality. So so.

Transcribed by https://otter.ai
