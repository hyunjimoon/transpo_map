Unknown Speaker  0:25  
Is it working? Oh, cool. You.

Unknown Speaker  0:40  
We can have

Unknown Speaker  1:24  
Okay, sorry about the delay. Technical

Speaker 1  1:34  
difficulties, anyhow, so that was mentioned from last time that I did not finish. So that was the beginning of the part about financial and I did get all the way to the section of statistical properties. And as you can see, the outline has these assumptions. So I was asked a question yesterday that made me think I neglected to explain to you at the beginning of this block, what is the structure of this block of seven lectures. It has some sort of a logic behind it, because it may be different from some of you when you've studied regression, and you may have studied it differently. And this emphasis on assumption is here because I'm following the following structure. I'm going to start with a basic linear regression, classical linear regression, the way, the ways that is implied, when using any kind of regression software, this assumption is implied. It's assumed that this assumption hold, but they don't hold all the time. So the way I'm going to do it is, I'm going to first explain this assumption, explain why they are needed. What do you get from this assumption? This one is a standard model. And then I'll talk about the sequence of lecture where we first introduce an assumption, starting last time, continue today. Then begin to talk about violations and talk about specific cases. What happened when assumption one is violated? What happened so right now, in today, in last time, in today, I will say, what do you get for making this assumption? And then I will say, Okay, let's assume that this assumption is violated. Any of this assumption, what do you do? How can you react to it? To get things valid results? Because if one of these assumptions is valid, then the standard model is not true. It's not valid. The output that you get is not valid. You may not know it, unless you take this course. I want you to know, to teach you to know when these assumptions do not hold. And then what you can do. That's the structure of the regression actions. That's what it takes. Seven lectures, lots of assumptions. Then when I talk about discrete choice, I will do the same thing. I will start using a simple model, the logic model, and then ask, okay, what? When it what do you do when it is rejected? When we go to more general models, this the way we proceed in the discrete choice. We go from logic, then we have another model called nested logic model, and then we have all kind of models in a big family of logic pictures. And the question is, Which model should you use, given what assumptions make sense, what assumption you can assume, what assumption may not be valid, what assumption can be checked against data? Some of these assumptions can be verified by the data. Some not, etc. Okay, so, so let's go. So if it was a missing motivation, maybe last time. I hope this is clear. Now. Now back to the technical stuff. So I'm going to flip the slides very quickly, just I started with a simple example. Then I talked about it. What is the meaning of linearity, and when is it linear? And then talked about curve fitting and the concept of least squares and y least squares, as opposed to some other objective function. And then talking about the mathematical property, so if you agree to use this least square as a fitting function. Then you obtain the estimator. We have an objective function, least squares, and then we have the first order condition and the second order conditions. But first we talked about the first order condition, what do they mean? And then we talked about the solution of the first order condition, which is given by this. So this is the estimator of the slow parameter, and to estimate alpha, I just substitute it the means, and we get alpha. And but in order to obtain this estimator, we have to define so the first assumption that we made those tests, this exists. We cannot divide by zero. So one of the things we'll talk about later, and what happens if it is zero for the for the bivariate model, immediately, when is it zero? It's zero is all the x's are the same. So you can immediately detect what it is, when it is you cannot infinitely many models. There's no unique model that you can estimate. What can you do about it? We'll talk about it later. To find some other data that's basically that, or make assumptions, additional assumptions. And then we talked about this, this particular estimator that it was on the previous slide, and we show that it can be written as a linear function of y, where we will write it the things in Cali bracket. Here you'll call it C, capital C, you see. So it's a function of x multiplied by y, when it's some of the data. So that's sort of a linear estimator. It's linear in the y. But this about the estimate, a lot about the model. When we talked about the model, it was a different kind of figure, and then the second order condition. And we can say that the second order condition hold C of a minimum, so the matrix of second validity is positive, except it is zero. This will be equal to zero

Unknown Speaker  8:27  
when assumption one does not otherwise it is positive. So this was the second order condition. So this concludes a mathematical property.

Speaker 1  8:43  
I then said, Okay, we have an estimate. We have data which is a sample of y x, like this cloud of points, and every observation is value of x and the value of y, and we have a formula now to calculate theta. We call it our least square estimator. And now, what are the statistical properties? So we can do boot, stopping, or we will do some theory like either one. So bootstrapping, you can calculate data many times by drawing and taking a sample. You remember bootstrapping so you can empirically look at the sampling distribution, we will do it theoretically, and there's always a period option available. And in order to talk about the sampling distribution, or the properties of the sampling distribution, we will be taking some expectation. So just to where my wave waving my arms? And I can use this. And so we talked about definition of expectation, the definition of conditional expectation. So expectation, that's the definition of the mathematical expectation. You have a distribution of a variable. You calculate this integral y times its distribution and the results, we call it the mean of Y, or the expected value of Y, but we can take the expectation on a conditional distribution. Why is this the model? You remember, the linear model, any kind of projection model, if y is some function of x plus an L term. So this function of x is the expected value of Y given X. So that's given that x is equal to some value x, what's the expected value of Y, and that's given by the regression model. So that's this function, the alpha plus beta X. And in order for this to be the expected value, we would like need to have that expected value of epsilon given X is zero. But anyhow, so when we talk about conditional expectation, it just means that you are taking an expectation over a conditional distribution. So this is a conditional probability density function of Y, given x, what's the distribution of y? If y, if epsilon is a random variable, if x is given, then this is like a constant. So the expected value of Y is equal to the expected value of the constant, which is a constant given x, right? It's a constant. If x is not, then it's not constant. But this one, we did say given x, given x is constant plus the expected value of epsilon given x, which we would like it to be zero. And we'll see why the importance of this in the moment. So that's just the definitions. And sort of the idea of the linear model, the idea of our regression model, this doesn't have to be linear. It could be some non linear function of x, but the idea of the regression model is that this is the expected value of y given x, and therefore, if there is an additive and all term, then he wanted to be independent of x, because we want to capture the full effect of X in the model. Otherwise we don't capture the full effect of X. Okay, so sampling distribution, so you know. But for sampling distribution, we want it to be centered on the true value, which means, no matter what is x, right? We want the expected value of beta hat. We can collect data with different kind of x's, but we want this procedure, the least squares that give us this beta hat as a function of x and y, to be not dependent on x for any any possible x's to be beta that we call it unbiased. So the sampling distribution of the beta hat is a formula and should be centered on the total value we then we sentence, that's where those unbiased. The question is, is it unbiased alone? We need to check it. And then, is it efficient? So these are the two key properties of the summing the solution. The third property was, what the shape of this distribution? If you want to calculate p values or any kind of confidence interval, every time we calculate probabilities, we make global, realistic statements about our estimate of beta head, we will need to know the shape of this distribution, yes. So

Unknown Speaker  13:49  
that efficiency line where, with beta tilde, is that just any, any,

Speaker 1  13:56  
just a definition, meaning that beta hat is our least square estimator. Suppose that we come up with something else, and for example, we minimize the absolute deviation like you remember I talked about this as an option. It is an estimator. So suppose that this is n and we need to show that the variance is smaller than it's efficient. That's ideas. So it's just saying that for

Speaker 2  14:27  
the beta hat is going to be have smaller variance than any other arbitrary

Speaker 1  14:31  
this here is just the world is to define the world efficient, but the specific property of it that will come to an we will have an efficiency theory that you'll see will be on the slide. Be patient. This is just definitions. So let's now look at it theoretically, see if it is unbiased. Can we do that? Yes, we have a formula, right? We just substitute for y, for y in the model. The model is given here, and then we just manipulate. So we have a formula. This is our estimator. We substitute, divide the model, and then we manipulate. We get that beta hat is equal to the two value, but it has an extra term, which is a function. The C's are these coefficient that you see here. So c, c n is x n by 2x bar divided by the sum over n. So this is a c and a c n, and so we summed over n of this function of x times epsilon, n and so. So this basically what this says. These are true beta. There's our fluctuation in the data because of epsilon. Epsilon is random. So whatever we estimate will be a fluctuation around beta, we know that. I mean, that's why we derive the sample distribution. The sample distribution appeals because from sample to sample, this is what will be different. Beta is a constant. Beta hat is what we calculate for the data, and depending on the data, this will be different. And therefore, if you go another sample, the estimates of beta x will be different. We get the sample distribution. So now we know that beta hat is distributed based on the distribution of these epsilons. So in order to know what happened to beta. Head around beta, we need to know about the distribution of this epsilon. So we will be now making assumptions about the distribution of this epsilon. You don't have to make this assumption if you want to do like apply bootstrapping, etc, you just draw more data and you get the sampling distribution. But we are working theoretically. So theoretically, in order to derive properties of beta head, we will, it will depends on what reason has to assume about the epsilon, the distribution of the epsilon. So, so the first assumption that we are going to make, call it assumption two, is that the expected value of epsilon given x is equal to zero. What does it mean? It means that every time when we have a line alpha plus beta X, that we have a cloud of points around the line, we measure the deviator. There the disturbances this L term vertically. So for any given value of x, there could be different values of y depending on epsilon, and the distribution of these epsilon around the line would be such that this distribution is centered on the line. That's what this assumption means. This assumption means that for every value of x, there could be epsilon going up and down, but there are distributed around the line. They can have different shapes and distribution. They can have different variance. But the center of the line the left for this assumption, meaning that the expected value of epsilon is the same. Zero is the same. No matter what the value of x as the exchange, the expected value of epsilon does not change. I mean, if you do, if you didn't make it conditional, if you said, oh, I want just the expected value of epsilon equal to zero, it's a different assumption. So this is not the expected value of epsilon equal to zero. That's that's always satisfied when you have an intercept, like if you have an intercept in a modern alpha, if it was, if the expected value of epsilon was not zero, then we just add it to alpha. So, so this assumption, expected value of f is equal to zero, is meaningless when you have an intercept, yes, no, this is not. This is not homosceda. We'll come to it as well. We give this assumption, eventually we'll give it a name. It's not in this lecture, it's in the next lecture. We'll call it strict exogeneity. I don't need to explain it, so it will have a name. But for the time being, that's egregious to call it assumption too. Can you go ahead on the final distribution

Speaker 2  19:57  
there for the distribution, therefore that's one there happens to be a point that's on the on the on the line, versus me on the respective value line. Is that just a coincidence? Did I

Speaker 1  20:14  
just want to make sure that would be nice that all of them would be exactly on the line? It's a perfect model. But

Unknown Speaker  20:20  
that that that value action would be at that at that vertical, you have a red line, where

Speaker 1  20:29  
this distribution means that the point can, with the probability of zero, be exactly on the line. This is all real number, the probability of a value equal exactly to a number. Enough is always zero, but but in loosely speaking, there is this density is centered on the line. It can be exactly on the line. Be, yes. Second, it's a scene. It's just a function of x, and if you, if you take conditional expectations,

Unknown Speaker  21:17  
you're talking about the previous state. So this C, right? It's a function of x. Let's say the formula for c is equal to what cn is. CN is equal to this service is the sum over n of this same which is CN times epsilon,

Speaker 1  21:46  
CN is just a function of x. So if I take a condition, conditional expectation given x, then it's a constant. So

Speaker 1  22:03  
So x could be a random variable, and it will be in any application. X is random, but the model is probability distribution of Y given X, right when I told the discourse is about conditional probability. Now here we are talking about conditional expectation, because we have y is continuous. So we are looking at on the distribution of Y given X. This, this is a that's what you are interested in, the distribution of Y given X. So what's the distribution of Y given X? We're saying it's equal to given x. This is a constant. It's a constant plus epsilon that fluctuates around the line. So the distribution of y is equal to the distribution of epsilon just with a different mean. The mean of epsilon given x, it will be zero, and the mean of Y given X is given by the regression with a systematic part of the model. So So, whereas in a probabilistic model, like in this screenshot, probabilistic model in the tablet example, we had the probabilistic model. It was a boundary outcome, tablet, not tablet, and the model was this, this function pi, probability of pi we had pi one by two, pi three. And the idea of regression model that we have a systematic part which is expected value of Y given X, that's what we model. So we model the expectation right? So in this model, we would in order to know how is y distributed along the lines, when we still know about distribution of epsilon, then we have a probability model. So if we, if you make an assumption about the probability of X, we say epsilon is normally distributed, then y is normally distributed. But around this mean, we have a conditional distribution, conditional distribution of Y given X. That's this kind of conditionality is behind any model that you'll see in this class, it's always conditional dependent variable, conditional explanatory or independent variable, x. So in the regression model, we only specify the expected value of this distribution because the full distribution of Y, it will have an expectation which is a central moment, like the point where it is centered around plus, it will have fluctuations That depends on the epsilon, right. And so if the solution for external is normal, then this will be f. Will be this f of y, given x would be normal as well. This will be assumption four. But before we make assumption four, the nice thing about least squares, we get ourselves about this expected value without making assumption about the distribution of epsilon, we will make it. We Q can make it, but we make the idea is you want to make fewer assumptions possible. Okay, where was the and so for beta head, the least square estimator, to be unbiased the expected value of this expression right, the expected value of this expression given x needs to be zero. So we take expectation beta is a constant, the expected value of this expression. We move the expectation in, because c is a constant. The C's are just constant. It's just given X. They're constant. So it's expected value of epsilon given x. So if you can make an assumption too, that the expected value of epsilon is independent of A given X is independent of x is then it's zero, then beta hat is unbiased. So we made assumption one for least going to exist. Now if we are really willing to make assumption two, I'm sorry, I'm pointing to the wrong one. This is assumption. This is a consequence. If we are willing to make also assumption two, then we can say least squares is unbiased. That's what we are trying to get. We are trying to get an unbiased estimator, because we are trying to make predictions. What will happen if we intervene in the system and change some x, what happened to y? We need for that. We need unbiasedness. So in order to get that bias this theoretically right. So you want to prove that biases theoretically, then we need to have this property. I'm sorry, I'm tweeting. We need to have this assumption. So the expected value of epsilon has to be independent of x. That's one later we refer to it by name, strict exogenating. And because, what does it mean? It means that x is not in the epsilon. Epsilon is external in the model, y is a function of x plus epsilon, and in this epsilon, we don't want to have any effects of x, especially on the expected value. So we want whatever these fluctuations around the lines are. We want this fluctuation to be centered on the line for any value of x. That's what this assumption means. Now it means that if you say, What's is it? Do we have something in the epsilon that will violate this assumption? Yes, there could be many situations. We'll talk about that. That will be what we come back later. We'll talk about when is this violated? That's the most important assumption of three squares.

Unknown Speaker  29:03  
Now, can you test this assumption for the data? No, you cannot, because the property of the residuals are always orthogonal to x, meaning that the residuals that come out of least squares.

Speaker 1  29:27  
So least square estimator make sure that the residuals that are generated are orthogonal to the x. That's the first order condition. So this is an assumption that is not testable from the data, but it affects the results. So we'll talk about specific examples that we know about in which this assumption is violated later, and that's why I'm spending so much time about understanding this assumption. Okay, next, less important is efficiency, but it's nevertheless important, right? So what's the variance by the head, what's the spread of this sampling distribution? So we know already that under assumption one, I'm sorry, assumption two, the sampling distribution is central than the two values. But now, what's the spread of this? Is it flat and wide? Is it narrow? Depend on this variance? So it's a variance of beta hat conditional on x. That's the definition of variance of beta hat, right? That's the formula just conditional with x, we are going to make an assumption that this expected value of beta given x is equal to beta. It's unbiased. So we'll calculate the variance theoretically, assuming that assumption two holds assumption one and two. So this is assumption one, this assumption two holes, then we get this result. So we can substitute it here. So we can after we substitute then the variance of beta head can be written like that. But beta head minus beta, we know we calculated this, we get this expression in an earlier slide. So to calculate the variance, we have to evaluate the expected value of this expression. So this this expression squared, so we have each element squared, and then we have all the cross product. So when this element is squared, we have c squared, and we can always move the expectation we have a sum like this into because the C's are constant, right? So this expected value of epsilon squared and this cross product will have the expected value of epsilon n epsilon n prime. So this is the sum over all possible appearance of observations, and not in observations. So this is for every observation. There is this expression, plus for every pair of observations given by these sums, we have these algorithms. Now, what are these two? Expectation of epsilon is our conditional expectation, and we made this assumption before there's assumption two, and now we have to look at these two expectations, expectation of epsilon n squared an explanation of these kind of cross border for different observations. This is a variance of epsilon. Why? Because that's a definition of variance for your mean, the expected value of epsilon given x is equal to zero. So this is the variance of epsilon, and this is the covariance of epsilon n and epsilon n prime. So these terms, again, are about the distribution of virtual so first we made an assumption that the distribution of return is all centered on the line. This was this assumption. Now we need to make an assumption about these variance and covariance, because we need to know them in order to calculate the variance. Omega hat, so the variance of beta hat is given by this expression, the C's of just function of x, and the variance of epsilon and the covariance of the epsilon are properties of the distribution of epsilon. Now, when I say distribution of epsilon, epsilon, how many epsilon get? N, capital N, for every observation, there is an epsilon. So for every observation, there's a million. And every pills of observation look at the COVID. This observation COVID. So here we will make an assumption three. And the assumption Slee is divided into two separate name, long wells that you need to remember the variance of epsilon. The conditional variance of epsilon is going to we're going to assume is a constant sigma square. So what is this? This is called homoscedasticity. What does it mean? It means that all these distributions depending on the value of x, this distribution, all has the same variance. So if assumption two was that the distributions are centered on the line. Now we're going to assume that they have the same variance, and we're also going to assume that they have no covariance. And no covariance has a name which is no autocorrelation, meaning that epsilon are not correlated with themselves and and with lot likelihood functions. We made this assumption of no autocorrelation, because we assume that the observations are independent. If observations are independent, then these covariances will be zero. So this, in this particular case, we call it no other correlation. Previously, we said that the observations are independent. So when we draw one observation, we draw an epsilon. That's the next observation, we draw a new epsilon, but independently of the first epsilon, that is no other correlation. So constant variance is known as homoscedasticity assumption. No auto correlation is the this assumption here the dependence assumption and the combination of these two, we call it assumption three. And yeah, go ahead.

Speaker 2  36:24  
So at this stage, going through it, we've assumed that they have the cost and variance for all epsilon do we have? Have we made an assumption about the shape of

Speaker 1  36:36  
the distribution, the epsilon assumption, full assumption, full what? Would be what you're looking for, yes, we are taking

Unknown Speaker  36:50  
conditional variance of Yes.

Speaker 1  36:55  
Why we are conditioning the data? Because we want to show how does it depend on the data for the X, like for unbiasedness, we will show you that it's not dependent on the X, that this is unbiased for any sample of x. And this is, again, we want to have a formula for the variance, because we, we as analyst, we can control x when you design a regression, you can select it. Suppose that you're doing level two experiments, you decide what the x's are. Y is random, but x, you can decide sometimes x's are London as well, like when I conduct the survey and I observe your income or your height or whatever. That's random. But if I conduct an experiment in the laboratory, then I decided what the x's are going to be. So anyhow, we want to know, how does it depend on the X? The model is conditional in x. We want our estimator to be to see how they vary with x, and so we know that the expected value of beta l under assumption two does not depend on x and it's unbiased. It becomes a two value that's useful. And the same for variance, we find out that the variance may depend on x, we'll find out. So assumption one says no, doesn't depend on x. Doesn't depend on x because the variance under assumption three, the variance is a constant sigma square, and the covalent is on zero, so does not depend on x, so the variance of beta hat, we depend on C's. So there will be a dependence on x, so the C but not so the distribution of epsilon. So this is so we, as I said, assumption two and assumption three and assumption four, assumption about the distribution of the epsilon. So we know that the randomness comes from the epsilon. We're making an assumption about distribution of epsilon. And say, under this assumption, we get some properties, theoretical properties of the sum and distribution. So we know that under this assumption, assumption two, there's unbiasedness. Now under assumption three, what we can do from assumption three? Well, you know what it is. From assumption three, we can now calculate the variance of beta hat, because the thing about this covariance is all zero, so we have only the first term, which is the c squared sigma squared, and we get a formula. So this is a formula for the variance of beta n. It does depend on x, so it's a constant sigma squared divided by this function of x, and what it tells us. Now we know about that under assumption two and assumption three, the sampling distribution is centered on the top value and as this spread presented by this expression, which makes sense, because it's sigma squared. What's sigma squared is a variance of epsilon. So as variance of epsilon, increase the variance of beta had increased. And what is this at the bottom? If I divide this by n, that's a variance of x. And if the variance of X, increase the variance of beta had decreased. Now, remember, if X has no variability, we have no estimator. We need variability of X in order to get the line. If all the x's are in one point, we have infinite So the variance of beta hat is decreasing when there is no variance of X and the sample size and the sample size increase, this sum at the bottom is increasing and the variance of beta hat goes down. So this formula is intuitive, makes sense and and now we can, we can estimate the standard L. Remember the spread of this distribution. We calculate standard zero, but we need to know sigma squared. What's sigma squared? Sigma squared is a variance of the epsilon. We don't love the epsilon, but we have the residuals, so we can calculate the variance of the residuals. And this we call it as we divide it by n minus two. If we divide one, to divide it by n or n minus two makes no difference for very large samples. If n is 1000s, then minus two. It doesn't make no matter fit for any of us. This is to make this estimate unbiased. And we call it a square, or sometimes sigma head square, sigma square head. So this is an estimator of sigma square. And if we know if we have sigma squared, then we can calculate the variance of beta using this formula to substitute to sigma square s squared. So now that's what they give you an example when you use some software, and in the software, it calculates standard errors. Version. What is it based on? It's based on assumption two and assumption one, two and three. So if any of this assumption is we talked about so far, is violated, then the output is one. If assumption one doesn't hold, then you get biased results. If assumption one holds her assumption so assumption two holds, but assumption three does not hold, then this formula because we made assumption, otherwise suppose that this assumption is not absolute, suppose that this absolutely assumption three, suppose that this assumption three is violated, then what can you do? Well, you have to apply this formula here. You have to know the variances. You have to do covariances. I mean, complicated. So any software that you're using that calculates standard errors will use this formula or this formula. So this formula, so if you have to know if assumption, if the assumption is valid, then you have the wrong output. Okay, now, with assumption two and three, there is a theorem. We buy a theorem, efficiency theorem. It's nice to have that because you know under what conditions you you cannot do better. So if assumption one, two and three hold, then beta hat is COVID, Gauss bound. Beta hat is the best Linear Unbiased Estimator. What does it mean? Best means minimum, but not among all possible possibility, but class of model that estimator the linear linear in y. Remember, we make an assumption that it was some coefficient times y somewhere that would linear means here and unbiased, meaning unbiased estimators, unbiased, then it is said to be blue, meaning best Linear Unbiased Estimator, meaning other non linear, or bias estimator may be better. We don't know if we haven't found them yet. We may they. Insist. So what did we do so far? With assumption one and two, we got unbiasedness, and with assumption one, two and three, we got deficiency. And also with assumption so we got this formula to calculate, theoretically, the standard you don't like this formula, you don't know the formula. You can always do good stuff, but, but now you have a formula, and that's what often is you to calculate the standard at all and but by making this assumption, we also have this theorem. So we say sufficient, we have an efficiency field with some qualification, usually, okay. And then you make the assumption, the longer weighted assumption about the shape of the distribution of epsilon. So we say it's centered on zero on the line. It has specificity in all the correlation about the vertical covariance, and then the shape, we said that it's smaller. So if you assume that epsilon, the distribution of epsilon given x, well it's independent the distribution of epsilon centered around the line, which means, you know, so epsilon given x, mean zero and giant sigma squared. And this implies, because beta is just a normal, it's a linear function of the Y,

Speaker 1  46:55  
then the beta hat estimator is linear. Then beta head is also normal. So beta has given X is normal under assumption 1234, beta, at least we get this. We know that now we know everything that needs to know about the supplies distribution. It is centered on the two value. It has this variance, and it is normal, we can calculate p value, alpha, beta, all the things we talked about now the confidence interval, you can now calculate because you have normality. Under if we assume that the epsilon is normal, then we have a better efficiency. What we get from it? We get an ability to calculate probabilities, but we also get a better efficiency with best bios, before we have the best Linear Unbiased Estimator. If epsilon lowering the simulated then you can show that the least square estimator is actually the maximum likelihood estimator. I'm not showing to you here, but it's any in any textbook. So beta head is the maximum likelihood estimator. It is the best unbiased estimator, meaning it attain the capital out low bound, and you cannot do better among all unbiased estimator.

Speaker 1  48:40  
So this concludes the lecture from last time by variant model, we have the V square estimator automatically. We can also estimate the s square, the variance of the epsilon. So far,

Unknown Speaker  49:02  
we have was any questions, why I'm spending so much time on this assumption. So

Speaker 1  49:25  
the next lecture, the next lecture is when linear algebra comes in. I promise to be very gentle, so I will define everything and explain what we do, so don't get scared. So most of it is just a notation, so notation that allows us to do I did now for any number of x's, and then so this rotation, do again what we've done. So I'm just now, I'm just repeating what I've done in the lecture the first one, but I'm going to do it with any number of x's, multiple x's, and and using vectors and vectors notation and and then we'll have the same assumption, the same property, everything the same but in this lecture, I define names for the assumption. So from now on, instead of saying assumption 123, and four, we will have name for this assumption. Okay, hopefully that will be easier to talk about it in the future. So the bivalent model, then, I guess you've seen this equation before. Now, multivariate. So Y, capital Y, will be this vector of y, the column vector. So any vector, unless stated otherwise, is a column vector. So we take all our data and put it all together. So all the y's will be put together in a vector, which we are going to call y. Okay, then we are going to have multiple x's, right? And instead of having an alpha for an intercept, we'll just call it an X, and assume that if you have an intercept, and one of the x's usually the first one will be just a column of ones, okay, so, so I don't have to have an intercept in slope and then just x's, and the convention will be that the first x is just one, okay, so, so here every before we hold scalar notation. So for any observation we have, y, n is equal to x1 or n1, times a parameter, beta one, et cetera, x, n, k times beta k. So the notation for the variable will be the variable will be from one to capital K, or the index small k will integrate the variable. So in a given column here, the whole x1, x1 for observation, one x1 for observation, small n, and this x1 for summation, the last observation, capital N. So we have all of x1 all of x k, small k, and then all of x for capital X, capital K. And this matrix of X is, we call it X, capital X, and this is a vector n by one. This is a matrix n by capital K, capital N by capital K. So x is like this big matrix that includes all our experimental data about our planet. It's a matrix that we expect it to be thin and tall, right? It's rectangular. It has the horizontal dimension is a capital K, and the vertical dimension is capital N. So it's an n by k matrix. Sometimes it's referred to as a design matrix, because if you're doing laboratory experiments, and x are all laboratory settings, then this is like, how do you the experimental design, and then observe outcome is y. So x is this matrix, capital X, beta is a vector. And what you see, you see there's this coefficient for given column of x1, all the beta are the same as beta one, beta k, beta counter k. So this beta is a vector of length k, and well, length k and epsilon is a vector, but like Y, it's a vector size n. So it's an N by one vector. Okay, so what you see here is a product of a matrix times a vector. So if you have a an n by k matrix, and you multiply by vector, k by one the outcome would be a vector and n by one vector, and n by one vector. So the dimension, the horizontal dimension of the matrix and the vertical the length of the vector are the same. Then you can multiply matrix by a vector, and you get what you see here. So the results of multiplying this matrix of x by this vector beta are this. We get all our data shown here. This in this notation, meaning this is for the we like the model for every observation. So for given observation, we have the vector of y. We have the x's for given observation from one to k, and you have the vector of coefficients for one to k. So again, this comment is what they told you about, that if x and one is equal to one for all n, then beta one is, again, just the definition. The definition is repeated in this box here, and we will find this matrix X, consisting for the x the columns a particular value, the capital, K column and N laws. Now, if we take the x's for given for given observation, we call it x1 transpose. So the transport transposition sign, because we said every vector will be a column vector. So if we take all these values, excuse me for observation one, and every put it as an colon, you get the x, y, so you have these vectors, and these are for given observation. So we can write a model for given observation. For given observation. The model is given by this. This is a scalar notation for every observation. So for one observation, if we collect all the betas into a vector, beta column vector, and collect all the x's, x n to be a column vector, and then we transpose it to get the law vector times beta. We get this expression here. Okay, so this is the way to write the model for one observation, and this is the way we write the model for the entire data set.

Speaker 1  57:40  
So here, y, n is a scalar, epsilon n is a scalar, but the X is a vector, and beta is a vector, but it's x n for observation N, and beta is a vector for all the same for all observation, of course.

Speaker 1  58:01  
So if you can memorize this definition now, and sometimes there is a confusion, because you see one a particular entry of the matrix X, we have, we call it n, and particular entry, let's look at this.

Speaker 1  58:33  
I don't have this notation, but then we look at the particular x. You see x, and K, small n, small k, meaning is the case explanatory variable for observation N and and so it makes sense to have because it's a matrix. It's an n by k matrix. So this is the row index times the column index for this matrix, X makes sense sometimes. So we always arrange the matrix as the n by k. But you'll find in some textbooks that when the individual x's are shown this subscript are sort of exchanged, so you make the first one for the variable and the second one for the observation. It's just a convention like this, but the matrix remain the same, so we don't change the way the matrix is arranged. So they may lead to some confusion, but anyhow, I won't know about it. Okay. So this is all about notation. Any question about the notation?

Speaker 1  1:00:02  
So, I understand why this is so efficient in terms of lighting, because we can get rid of all this summation of n, because I have y is all the ends, all the ends are included. So we get rid of the sun, and that's why it's one of the reason. So let's now use the model. The model will now be this, y equal x, beta plus epsilon. I don't have to write n goes from one to n is it's that's for all n, right. And we want to estimate the value of beta. The total value is beta, the estimated value is beta hat as before, but now, beta is a vector. Before it was a scalar. That's going to be a vector. And once we estimate beta hat, we can calculate the residual, same as before, but you can say we use a vector notation here. This is not just one beta. This is the whole vector beta, but beta hat, and the whole vector of the X is for observation error, and again, it's with the transpose. Okay. So now we want to, instead of calculating the we want with the sum of squared error, we call it SSE, and will be a function of beta hat, and we try to minimize it same as before. And so before this was just y minus alpha minus beta x. Now just y n minus x n, transpose beta hat, and this is squared and sum over n, but now we can write this without the sum over n by say, if I have this vector y, I subtract from it the product of x and beta hat, which is a vector y. So these are the residuals, a vector of residuals. And if I take the residuals into a scalar product, transpose it. So this is a column vector, A transpose it, making it a ball vector, multiplied by itself, a column vector. What do I get? I get a scalar. So this is the residuals. We call the epsilon hat. So this epsilon head, prime epsilon head. That's the sum. This is the sum of squared L So, so we can write this. So it's a function. This function depends on the value of beta, where beta, beta is not a vector. Before there was just a single base, then we found the minimum. And so we took the derivative, etc. We do the same what would you do it now is when I have, instead of having two derivative with respect to alpha and beta, when I have derivative with beta one, beta two, all the way to beta k. And so we are going to do exactly the same thing. So don't get scared by this, taking this derivative and this term here is, is just the residual inside the residuals multiplied by by, well, it's not yet residuals because it's not beta hit. So this is like the L terms. So this, of course, the scalar product of the factors. Okay, now we do sometimes position, so we transpose this, we multiply all the terms here, we get this expression, and, and these two are the same. And so we can write till two. The task force of this is equal to that. So it's equal. These are all scales, right? This is, this is scales. These are all scalars. And, and so we get, but we get an expression for the sum of square there, or as a function of beta, and now we just differentiate with respect to it. So this is derivative, respective factor. So okay, don't have to memorize any of this. I just take my word that when you differentiate this respect to beta, this just then doesn't matter. This is linear on beta, so we get something here, and this is quadratic in beta, because it's beta prime, this cross product matrix X, prime, x times beta, we get to get again the two at this thing. So this is linear. Is a linear equation in beta, the two, of course, because we set it equal to zero and call it beta hat. So then we can change sign, move this to the other side. We get this expression, get x prime at y is equal to this x prime, x times beta hat, and here comes an assumption to solve. To get a solution for beta, what we have to do to solve a system of equation like this is a system of equations, and in order for a solution to exist, you need to be able to infilt this matrix, X, prime x. Now X, prime x, you'll see it many times. What is it? X is an n by k matrix. When we transpose it, it becomes k by n, and x is n by k, so x prime x is what is a k by k? Square matrix, symmetric matrix, and so it's a square symmetric matrix. The items on the diagonal are just X by every X by itself, summed over all the data, and we often have another cross product of X's sum double the sample. So, so this, this is a critical matrix, K by K matrix, and the genome need to injure it. So, so this is x by x inverse, which is a k by k matrix multiplied by this product. What's this? This is x by x, transpose, which is n by k, transpose, k by n times n by one. So it's a this product is a k by one. So it's a k by k times k by one, equal to k by one, which is a vector of the power, but it's k by one. So this is the first order condition. So we get the solution for the beta without inverter summations just get an expression. So the assumption one in this notation means that x prime x is invertible. I have to be able to invert this x minus K by K matrix. And it turns out that if I have an intercept, and if I have another column of x, which are all the same, right, no no variability that was the case before, then this matrix will not be inverted, but it will be singular matrix. So, so this x, prime x has to be non singular in order to be able to interact. So, so that's assumption one. So for single variable, assumption one, it just has to be vulnerability. Now for any variable, it's more general. And so when, when is this matrix? When is this matrix non singular? I gave you an example. If you have an intercept and another variable that there's no variability, this matrix will be single.

Speaker 1  1:09:02  
So so this is the first order condition. And this is a equation that memorizes X, prime x, inverse x, prime y. And

Speaker 1  1:09:21  
it includes as a special case the equation that we had before. Here. It's just like giving better intuition for that. I gave you this intuition for the previous lecture in scalar notation. I showed you that beta hat can be written as a covariance divided by it was in previous lectures, if you don't.

Speaker 1  1:09:58  
So there's a similar interpretation here, and if I divide this by n, dividing inside by n is the same as multiplying by n, because that's an inverse line. So putting this n here inside is the same as multiplying by n. So I need to divide this by n, which I do here. So this is just like dividing and multiplying by n. So, so this is like a matrix X, prime x, divided by n. So all these products, everything is divided by n. So it's basically like the variance, covariance matrix of the x's. So we call it here S xx, meaning the diagonal, are the variances of the x's. Of them are the covariances.

Speaker 1  1:10:55  
And that's something that you can calculate from data. And if you have data, you have data now on capital K variable. Well for if you have an intercept, then one of these variances would be zero. Anyhow, we can calculate this. And this is a vector of the covariances between the x's and every x and y, the y is correlated with the x's, and that's given by this matrix. So this is a covariance times the variances, the inverse. So that's analogous to what we had before, covariance divided by variance, the covariance of X and Y divided by the variance of x so this generalization, this matrix it is, it makes notation generalize what we this just define, again, what's in the xx and what's in The xy, so the sub level average of the cos products, this gives you a consistent estimate of the covariances and the variance covariances of the x's divided by n instead of n minus something residuals. So we have a vector of residuals. I'm standing in the whole place I should be standing here with getting close to that. So this was residuals. So this was like the fitted values once I estimate of weight I had multiplied by the matrix. I get something which we call later y hat. And Y minus Y hat is epsilon hat. And from the first order conditions, same as in the previous lecture, the x and epsilon head are because you can see it here. This is the first order condition. You remember this equation? This is the first order condition, the one that we solved for beta. But this first order condition, before we saw it for beta, it just x, transpose times Y minus X beta hat, and it's equal to zero. So the saying that beta head is a risk for an estimator means, means So, means that the residuals have been calculated in such a way that they are orthogonal to every x, because what is, what is X transpose. X is an n by k. X transpose is the K by one. So every row is like every variable. So this is a vector of zero. So this every vector of x, meaning x1, x2, x3, every for from one to k multiplied by by the vector of residuals is always zero, so the residuals are orthogonal to every vector. Initial is an intercept, meaning that they sum up to zero, because if you multiply it by vector, 1111, is the sum of this in scalar notation. This equation is written here, the capital K equation, or this goes for the capital K of these x's. And if we sum it over the sample for every k is equal to zero. So you remember I said that the assumption two is cannot be tested from the data. This is it? This assumption two? Assumption two says that the x's are independent of the epsilon are independent of the X they're orthogonal. So that's, again, means independent. That's where you find independence, extreme, clear cut independence or something. But this is built in. So the least square is designed such that this will be zero. So testament, you cannot apply this. Well, look at the residual and say, Ah, are these are independent of x. Let me multiply them. By definition is equal to zero. So this way, assumption one cannot be tested like directly from the data. I'm sorry. Assumption two. Assumption of one can be tested because we have the matrix x by Max, and we check if it's singular or not. Check it for singularity. What does it mean? Singularity? Singularity means that there is some linear dependence between columns or synaptic matrix, so the square symmetric matrix, so if you find that the rank of a matrix is less than capital K, okay, so that's the length, Yeah, and you can do our whole pop over effect. It. So this also residuals, those issues. Also

Unknown Speaker  1:16:27  
next we talked about the assumption about epsilon and something. So the fitted value on y hat, we have vector y and for given observation to y, N,

Speaker 1  1:16:45  
sub notation, the total value of weight and epsilon, estimated value with the head. And so what's the summary? The beta hat, our estimator is equal to x, prime maximum. So X prime Y, derive that as a d square s, estimator. But let's substitute back y the model. The model is x, beta plus epsilon. So now we get the beta head to be a function of the epsilon, the X on the epsilon. So, so these, this is what you call them, C, the C coefficients. Remember capital C. And so this is now capital C matrix notation, so there's no sum over n. It's like a beta head is equal to the two beta, and it fluctuates depending on this vector of epsilon. Epsilon is a vector observations. So this is a sampling of and the question is, what are the properties? The properties and assumption? We have four assumptions, and as I promised, we are out of time. We give the names from now on, I you know, we will talk about these assumptions, these names. We don't have to get confused like me. I mean, I don't know how many times did they say assumption one and again, assumption two, and so on. So no more. So assumption one means no perfect multicollinearity. So I define multicollinearity next time and but there is no perfect multicollinearity. Multicollinearity means it's later. It's 8.8 is similar. So just so no perfect multicollinearity assumption, assumption two, I already revealed to you this name and explain it strictly exogeneity. Assumption three is a spherical errors, meaning that sigma squared homoscedasticity and no autocorrelation together will be called spherical error. And assumption for all these normality names, and we present this assumption again in terms of make the dissertation next time. Good.

Transcribed by https://otter.ai
