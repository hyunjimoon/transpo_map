Unknown Speaker  2:15  
Now Click here.

Speaker 1  4:44  
Okay, 105, so I don't have my clicker here today, so I will, I will have to point with a cursor on. Screen. Let's think. Let's hope it works. So today we are beginning the sort of the second part of the regression block. In the first part, the first three lectures, I basically covered basics and so. And from today, we begin to talk about, okay, the basics involve these four assumptions and what happened when they are violated. And so we'll consider the violation of each one of these assumption in turn. And so I have a first section here, which is about a review of what we covered so far. So I will go through that very quickly. Excuse me, I guess I should mute the microphone device with again, anyhow, so I will. So in the first part the review, I quickly go through the slides. It just for you. It's a review and and then I will take up one assumption at a time. And as you know, the assumptions, assumption one was no perfect colinearity or multicollinearity. So that's, that's the first assumption. And, and and so I will spend most of the time today on the violation of that assumption. On the assumption, the assumption one, we called it. It's the assumption can be divided into two groups. It was assumption about x the matrix X, that's assumption of no perfect colinearity, and that will be the main focus of today's lecture. And then we have three other assumptions, which are assumption on the epsilon. Remember, the model is y equal x, beta plus epsilon, actually. I mean, Well anyhow, it's right here. And in this model, we have assumption about x, which we are going to talk about today and but we'll talk about other assumptions as well, but the other group of assumption, assumption on epsilon, will be the violation of this assumption epsilon would be primarily the topic in the next three lectures. So introduce you to this notation with vectors and matrices, and we define the ordinary least square estimator, minimizing the sum of square errors. Once we obtain value for beta hat that minimize these values, we get this, the least square estimator, beta hat, equal to x, prime x, inverse x, 1y you remember that one. And then in order to solve the first order condition, to get the solution, we had to make assumption one. And the assumption one is that we know we need to be able to invert X, prime x. So for X, prime x to be non singular, and we need to have assumption one, which is that the rank of the matrix X has to be k, k. You know, the matrix X is n by k, N, number of observation, K, number of columns in x. So the columns have to be linearly independent, meaning there is no exact linear relationship connecting different columns. And I gave you some examples like, suppose that you have two gender variable, and you have male and female, and you have two dummy variables and an intercept, then you know that the intercept is equal to the coefficient of male plus the coefficient of female dummy. So that's that's an example where you have a column of 11111, and then you have two other columns with zeros and one, but they add up to one always. And so that's an exact example of an exact linear dependence. In that case, the rank of this matrix. X will be k minus one. And so therefore the this matrix, X, prime x, cannot be inverted, because the determinant will be equal to zero of x, prime x. So, so that's the assumption that in our burden, and it's the question is, what are the mathematical properties of this estimator? The results of this estimation are two. We get beta hat, but you also get the residual. And the residual, we call them here, epsilon hat and the residual have this property, that they are orthogonal to the X No. Epsilon hat is a vector right is an n by one vector, x, x, matrix X has capital K columns, each column, when multiplied by epsilon, hat, gives you zero. There's a scalar product of each column. That's what x transpose, epsilon hat. X, when you transpose it, it's now k by n, and you multiply it by this vector of n, by one, so every X you know, x1 x2 to x, k, when you multiply that vector by the by the residuals, they are orthogonal. And if the model has an intercept, then there are special results. So if the model has no intercept, we have this orthogonality. It's always true whether or not there is an intercept, but if there is an intercept, we have one column which is all 11111, right? So if you multiply a column of ones by the column of residual. What do you get? You get the sum. So I need my begging, my pointer. But can I, can I point this to you? Can you see there? So what's better? We making my arms or pointing with the other screen with a cursor? You know, I have limited motion with my shoulders, you know? You know, I'm not kidding when that's one of the problem with aging. There's something called in other countries. It's called the shoulder of the 50. Of the 50s, meaning after you are 50 years old, watch your shoulders. Make sure you exercise well. Anyhow, you don't have to worry about it. I cannot say don't see anybody here that is coming even close to this problem. And okay, so, so if there is an intercept, meaning a column of ones, then the residuals sum up to zero. And for the same reason, the regression passes through the averages. So if you look at the cloud of points, y bar and x bar is one point in space, and the line goes through it, and the average fitted value of the fitted value, calculate the average and compare it to the original average. It's the same, of course. So and because of that, and this is just proved here, and, and so in case you have any doubts, that's here. So these are the mathematical properties and, and then, if you calculate r square, there's also a special property listed, not listed here, that you can calculate r square as one minus this ratio SSE divided by SST. Ok. Now, then we have make assumptions, or assumption of x to get existence. And then we began to make assumption on epsilon and assumption two, we now call it strict exogeneity says that the expected value of epsilon given x is equal to zero. So that's the assumption.

Speaker 1  14:31  
And the result is that the least square estimator, beta hat is unbiased. And so this was clearly the critical assumption. Then we make another assumption on the distribution of the epsilon. Here we make the assumption that the mean of the distribution of epsilon is independent of x, or it's equal to zero for any value of x. And here we are in this. Assumption about epsilon. We make these two assumptions put together, we call them spherical error variance. We assume homoscedasticity and no autocorrelation. And when we make this assumption, we get first we get the result that we get is that the variance of beta hat. Now, remember, beta hat is now a vector. It's a k by one vector. And so what does it mean? A variance of a vector. It's a matrix. What's on the matrix? On the horizontal on the diagonal, you have variances the off diagonal covariances. So it's beta is a vector. Beta hat is a vector of k by one, then the variance of a k by one vector is a k by k matrix, which we'll call sometimes a variance covariance matrix, or the variance matrix. It's a matrix, and under assumption two and assumption three. If you make these two assumptions, then the variance of beta hat of the least square estimator is equal to sigma squared x, prime x, inverse. So we know this matrix already from the estimator. And then we need to calculate sigma square which we estimate using something we call S squared. So we get this formula, which is often used by most software to calculate the standard errors, by calculating s and then substituting for sigma squared. And you take the diagonal, take the square root, that's how you get the standard error of beta hat. And there is this efficiency theorem called the known as the Gauss Markov. So if you make assumption 132, and three meaning it exists, no perfect colinearity, no strict exogeneity and spherical error structure, then beta hat is the best meaning, smallest variance, Linear Unbiased Estimator. So there may be some other non linear estimators or estimators that are biased, that may be better, but it's defined a class for which the least square is best. So that's what we've shown so far. We made these two assumptions, first exogeneity And then spherically. Sphericality assumption. From the first one we got on biasness, and then second one gave us a formula that we can easily calculate, and it gave us an efficiency theorem. The first assumption is clearly more critical. That's the word critical here, because how do we calculate standard theories of secondary importance to calculating the mean, of course, and then we make assumption four, which is normality. So if you assume that the epsilon distribution of epsilon conditional on X, remember, for every value of x, we look at the distribution of epsilon and is a normal with mean zero, based on assumption to exogeneity and variance, sigma squared, right? I'm sorry, this is assumption, right, but this is the zero means that epsilon is has expectation, which is all zero for any value of x. And the swing of sigma squared come from the assumption of spherical errors, right? And also the epsilons are independent, right, because the no autocorrelation. And so now we add to assumption two and three, to the exogeneity and to the sphericality. We add the assumption of normality, and as a consequence is, if we make this assumption of normality, then also the sampling distribution, beta hat is distributed norm. So now we know everything there is to know about the sampling distribution. We know it's normal. We know it is centered on the total value, and we know that the spread of this distribution is given by this matrix here, sigma squared, x, y, Max, inverse, and with a normal. That the assumption, the least square estimator is also the maximum likelihood estimator, and therefore the variance of beta hat attains the granular low bound, meaning it is efficient. You cannot do better. Don't have to limit it to a class. It's for any unbiased estimator where it's still limited for an unbiased estimator. So this were the results. This is just a summary of these four assumptions and the results. So now we are going to relax this assumption. And so the way we are going to relax this assumption, let's keep it just to say what the plan so the plan is to start with the assumption, one assumption on x, and I'll spend now some time talking about that. And so that will be the section on perfect multi linearity. And then we will begin to talk about the violation on the assumption on epsilon. And the first one that we'll deal with very quickly is no normality. But before that, in order to proceed after non normality, and including the non normality using we're going to use, rely on the central limits theorem, which are asymptotic theorems, so things that happen when the sample size become large and and so we'll talk about asymptotic properties, and then we can then talk about asymptotic normality, and then asymptotic property that will come up later. So, so when we talk about violation of non normality, we still asymptotically, we can have the beta hat to be normally put, approximately normally distributed, but but then we will also develop other estimator to deal with violation, which will not have the finite sample properties that we talked about so far this property are called finite sample property. In order to prove them, we did not take limits to N was given, was a given sample size, but when we go to talk about some of the violations and how to deal with them, we are going to be able to recover some asymptotic properties. May not be able to get the finite sample properties. So that's why, for many applications, this is, this is really what we need, because we have large samples. But if we don't have large samples, then we have to check that the sample is large enough to give us consistent. We will call we will define consistency again later. Okay, so, so the next few slides, discussion of perfect multicollinearity violation. Okay, so, and in this discussion, we'll make a distinction between perfect versus near multicollinearities. The colinearity can be perfect, but we can have a degree of multicollinearity, but it's not perfect, in other words, and so when we say there is linear dependence between variables, it means that variables are correlated. If there is linear dependence, there is perfect correlation, but the correlation can be not close to one, but not, I'm sorry, yeah, close to one, but not exactly one. So, so that's the idea. So that's the idea of Neo so we'll talk about that. We'll talk about if there is multicollinearity. Well, we know that if there is perfect multicollinearity, we know what happened. Then it's a violation of assumption one, and there is no estimator.

Speaker 1  24:33  
But we worry about situation. When we say there is multicollinearity, we say, okay, it's not perfect multicollinearity. So the estimator exists, but it's very it's a high level of multicollinearity. And the question is, what does it what will happen? So what are the effects? We'll talk about the effects of multicollinearity. Then you say, how do you know that you. Multicollinearity indicators. There is a test which I just mentioned, what it is, and I will not go through the detail. And then finally, we'll talk about ok. So if you have multicollinearity, what can you do? So if any two variables are perfectly correlated, meaning correlation equal to one positive or negative. The rank of x, prime x is less than k x. Prime x is singular, and it's a violation of the assumption of no perfect co linearity, and it's a least square does not exist. Cannot calculate it. And I told you already kind of in passing, then what can you do? I mean, we'll talk about different solutions, because the results is not unique. So sometimes a computer program will say, OK, X, prime x is singular. I cannot invert it. But what they will do is they will still find the minimum, least square solution. But the solution is not unique. And so if you get the solution output, it's not, there is no unique solution. It's like, you know, looking for the summit, like you have a mountain, right? Well, we have a valley. We are looking for a minimum, right? So we're looking for the bottom of the valley, but it's flat, and there are many in this bottom of the valley. There are many possible points that you can find that's that's, if assumption one is violated, that's a consequence, okay, but suppose the question that we primarily are concerned about therefore is, I mean, if there is perfect correlation, we can detect it, we can find it out and And then we will have to deal with it. Somehow. We'll come to it. But usually what happens is variables are highly correlated, not perfectly. So if there is near collinearity, we say there's multicollinearity. It's not a violation. So all the properties that, if assumption 1231, and two hold, we get unbiased results. So if you have multicollinearity, we get unbiased results. If it's perfect colinearity, we don't get results. So what? So it's not a violation multicollinearity. The question is, what does it do? So, so extreme multicollinearity, as long as it's not perfect, does not violate any assumption, but the assumption on x. So ordinarily square, if assumption one, two and three hold is unbiased and we have efficiency, etc. But there is a consequence of great colinearity. Multicollinearity. What happens? The way you notice it is when you look at your output, you get large standard errors, and so you remember the output have the coefficients, beta hat. The next column were these standard errors which were calculated from sigma squared x, prime x, inverse kind of from that formula. And it's the diagonal squared and the diagonal. So anyhow, the standard errors will become very large and and so if you calculate confidence interval on some coefficient, it will be wide. If you look at T ratios, they will be small. Because the standard error or t ratio is an estimated value divided by the standard error, the standard error will be large. And basically, what happens if two variables are highly correlated, the model, the analysis, least square estimation, is has difficulty distinguishing the effect of one variable from another. So in your data, two variables are moving together, so and you observe different differences in y, and you are trying to explain the differences in y as a function of two separate variable, it's hard to distinguish. So, for example, if they are perfectly correlated, we cannot distinguish as well. We cannot separate the effect. One for the other. And so that's, that's the problem with multicollinearity you're trying you have multiple variables and and they you cannot, you cannot estimate well the effect of separate variables. You also can get numerical problems, because you're trying to invert a matrix, X, prime x, which is near singularity, so the determinant of this matrix goes to zero. And that's why you know the value of x, prime x, inverse become very large, and you can have rounding arrow, and of course, you may maybe it may be difficult to invert. So one, one way of dealing with it with a numerical problem is to use higher precision calculations. You another way. If you have large variance, if this is very large, then you need to have better data, or more data. So that's why it's never, it's never a good idea. Good idea not to use data that you have more data, the better. But with a small if you can increase the data set, you can address multicollinearity problem or reduce the effective multicollinearity. How do you know that there is a multicollinearity? Multicollinearity is affecting the results, one kind of indicator that the overall F statistic. You remember I talked about this F statistic, the text that is designed to test the significance of all the slope parameters, and if this f is significant, but the T ratios are all small. So what's going on? I have, I have a model. The model is all the slopes of the parameter together are significantly different from zero, but all the individual parameters have very low t statistics, meaning individually, they are not significantly different from zero. Why? Well, because of multi could be because of multicollinearity. Another way you will notice it is that if you change the specification a little bit, and then of the coefficient change a lot, very unstable model, the another way you notice it is coefficient begin to have very large values positive and negative. And when you look at estimation results and you see very, very large values of coefficients, it's you also can get wrong signs. So if like, if you have two variables, and now they are moving quickly, so they should both supposed to be negative, but because they are very large standard error, the model is unstable, they can get the wrong sign. So I was removed when one other indicator is a high derivative correlation between one or more pairs of independent variable. Now, but here again, there is no measure that says point eight is high. Point nine is high. We don't know. So the problem exists when it affects the results. So when you get very large standard errors, the model is unstable. You get wrong signs, then multicollinearity is a problem, but this is like an early indication if you just calculate the correlation between variable, and you find a high degree of correlation, then you're likely to have a problem. Yes,

Unknown Speaker  34:22  
you will be unbiased. Yeah,

Speaker 1  34:31  
it is unbiased. Unbiasedness means that the sampling distribution is centered on the true value. But what mercurity In reality does? It spreads out the sampling distribution, the standard errors become large. So, so if the sampling distribution is tight, then the. It will fluctuate from sample to sample, but within a fairly narrow range. But now, if you have multicollinearity, if the sampling distribution is very wide, from sample to sample, it can fluctuate a lot, and it can change sign. So it seems like counter intuitive. You said it's unbiased. Why do I get the wrong sign? Why do I get such a huge value? It doesn't make sense for multicollinearity. I mean, just, you know, if you if you have two verbs that are highly correlated, then they'll have large standard errors, and the coefficients will tend to become larger and larger. If one has to be positive, the other ones are supposedly negative. They may remain positive and negative, but they will become much larger or move in the other direction. Everything can go. And so a quick indication that you have a problem, as I said, is you look at the results, and the coefficient looks very large in absolute value.

Speaker 1  36:12  
So if you have a high degree of correlation in your data, it's clearly an indicator. And you also it's usually not printed out, but, but when we calculate the variance covariance matrix, sigma, square, x, y, Max, inverse, you get also covariances. So if you are using the model for forecasting these covariances are important. We usually don't print them out because we don't use them often to interpret what the model says. So we just want to see the standard errors. But if you look at the covariances, you will find, you'll find that there is a high correlation between estimated coefficients, meaning that some coefficients are moving together. There is, I'm not going to spend much time on this slide. I don't think it's very useful, but it just gives you an idea on how people tend to check for if they worry about multicollinearity. So that's one way of kind of checking for nonlinearity in your data. As I said, you know, if you know that there is perfect non collinearity, you have to deal with it. There's no way you cannot deal with it, but you'll get singularity. But if you don't have singularity, the results make sense, then you don't have to look for multicollinearity. But if the results are wrong signs or large standard errors, things are not coefficient too big, then one way of checking what's going on, what's causing the problem is this, variance, inflation factor. Idea is, for every variable in your in the matrix side variable, except for the first one, for the column of 111, you regress it on all the other variables, including an intercept. So you pick up column two of the matrix X and regress it on all the other columns.

Speaker 1  38:42  
And and call the goodness of fit. Call it r square k, because we do it for every k. K goes from two to capital K, and then for so for each k, you can then calculate this. I mean, if this R square is high, this means that there is a maybe a problem. But instead of just looking at this R square and say, Is it high or low, they tend to calculate these type of measure. And we says, OK, if it's if there is no relationship, if this R square is zero, then this variable will be this indicator will be one. So the problem is, when this indicator is large, like 10 or five or 10, when this r square, when r squares get closer to one, when R square is one, then we have a perfect linear relationship somewhere. Then we have perfect colinearity. And so then this, when R square approaches one, this indicator goes to infinity. In. So that's, that's a way, but so basically, I mean this, this particular transformation is not so important. The key here is you take every column, regress it on the other column, and if you find very high correlation that this variable is causing me the problem? Yes, we talked

Unknown Speaker  40:22  
about last lecture that you can also have multi

Speaker 1  40:30  
collinearity, where a combination of columns will this detect Yes, yeah. If there is any relationship, linear relationship between the variable, such regression, will detect it. Yes. If it's only like one

Unknown Speaker  40:42  
variable that has a very few variance

Speaker 1  40:50  
that's not could be, I mean, yeah, but not only that. So that's, that's an example. So suppose that we have one variable that has very I mean, say, no variability, then this R square will for that variable would be one. But my question is,

Speaker 2  41:14  
like, what is like? What is that variable colinear with

Speaker 1  41:19  
itself? No, no, it's colinear with a constant, because if you're regressing one column with a dependent variable, with the independent variable and all the other columns, including the intercept. So if I have a variable that is all constant, then this, this r squared k for that variable would be one,

Speaker 1  41:46  
and the other situation will be suppose that there is like, let's assume that they made a mistake and they coded, I have a dummy variable for male, dummy variable for female, and I Have an intercept, and I suppose that I regress this variable. So this variable regress on an intercept, and the male variable will give me r square equal to one, because I know if the male is one, then this one is zero. And I thought, right, I can predict it exactly. So it's the dependent variable is just one minus the male dummy, perfect linear relation. So if I regress the female dummy is the dependent variable intercept and the male dummy, I get r squared equal to one. So this indicator, this is situation that you can observe, right? You can detect that's a singularity case. But the question is, what happened when we are close to singularity? When do we have a problem? What are we likely to have a problem? Which variable is likely to cause a problem then? So if you regress the female dummy of a regress anything right, we will not be able to estimate the regression. If we take another variable, suppose that they have another price, and I regress it on a constant, and the female dummy and the male dummy, it will be singular, I will not be able to get results. So anyhow, so that's it, just an indicator, I mean. So if you, if you, if you want to say, okay, which variable is causing you observe a problem that which variable is causing it, and you're looking for a mechanical way of doing it. Just run step one. Do step one, run this regression for every explanatory variable as a function of all the other explanatory variable, including the intersect. Is it clear? Not very complicated, right? I

Speaker 1  44:04  
So what can you do about it? So you have multicollinearity, and it's not perfect multicollinearity, and we talked about the consequences, the consequence we have to do something with it. We get results which are not useful because you have very large standard errors. Maybe clearly the coefficients are unreasonable. So we talked about large sample size. Another thing is combining variables with external information. I'll have an example to talk about it. Next one is use factor analysis to reduce the number of variables. And the last one is ropes of finding variables. Yes. Now we'll talk about each one of them. Let's see what I have slides on using external information, but that's it. The other one I'll talk about in this slide. So I have an example of external information, but the first bullet, we talked about, it already. More data, it's better. Second, I live an example that I'll talk about it, and then I'll come back and talk about the other two. And the other two are kind of very dangerous. So, so the other two approach. People do them, but, but I'll explain why you have to be extremely careful when you do them. Okay, so combining with external data. So, so in transportation, that's a transportation example. Most models include time and cost, and most other kind of demand models include a price of some sort, and it's a quality of because that's a trade off. That's a key trade off between price and quality. The question is, how is quality measured? It's, of course, very different for different in transportation. It's clear it's travel time and it's a travel cost and travel time. So the combination of the two is referred to as the generalized cost of travel so, so the generalized cost, you assume it's an equation with beta zero plus beta one, travel cost plus beta two, travel time plus epsilon. And suppose that travel costs and travel time are correlated, highly correlated, because both are functional distance, so you have demand for different trips, long trips, short trips, et cetera. And depending on the D the distance travel time is like just a measure of it depends on speed and and then travel cost depends on some tariff with a constant, and then per mile fee. And so in that case, if this relationship, see if this relationship are exact. So these are exactly linear relationship. So if, whoever prepare the data just use such an equation to calculate travel cost and such an equation to calculate travel time based on some speed, then there's an exact linear relationship between travel time and travel cost, right? So, so it's an example that you may not know. I mean, you get data and you have travel time and travel cost, and they are perfectly correlated. So it's a perfect multi colinearity. So if you try to estimate the model in which you have travel cost as a variable, and travel time is a variable, because travel time is an exact linear relationship, if you substitute it, you can see that basically you travel costs. You can estimate only one coefficient for travel costs. And what's this coefficient of travel cost? It's equal to beta one plus beta two times gamma two. Beta two times gamma one will go into the constant, but that's what you can estimate. You see you cannot. You cannot estimate beta two. You cannot separate beta one from beta two, and you have an exact relationship. But what happened is, if this way the data was calculated was not exactly, that's this, this and this equations were not exactly. I mean, there's some other variable which we affected whether you go during the daytime or at nighttime, whether you go north or go south or go east or west, whatever. So maybe the calculation of travel time and travel cost is a little bit more complicated, but it's inherently a function of distance. Then this relationship will not be exact. Will be approximate. That's what we have. We may have a high degree of correlation between travel time and travel costs. So the question is, how can you use external information? So you can use external information by having knowledge about the ratio, in terms of in generalized cost, we have beta one and beta two. If you calculate the ratio of beta two and beta one, it has an interpretation that's willingness to pay for travel time savings. That's a trade off between two variables. So the. And and the units of this. Beta two over beta one would depends on the units of travel cost and travel time. So if travel cost is in dollars and if travel time is in hours, then beta two over beta one has has units, which is dollars per hour. So that's often referred to as value of time, or willingness to pay, or the trade off between time and cost. So this is inherent to a lot of demand analysis, again, the trade off between price and quality, and it says quality is often measured by different features. I mean, what can the service do for you? What does it offer? And then you can calculate multiple willingness to pay, how much people are willing to pay for more memory or for more speed or for more I don't know for a larger screen, etc, and, and so these willingness to pair calculating by taking ratios of coefficients. And this an example of such. And suppose that you have external data from another study. And so you can say, well, these are people that are going to work, and if they travel longer, they lose wages. So maybe this delta should be some function of the wage rate. And so there is historical data that maybe you'll be able to use to so those are many transportation demand models that have been estimated in the past where travel time and travel costs are highly correlated. And as I will tell you in a moment, taking one of them out very bad. You don't want to do that. I mean, you can see here, excuse me. So in this case, by if we just include travel cost, the coefficient of travel costs that we'll estimate will be beta one plus beta two times gamma two. It will not be the effect of cost. It will be some combined effect. So taking out is not a solution, but if you have external information, and you tell you you know that you have some information about delta this, then you can create a composite variable, tau cross plus delta times double time, and you estimate beta one, which is the effect of cost, and beta one times delta is base your estimate of beta two. So that's how you can use external information, exogenous information, to get estimate for all the parameters in the model. So if you want to estimate this model, we have a high degree of co linearity. The coefficient beta one and beta two will become very large, wrong sign. The model is not useful. I have external information about delta. I say, let me estimate the model, but force the ratio of beta two over beta one to be equal to some constant we specified. And that's that's it. Okay. Now I said, I'll come back to it to two other things. And says, why not to do them? Or how to do them, or when can you do them? And what is the danger here? So some people say, I mean, who is familiar with the technique of factor analysis?

Speaker 1  53:50  
Few of you. So factor analysis is one type of method that is used for data reduction. You have this matrix x that we talked about it right? It has n rows. It has capital K columns. Suppose that k is very large. We have many variables. And the question is, when we put all this matrix into regression, we get results that don't make much sense, and so we use data reduction method, so fractal analysis, or principal component analysis. So instead of factor analysis, I think probably many of you are familiar with principal component analysis. So that's the idea. You take the principal components and put them in a regression. And so, you know, if you have k variables, you can reduce it to a smaller number, and but you leave some some information out, right? Because, you know. You take the principal component for the largest eigenvalues. I mean, I said I will not use eigenvalue in this class, but principal component analysis, if you know about how you do it, you can you get this kind of function, linear function, that combines variables and you create this component, and now you take, like, you know, few fewer variables. Instead of having k, and k is very large, you take the principal components and use the principal components as a variable in the regression. And so the what's the problem? The problem is a you have, you have error in variables, because this, you're using a statistical procedure to get the variables. And so it's a for reasons that are explained later. It will be a violation of assumption two. And so if you violate assumption two, you get biased estimates. So even if you believe that these are the important variables, you get biased results. And the bias effect if you just do it to some of the variables. So so this, using this factor analysis of principal component as a kind of algorithm, I will reduce the results. You know, can be very dangerous, because it violates assumption two, so you get biased results. But you said, suppose that there are variables which are really all trying to represent the same thing I have, you know, I want to measure sort of quality, and I'm not sure exactly how to measure quality, and I have different variables that represent the quality with qualities. I don't have one single variable for quality. Can I combine all these quality related variable into a variable called quality? And the answer is yes, but you have to do it in a way with multiple equations. You have measurement equations. So you add a measurement equation, you add another equation, which says quality is you have a model of quality. Let's say quality is indicated by these different variables, and so you have multiple equations. It's much more complicated. Or you get something which is called the mixture, because if you use a statistical procedure, and you get statistics out of it, and you want to use it in the next statistical procedure. This is statistic. It has a distribution, so you have to take into account the entire distribution. It's an advanced topic, so just doing it as an algorithm, mindless algorithm does just doesn't work. So, but it can be the can be situation where, in addition to your regression model, we call it then a structural equation. There are some additional equation which are meaningful. They are measurement equation. They're saying here is how we measure quality, degree of different indicator of a latent variable called quality. And then there is a structural equation models called which you have multiple equation, and I teach it in 125, meaning not it's not part of one to two. So again, just mindless data reduction to get fewer variables. And then you don't have COVID, COVID linearity, but you don't know what you have. And then finally, that's what often is done. So I have two variables that are correlated. I cannot have large standard errors, etc, the P values will be very large. I don't want the p value to become small. Just drop variables. So what's wrong with that? Who can tell me, yes, you

Unknown Speaker  59:22  
are the some information.

Speaker 2  59:31  
They're highly related. To decide, you know what information is more important in the model.

Speaker 1  59:40  
And then I think you answered the question, but I will state it a little slightly differently. Basically, if a verb belong in the model, and then you take it out, what happens? Goes to the autumn, so so we know it will model will not. Fit as well. We know some fit, but we don't usually worry so much about fit, as I told you before. But more important, if this variable that belongs in the model is now in the error term and it is correlated with a included variable, then you violate assumption too. It's no longer they exogenate The assumption so we violate again, assumption two. So, so these two possibilities that are often used a lot of time are causing a violation of assumption two. They exogenating, and so they shouldn't be done. If you're interested in obtaining unbiased results, you want to identify the effect of a variable. Then if you're just trying to get fit and you look for p values, maybe, but if you're looking for unbiasedness, then you have to always worry about violation of the exogenous assumption. And these two now, when is this not going to violate the exogeneity assumption, when the variable that you delete is not correlated with the variable that you are in, but then you will not delete it. You deleted it because they are highly correlated. So if you delete a variable that should not be in the model, then it's okay, deleting a variable that should not be there. So I mean somebody say, let's try to put a variable like the telephone number, see if it has an explanatory verb. They delete it, of course, has no effect. So that's kind of obvious. Okay, so that's all I wanted to say about coloniality. Yes, only one

Speaker 1  1:01:55  
is visible. I mean, second one is second one is okay too. I mean, you need to get basically, the problem for multicollinearity is meaning that there is a problem with your data. You need more information somewhere, either you find it here or you get more data. So assumption one is, collect more data. Assumption two, try to use some information, knowledge that you have from That's it. That's the only, that's the only solution. Problem method three and four are listed here because people use it, but I'll explain why not to Yes, providing

Speaker 2  1:02:40  
cases, like option to like, providing variables more information, like having like, more nuanced variables, also lead to more heterogeneity that could become a problem with the fit, as you're saying, right? It could get to a point where your data becomes kind of unmodable, right? Like there, the correlations X and Y kind of ceases to exist, and then it becomes not useful. So,

Speaker 1  1:03:16  
like, is that? Is that a reason to be aware of option two, or might be, yeah, I mean, when, basically, if your data, you have what you are describing, I think I understood correctly a model where you have many possible explanatory variables, but you have limited data. So the question is, what do you do then? So basically, collect more data or make assumption based on domain knowledge, that's it, or take one to five, and then I'll teach you. Well, there are methods in multiple equations where you cannot develop. I mean, see, I mean, it's like this sub bullet here is very important. And when you have, when you have a theory that says, I have a latent concept called quality, and it's indicated by different indicators, you develop a measurement model and combine it with structural equation. Okay, so this all I wanted to say about multicollinearity. I'm not much to say about it. It's a disease. You know, when it happens, you need more data on you need some knowledge, external knowledge. It means that in your data, things are moving together, and you cannot separate the effects from different from different explanatory variables. And there is, there are no magical solution. Deleting variables is not is okay only if the variable. Should be deleted, meaning they do not belong in the model, or they just happen to be COVID. I mean, uncorrelated, but that's not, clearly not the problem. So not deleting the deleting variable that should be in the model is wrong. Yes, say it again, yeah.

Unknown Speaker  1:05:30  
I mean this, this is what you do here, right?

Speaker 1  1:05:38  
Yeah, you know, here is you combine them. That's the idea of combining using external knowledge that tells you I can combine cost and travel time using this parameter, Delta. I

Speaker 1  1:06:03  
And well, it depends what is delta. If delta is willingness to pay, right, then the combined variable is now is money. It's dollars, right, or something like that. But if you multiply what happened when you multiply cost by time? I mean, it's not that's not clear what you're doing. So I don't there is no prior knowledge that will allow you to do that.

Speaker 1  1:06:41  
If you multiply variables, it means that the effect of one depends on the effect of the other, right? If I multiply, that's called an interaction. So if I say, I want in my model, not just to include travel time and travel costs separately, I want to include them as a product. And if I include them separately, they're perfectly correlated. But let me include one of them only take travel time out and multiply travel time by cost. Think about the model. What does it mean? The parameter has to have a meaning, right? The elasticities you know, willingness to pay. You calculate and you try to interpret what they are. So if you multiply time and cost, you get now a parameter of cost, and then the parameter of times time cost, because it means that the effect of cost is a function of time, right? So, so maybe if both of them are negative, as as time becomes longer you come, you become more cost sensitive. Does it make sense? And then it says that the effect of time depends on cost, meaning that if cost is equal to zero, then I don't care about time. I'm willing to travel for infinity because I don't have to pay. So it's not true as well. So the model is not valid, so you need to the model has to make sense. It's not modeling is, unfortunately, it's not just machine learning. Is you press the button and you get it. Even machine learning is not like this. You have to think about what you're doing when you define the features in machine learning here, you have to think about what you're doing when you define the explanatory variable and when you write the specification, because specification makes sense, it has to make sense the parameters of interpretation. But trying interactions is OK. So to include, for example, to include travel cost, travel time and the product, travel cost and travel time together may be okay. It may be significant. So the effect of travel time or travel cost is like some parameter, beta one, beta two, plus the effect of the other one. It's a testable hypothesis. This can be tested against data. That's the advantage of testing for non linearities. So you can include, so you say, Oh, this is, this is like, this model is very simple. I just have travel time and travel costs. How about including Travel Time Square or the log of travel time? So very often, people perceive cost and time in some sort of diminishing return kind of effect. And if you change the if you change cost by $1 and the trip cost now $1 and you make it $2 then people are very concerned about it. They double the price. But if the flight cost you $1,000 and you change to 1001 you don't even notice it, but it's the same $1 so you can have all kind of non linear effects that I'm not talking about here. But you have to, when you do specification testing, that's what you have to look for? We'll talk we have a block on practical issues, and this will come up. Okay, so in that block of material, so I'm going to teach you the analytics of regression, then discrete choice, and then we have a block on practical issue that will come back to that. Okay. So what else do I have today? I want to talk about violation of normality and because it's easier, and then we'll talk about asymptotic properties quickly. We don't have much time, but that's okay, we can continue next time. So that's the easiest one to deal with, right? So suppose that we have this assumption for normality. Suppose it doesn't hold. So we what do we lose? We lose the fact that beta hat is not normally distributed. It's not it's not the maximum likelihood estimator. So it's not the best advice estimator we cannot all the test statistics that we talked about, distribution, the F, the chi square, et cetera, t distribution, the z distribution, the normal distribution, all the calculation of p values. And we cannot do that because we don't know the distribution. We don't know the shape of the sampling distribution, so we so if the assumption is failed ons is unbiased, if assumption on x and nothing that assumption on X No, perfect multicollinearity holds right. But if the other assumption on epsilon, meaning exogeneity and sphericality, hold then still, it's unbiased, it's, it's, it's, and I calculate the standard errors the way I calculated it, and I have, but for efficiency, I have a normality. I can rely on asymptotic results so. So that's why it says, If this fails, then it's still unbiased. But for large samples, the distribution will be I can rely on the central limit theorem. The distribution will be approximated by normal for the beta hat right and and it's asymptotically normal, and it is also asymptotically efficient, meaning that the sampling distribution is the best we can get for large enough sample. This means asymptotic efficiency. So so I'll briefly talk about asymptotic properties. So there are three asymptotic properties that we need to talk about. It that the first one is consistency, which is what replaces unbiasedness. So for finite sample, we can, we can show that the sampling distribution is centered on the true value. That's unbiasedness. Consistency means that as the samples become larger and larger, the sampling distribution collapses on the true value. That's consistency. So that's the large sample of the asymptotic property of ordinary least squares that we can get if unbiasedness does not hold, and then asymptotic normality and asymptotic efficiency I just mentioned. So consistency. What does it mean? So we for these squares, we know we've written equation one before. If you take beta hat equal to X, prime x, inverse times x, prime y, substitute Y to be equal to x, beta plus epsilon. You get this first equation there, beta hat equal to beta plus so this is the equation that we use to prove unbiasedness, but we can also use it to prove consistency. And so we can divide here by n, which is the same as multiplying by n, and we can here divide by n because of the inverse. So this is just dividing and multiplying by n. And so now the question is, what happened to beta hat as n goes to infinity, then becomes very large? Yeah, and so there is an operator called pilim. And pili means the limit of the probability of the of a statistic being between within a small interval defined by infinitesimal, small value. And that's, that's a pilim operator. So again, I'm not going to go into the detail I showed you the definition of pilim earlier on, and so, so to prove consistency, you just need to so show that the probability limit of beta hat, and is beta and and so if the probability limit of these two is such that this is the probability limit of This matrix is just the x prime x inverse is converged to some finite matrix Q, and and then if the probability limit of this thing goes to zero, as you'll show in the Next slide. So if we we say that the probability limit of this x prime epsilon goes to zero, as n goes to infinity, then we saying that beta has to is consistent. Now, so what is a this is an assumption. The assumption is that that this product

Speaker 1  1:16:44  
x times epsilon, no epsilon has mean zero, right? And so the this thing is like is a measure of covariance. So it's a measure of the covariance of x and epsilon. So there is a different interpretation of independence and zero covariance. So this, this is a property that says, if x and epsilon are not before we talked about the strict exogeneity, we said the expected value of epsilon given x is equal to zero. So we can replace this. We can take this assumption too, exogenating, and replace it with a different assumption. And the assumption will be that the covariance, excuse me, the covariance of x and epsilon goes to zero as the sample size become larger and larger, meaning that they are not correlated, and if they are not correlated, meaning that there is No like a linear relationship between x and epsilon, and so if this correlation is zero, then we say that beta hat is consistent. So we if we have the assumption of strict exogeneity, meaning that expected value of epsilon given x is equal to zero, then proving consistency doesn't give us anything. But if, for some reason, strict exogeneity is violated, for some reason, now you cannot make it. Make this assumption. If you can make this weaker assumption, that x and epsilon are uncorrelated, you get an asymptotic property of consistency. So it's not so in that case, you cannot say that least squares is unbiased, but least squares is consistent, meaning, when we have large enough sample, we know that the sampling distribution will will collapse on the two value. So that's basically the property of consistency. We talked about it earlier, and and this is just a substitute for the assumption to the strict exogeneity. That's one asymptotic property. The other one is asymptotic normality. So we mentioned it before. So if we know that if the distribution of X is normal, and this is a strict exogeneity. And this means spherical, sphericality, then beta hat is also normally distributed. Is this vector of mean beta the true value? And this is. Variance covariance matrix, but suppose that it's violated. We don't know what's the distribution of epsilon given x and so, but we can apply the central limit theorem, and we know that the distribution of beta hat given x is asymptotically normal here. I did not forget the.so this means distributed as asymptotically it's normal with this. This is a Q inverse, which is Q matrix is given by the what this matrix is converged to, so it's a and, and so think about Q. Inverse is just the inverse of this matrix, X, prime x, divided by n, and it's just a constant. And it's also possible to show that the probability limit of s squared is sigma squared. So this estimator of s squared is a consistent estimator. So in other words, if we don't have normality, we can asymptotically, but we have sphericality, then we can say that we have a consistent then we can say that we have consistent, asymptotically normal. Least queries consistent and asymptotically normal, but we still are making the assumption of sphericality and and then we have asymptotic efficiency beta. It is consistent, asymptotically normal. And then, using the camera lower bound, we have an asymptotic efficiency. So, and, but so we got property, but we did not, we did not deal with the violation of the sphericality. So we talked about so least square, if we are violating assumption two, meaning that, but we have a weaker assumption, saying that x and epsilon are not uncorrelated, and then we can still have we have least squares as consistent and asymptotically normal and asymptotically efficient. So we have all the nice properties, but we still did not violate the sphericality. So next time, in the next two lecture, we will talk about violation of these two assumptions that consists of the sphericality, right? So these two assumptions will deal with, first with heteroskedasticity, violation of homoscedasticity. Then talk about violation of autocorrelation. And then finally, we'll talk about situation where even the weaker assumption, exogeneity assumption, the weaker exogenous assumption, does not hold. Then we have something called endogeneity, and we'll have to deal with it. So there is a summary in this lecture. There are some summary that I'm spherical aerosol, just to review what it is, what is violated. This is just a summary of lectures that are coming. So I don't need to cover this. I still have two minutes. What are you? Collecting your stuff, getting ready to leave. But okay, so I just, these are just summaries of lecture to come, and so, so that's the consequence of transfer legality and endogeneity. This is summaries of lecture to come, so you can review them and and then I can save time by going directly into now the today, what we did cover is the perfect multi collinearity. What to do about it. And near perfect multicollinearity does not violate assumption, include sample size. Violation of assumption towards exaggerating means that least square is biased. But if x and epsilon have zero covariance, kind of asymptotically they have zero covariance, then we still have consistency. But if this does not not available, then we'll have endogeneity, and we'll come back to it in lecture 12, and non spherical error will lose efficiency, right? So if we. Have and and if we have non spodial error, then what is affected is, how do you calculate the variance, covariance the standard error, and it affects efficiency and so, so, meaning that there is no efficiency theorem that we know of, but we can regain it using a technique called generalized least squares. So the basic idea of the next two lecture will be to teach you something called GLS, and that's how to regain efficiency. So in lecture, in the two next lecture, which I wrote, lecture 10 and 11, we are going to different applications of something called GLS, some special version of it, and the general version, and what it will get, it will give us back efficiency when it's violated. And and then, and then, in lecture 12. Say, even, supposedly, even the weaker assumption, exogeneity assumption is violated, then we have endogeneity. What can we do about it? And there is a technique of using instrumental variable, and that will be lecture 12, and that will be the end of the lecture, Lecture block, COVID, good now You can Be Bye. You

Transcribed by https://otter.ai
