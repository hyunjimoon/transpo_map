Unknown Speaker  00:00
Which is titled practical issues, which is

Speaker 1  00:07
relevant to any kind of modeling, modeling, including in this course, you learn about transaction and then the basics of discrete choice. So whether I did model or logic model, or logic model. These particular issues. The particular issue that we talked about last time was, how do you do aggregate forecasting when you have non linear models? So it could be regression as well, if any of the driver handles regression. So when you have I can get forecasting with non linear model, non linear inter variables. Then we talked about different methods of doing it last time. Today, the topic is, the lecture is like mislabeled it's not just about statistical tests, but the topic is how to come up with a good specification, because we whether it is on rational display choice or logic model. In a logic model, you have these V functions, utility functions, you need to specify them. We know it's a function of attributes and characteristics. In a regression model, we have a list of potential explanatory variables, but there are many ways in which they can before it can be specified. We can include all kinds of interaction between variable. We can include non linearity. What tools do we have to come up with a good specification? That's the topic of today's lecture. Next time we're going to talk about something and endogeneity. We talked already about indoctrinating in the case of the many of modern but there are some special results for special vessels that are required for a logic model, the school course model. So we come back from the Genet next time and also talk about something. Okay, so for today's lecture, like I have like a menu of topic I can talk about, initially, hypothesis testing, and to the section two and three are more about the use of statistical hypothesis testing is one of the tools and other tools. And then I'll talk about more specifically about something called non western Dipolog. Let's talk about non linear. How do we introduce nonlinearity? And finally, about prediction tested by you, the P hat, the Y hat, and how we can use it initially to check for outliers, and then using market segments or subsections of data to predicting behavior problems. Okay, so that's so basically statistical test, goodness of faith, non linearity and prediction test meaning, let's look at the predicted value and seeing because they behave forward. So that's that's other tools that I'm going to talk about today, and some of them I already mentioned before in previous lectures. So this is like an opportunity for you to ask me questions. If there are any questions on these topics, so feel free to interrupt me at any time. So first about general introduction, you know, we make a difference between classical statistics and machine learning, and the idea is that there is some up for your knowledge. There is some viewing behind this model. There is some idea about what should be the variables, how the variable should affect the dependent variable. And so when I talk about specification testing, I'm saying that, but there are still alternatives, and how do we evaluate different alternatives? So the assumption is that there is some a priori theoretical knowledge in machine learning that are designed for big data. The idea is to have an automated procedure, like they have a flexible specification, and then let the algorithm decide what the specification is. So the decision of the specification is left to an algorithm. And so the idea is to learn from the data about what about the specification should be. So the key difference between the two is the reliance on some apoyo economic and so I'm talking here about the self, not much machine learning, so in this lecture, but assume that the model structure is given. So are we estimating a logic model or ingestion model, and later on, in this course, we will talk about extension of logic and more complicated, discrete choice model, given about them, given that we specify, and in a discrete choice model, we need to specify the utility function. And later on, we will talk about also display choice. Okay, how do you decide whether the logic model is appropriate with the way we were doing it? We will introduce a more general model. Well, logic is a special case, and then we will be able to test whether the restricted model is acceptable or not, and you know how to do that. So based on Apollo in consideration, we have a sense it's certainly extremely large of possible models, but we have criteria that define whether the model is acceptable or not, whether the model is reasonable. And by that we mean we the a priori consideration. Say, price has to be in the model has to be some measure of quality. The effect of price the demand. If it's a demand model, it should be download, sloppy with price. The effect of y should be negative and should be somewhat of a city makers allow some diminishing return. Now these are probably only consideration about the effect of variables that define whether model is reasonable or acceptable or not. If it doesn't meet this criteria. Doesn't matter where it fits the data. You know, we cannot use it because the prediction will not be transported. And so that's the key. The idea is we first, when we evaluate the model, the first thing we say is this model acceptable, and then we are going to use all kind of other tests, like statistical tests, goodness of faith, prediction test. These are the tools that can allow us select among models which are acceptable, so therefore all the examples that they presented to you, you noticed that. Hope that the first thing that I asked my voice,

Unknown Speaker  08:16
it's not just my voice, it's my eyes. Probably something in here, happening,

Speaker 1  08:32
and so, so, what did I say? Lost my title, so, but that's idea. The idea is still seeing me. Do we look at estimations and say, do they make sense? Do they agree with a priority knowledge? If it's not send them, it doesn't matter. The rest doesn't matter because the model cannot be used for a task worthy analysis. And so. So just good fit does not necessarily mean adequate model, exclusively on goodness of fit to select my competing model. Okay, so first thing you do at the front of the other tests. So the common, the common you do is look at signs relative magnitude. And we also have, in the screenshots model, we have these alternative specific constants, sometimes that you've seen, it's like the attack intercept of utility function, but they're also affected by the label that we give to the alternative. So what is an alternative is, is bus or it's a tablet, it's it's an attribute. The name is an attribute. So sometimes, if we see that the coefficient of this alternative specific odds are explosive, very large, then there may be something suspicious here. Maybe an alternative is dominant. Maybe everybody in the data select the same alternative. Sometimes we look at coefficient ratios because coefficient ratios of interpretation, and especially if you, if you take, if you have a utility function, there's a list of attributes, and you divide all the coefficient by the coefficient of the price. Then if you get this type of variable value of time or willingness to pay, because now the ratio of coefficients have units. It's like dollars per unit, or it's, you know, in the previous lecture, probably you did not notice it, but the the appendix. If you pay attention to the appendices, there was a calculation of what's the value of going from measure to flat, how much people are willing to pay support, the expected cost is the same. What is the can you come up with a monetary value for the difference between flat and measurement, and it can be calculated by using the utility function. It says, because what do I need? How much do I need to increase the cost to make utilities equal? And because the utilities are not equal because I have different constants to the constant and interpretation, because I look at the difference in the constant and it's what's equivalent money that is equivalent to this change in constant. That's a value. So it's a value of flatness, so to speak, in contemporary so these values can be calculated. We often don't have a specific value in mind or an apologies, which will be trustworthy, but we know it cannot be $1,000 $1,000 cannot be the value of flatness. So if you find out that the alternative specific concept for flatness is extremely loud, then something may be wrong in the model, that the model is not specified correctly. So looking at these ratios and saying, do they make sense? I mean, and that's okay, so these are the informatives. That's the most important evaluation that you should do. Anytime you have a model you're looking at the estimation methods. Is that sign, relative magnitude? Are they too big? Other ratio, does the ratio makes sense and and if they do, if you pass all this test, then you can proceed to make statistical tests. Goodness of faith, etc, etc, etc. So it's most important yet I'm going to spend the least amount of time on it. Why? Because it's common sense. It's just this is something. You have to bring it in your mind. Some of the most important things you have to do, those results make sense? Does this model? Is this model? Isn't it? Will I trust the prediction that this model will produce? But there's still alternative the alternative model will satisfy these informal tests, and we can use statistical tests, because you have alternative specification and and then we, we thought, I thought you already had to use hypothesis testing. But they want to make a special point in this lecture about hypothesis testing that they already made before. But I want to maybe strengthen it and so. And the question was, what should be the significance? What should be alpha level? Significance? Should it be 5% should it be 1% should it be fraction of 1% and and, you know, we talked about this, and I told you that the type 1l is calculated this alpha is, is a conditional probability. It's conditional probability on the hypothesis being two and so, so it's a conditional probability, and that's why you know you you have to be careful about using such a small alpha. And you've seen this picture before. I don't want to do it again. And this definition you've seen before, and this is an example of a D test of one one regression. You've seen this before. This before, and this is about like your glacial test, and since we are doing maximum likelihood, then we can apply like your glacial test, which is kind of an optimal test. And you've also seen how to do this test and and that's just an extra information for for some non linear models, sometimes this likely ratio test is difficult to compute. When is it difficult to compute? Because to calculate the likely ratio test, we need to estimate both the restricted and an unrestricted model. So that's the whole idea of the microgrelation test, or an F test that you can do in regression, is you have a more general model, and then you have a hypothesis, which is a restricted model. So this is what we refer to as a nested hypothesis, because the the restricted model is a special case of of the more general model. And so in the classical test like relation test, require to estimate both the restricted and the unrestricted model, and so if it is convenient to estimate the two models, then that's the best has to do. But sometimes there are situations where the restricted model is difficult to estimate, and other situation where the unrestricted model is difficult to estimate. So let me give you an example from discrete choice. And suppose that you want to estimate the logic model. There is a logic model acceptable relative to a more general model. But I don't know. We haven't studied it, how to estimate the more general model. So how chemicals model? Well, turns out that no use an approach for you. It's called the Lagrangian. So there are, of course, each one of them can be associated with equation. I'm not going to lie to you here, but it doesn't matter, because every time you're going to do a test, the test already has been developed, so you already know that equation. So when there is a point of using the world test, for the country multiply test, we will describe the test. So for the time being, this just tells you that there are other options other than the right direction. But anytime the situation that we will encounter will mostly fall into this category, and then it's triggered to do the test in reduction equals the f test, and you'll see the equation for the f statistic. And, okay, so it's just an example of a recumbent, glacial test that comes from, again, the previous lecture on the tablet ownership, you still remember the tablet ownership. If you consider this nupothesis at the price of the same meaning that the tablet ownership does not depend on education, you can this is the example in which we calculate this unrestricted model, individual education level, specific parameters estimated, and that's a regular function. And this is if you assume that implication has no effect, that's a restricted model, then the PI estimate is 1/3 that's the likelihood function. And you can see, remember, likelihood function goes from minus infinity to zero. It's a negative number. So this is less negative. Clearly it fits significantly better. The first thing is, how do we evaluate this difference between these two? We calculate the likelihood that statistic we take the difference multiplied by minus two. It doesn't matter. It's always positive, and we can calculate the p value. And this gets clearly highly, highly significant. It makes sense. You remember, we evaluated the when we looked at this model, you evaluate the monotonicity of this, the fact that it is increasing with education. And that would make sense. And then it not only makes sense, it's also significantly different for the model that says no effective education. Okay, that's but this is a table that they want you to understand and which will say, why, why alpha should not always be a small number. And so the idea is, what kind of test? What should be the outcome of the test? You know, we want to avoid type one and type 2l not two possible error. We have a hypothesis. Type 1l is, we reject my two, and type two is we accept one false. This was the two possible errors. And so if we can have the probability of an error and multiply it by the cost of an error, I can, I can, I can want to minimize this, this is and so let's specify all the elements. So this is type 1l this is type 2l and the probability of a Type 1l we know, we know is the conditional probability H, zero is two, is alpha. So we need to know. In order to calculate the probability for type 1l we need to have these probabilities as H. Zero is two. Of course, it seems judgmental, but that's that's so we denote it. We call it gamma. And so this is the probability of a Type 1l, lambda times alpha, and the cost is denoted by c1, for type 2l if the probability of H zero is two is lambda, then not always one minus lambda. And here we call it beta. You know, we talked about the calculation of beta O, pi is the power of the test. So that's beta. And of course, you call it c2 and if you multiply lambda, alpha, c1 one minus lambda, one minus pi, c2, the sum of this is the expected cost of committing it all. So the question is, how do we select Alpha? So before I told you, be careful. You don't want to select a small alpha, because beta may be large. So, but now we are going to look at three different scenarios. So let's look at three different scenario for the value of this parameter. So the first scenario is called Classical, so that's what usually is being taught. When I post this testing is being taught. And in the classical test, the classical test is scenario is appropriate when lambda is approximately one, meaning that you have a high a priori knowledge that H zero is two. So you're not just testing randomly. Pick up and let me do a test of each variable. I'm testing something which upper young believe it's two. And if there's no big difference between c1 and c2 no dramatic difference, then clearly selecting small alpha makes sense, because you know, lambda is close to one and one minus lambda is close to zero, and the rest, you know, no big differences. We can clearly not worry about type 2l, we worry about problem about type one, zero, and using small alpha makes sense. But remember, otherwise you want to think that they tell you no,

Unknown Speaker  24:22
and so compelling

Speaker 1  24:33
to acceptable to us. Alternative to acceptable to us, but we don't have a strong a priori. Given that the two, the two alternative models, specifically meet our apoyo requirement, then they can handle it equal 2.5 because we are kind of indifferent between the two. We don't know which one is two and and so that's the first give you the first indication that now I need to worry about that too, because I have one minus lambda is significant. And now also, in addition, in specification testing, you're often in a situation where c1 the cost of committing type 1l is a lot less than type 2l why? Because very often the test is, should they include the variable yes or no, right? And so if I if I committed type 1l which means I excluded the variable that should be there, right? So, so we can think in this kind of situation that c1 is less than c2 because c2 you know, accept, when false, we include the variable, okay, whatever small it should be. It shouldn't be in the model, but it is never small. It will have a small coefficient, so it's a loss of efficiency. But if you exclude a variable that should be there, then it should be more significant. So we expect c2 to be greater than c1 and in that case, we definitely don't want to have so that's the case, but we definitely don't want to have a small alpha when we do a test. And finally, there is no test if I apply Yo, I you believe that the new hypothesis is false, then lambda is equal to zero, then you know c1 is equal to zero, then clearly, don't do the test. No matter what is the standard error, no matter what value, what's the p value? If it's calculated for you, doesn't matter. You just don't do the test. You and because it's selecting, essentially, is selecting alpha to be equal to one, like the example would be a price variable. It should be in the model, cannot be in prices of effect. So suppose that the standard are always very loud. It's not significantly different from zero. I cannot take it out with a game model without the price makes no sense. So not this. So that's it. So that's my I mean, I just this three scenario to tell you that, therefore, everything an algorithm says, the p value has to be, you know, less than point 05, that's meaningless. It depends on what was, depends on this parameters lambda and this, the cost of committing their own result can be different, but primarily depends on this one to zero thanks to language. And then when you in the context of this lecture, we are ideal. Okay, any questions? Yes,

Speaker 2  28:20
can you explain again why specification tests means lambda is doesn't mean that we're in different to the different

Speaker 1  28:32
specifications that we have. I have two specifications. Both are acceptable signs the magnitude or the informal test, I'm okay.

Speaker 2  28:46
So like a priori, I have the same confidence in both Yes.

Speaker 1  28:59
So if you had any remaining doubts about this hacking, why I hope that they will have made it clear to you why it shouldn't be done should be now. So the question is, what do I do? I do? Has to be in the model. Significantly it has allowed standard, though, if I calculate the p value is equal to point seven or something like this, P value will be large

Unknown Speaker  30:02
as design from the job.

Speaker 1  30:09
You collect the data. You have the data, the best data that you can find, estimating your model, and you look at your estimation results and then tell them d value has to be less than point 05, but it's something that is equal to point 1.2, what do you do? It is very critical variable. It's transportation. If you don't have cost and time, then what the model is not applicable. What do you

Speaker 3  30:50
do? Yeah, the FR is too high,

Speaker 1  30:55
so you cannot take it out, but still not estimating it works okay as many there's a large meaning, there is a big range of uncertainty. So you have to estimate and then you have to show it in your results, or you have to seek alternative information, source of data, but it's basically you have a great deal of uncertainty that you have to quantify. So the simulation, the simulation external, the micro simulation, idea that I described to you last time, can be applied. You do forecasting can you apply to the parameter? So instead of I talked about sampling something from the distribution of the variables, you can sample from the distribution of the parameter. So if I had a parameter, something distribution which is kind of wide, and so the value the parameter can be anywhere, and you experiment, you sample from this distribution and that piece of focus and see, what does it mean in terms of the forecast? So that's called Monte Carlo, and it's a way of evaluating something, so you have to deal with something. That's what the conclusion is. The conclusion is, what you do, you don't develop science, you don't commit suicide, you don't, I don't know. You're not going up a bank. You just have to realize that there's uncertainty and deal with uncertainty. And this multi car approach is just one way of presenting uncertainty as a way. But but that's related to the previous lecture that I talked about forecasting. I just talked about aggregating and ongoing. In general, the practical issue of focusing prediction is presenting and so, okay, I'm going to I don't know if this is useful, but this also an example from the telephone. So this is the model that you've seen with this alternative, specific constant, and the cost variable is complicated. Cost variable, and then you can connect some additional variable which and then you can do a test, because this an example of the number of users is already in the model, because this cost variable is a historical telephone call, you know, the usage in the past. So the fact that, if there are many users, it's already reflected in the cost. So the question is, is there an additional effect? Yes, you know, maybe useful, maybe not useful. It's not we. There is no like when it comes to beta c, the COVID cost. There is no compromise about that. It has to be in the model. Should the constant be in the model? Yes, of course, because that's a value of flatness and so, so this is a specification which is essential. I mean, I cannot do I cannot just so, I cannot do a test. When I do that, I cannot assume that lambda is equal to one, right, or lambda is equal to 0.5. Is okay. And so, we can use a larger numbers. Use depending on the U value. You can do an accuracy test. You can also just do a T test, because it's just a single in this case, so electrical ratio test is not needed, but you'll get the single cells by using doing a z test or t test or calculating the likelihood ratio. So we have two models. We calculate the difference. There's a test statistic, and the p value for chi square distribution with one degree of freedom is point. So the one, okay, so you can maybe Alpha doesn't have to be 5% but maybe, I don't know, 20% so, but this is clearly 31% okay, maybe, maybe you can take it out. So that's all I want to say about statistical tests. Any question about statistical test something more like particular kind of situation, but for the standard, spend my time reading. Spend my time raised earlier, so any questions or any doubts, just a very

Speaker 4  36:39
rudimentary question, what's the intuition of being rejected at any level of significance? Larger than 31% can

Unknown Speaker  36:51
be rejected

Angie.H Moon  36:55
at any level of significance? Yeah, I was aware that people are calling the significant test, but there are different versions of it, like null hypothesis significant tests and null hypothesis test without significant test. So like, what's the intuition behind the word significance? Significance?

Speaker 1  37:21
Alpha is called significance, and one one Microsoft has called Confidence. That's the

Speaker 4  37:33
word I got very confused you simply, I sense,

Speaker 1  37:45
what significance is often confusing because, yeah, it is statistical significance as opposed to substantive significance. That's why so it is causing confusion, because when it applies to like the cost particle, and so when you talk about significance, we talk about the statistical significance of the difference, say, from zero. And then there is a substantive significance. How much effect does a variable have? What does this value means in terms of value of time, or value of flatness, or any kind of indicator that you're going to drive out of the model, or how does it affect my focus? So to the significance relative to zero, it's not appropriate in this case, but in this case, suppose that they have these two models, and these two models would predictably different, maybe. And the question is, are the prediction significantly different? This is substantive, significance, meaningful. I'm going to make a decision, deciding on the one to do the tariff, and making decisions about the tariff. Two different models could be giving different revenue forecasts. But if the difference in revenue is like less than my longing that substantive significance, and this is statistical significance, what the data give us. Got it?

Speaker 4  39:37
Statistical Significance has no unit right? Yes, yes, no unit less kind of thanks for explaining

Speaker 1  39:50
any other questions. Okay, so next is goodness and faith. So just the bottom line that I have to repeat this all the time, because you cannot use statistical test mindlessly. Let me say all the p value less than point of five, fine, but it is good. No, that's not a problem. That's what I cannot tell you. It depends. It depends on the scenario. If it is specification testing, yes, then you can accept the model that has p value which are greater than 5% 20% okay, goodness of faith. For a nation, we presented our square in our bows, or adjusted our square because the bow, I think neither for God, adjusted same we can do for so this was for regression, meaning for maximum likelihood. We define r square and log of square, okay, and the difference is the deal. But instead of calculating the percentage of explained some square they all, we calculate the percentage of likelihood log likelihood explaining so the bottom is likelihood of the model, the starting value of the benchmark. And this in absolute value, this will be greater than that, and so having this difference is the same as one minus SSE divided by SST. So that's what we have here. So that's called rho square, and it has the same behavior as r square, if we this is the equally likely batch model. So the equally likely model will have the whole square of zero. So meaning rational model, well SSC is equal to SSD. Meanings are not very well, just an intercept, and you'll have or square equal to zero and and the perfect fit, and this will be equal to one, I'm sorry, zero, and this will be equal to zero, and law square will be equal to one. It's a perfect model. Okay, so that's, that's the whole square. And so the problem with o squared, the same problem that we saw with r squared, because it depends on the data. It depends on the dependent variable, plus y, in the case of regression, but now is the probability of I, whatever the choice set is. And so it depends on the choices. It depends on the number of parameters. And the key in the number of parameter is key because if you add more variables, you have more parameters. And low square is monotonic, monotonically increasing in K, same as in regression, and in other words, the same is a regression. You can use R square if you want to compare model and say one is better than the other, then the following condition has to meet. It should be the same data, same dependent variable and the same number of parameters. Then you can if our square or square is larger than you can say it fits better. Remember the same data, same data, the same exact frequency and the same level. One parameter I and and the problem is that the level of parameters, by adding parameter any variable you have, the risk of overfitting. Overfitting means that the Fit improves, but the model doesn't generalize well, meaning it cannot predict well. It predicts well only the data that was used for the escalation. But if you only have all our data, it doesn't predict well. So same as you do with r square, we have adjusted R square or adjusted o square, which is where you penalize from complexity, penalize from adding particles. So if you remember in terms of adjusted R square. It's not one minus SSC divided by SSD, but we divide by degrees of freedom, and, you know, n minus k and n minus one, and we get, well, it's not there's no unique way of doing the different justification for this penalty, for the loss of the grace of freedom, and the one that I'm using, the one that I'm suggesting is, is this one where you'll be penalized by K and it is based on something known as Akaike information criteria, or AIC. What is it? It's just based on some expectations. Actually, what the paper about is maybe. And so there is some theory. Doesn't matter. There's some theory behind this penalty. And so this very simple penalty to implement, and there is just a you penalize the log likelihood by setting it back by k log likelihood units and and so if this means that if you add a variable to the model, if the log likelihood increases by less than one log likelihood unit, then this actually will go down. So it's not monotonic, risk K, it will reach a maximum, and then it will start to decrease. And same as albe square, same idea. And so the advantage of having this adjustableness to fit is that now, in order to say that one model is fit better than the other. I can we can eliminate the last, the last item, so it has to be the same data, the same dependent variable. Then models can be, can be one model with a model variable, another model with fuel variable, and you compare the Fit by using this robot square. So and usually when you have a model with more variable and robot squares, any question about fit. So remember the same things, same data, same dependent element, when you are inducing this role as a goodness of fit, like, what

Unknown Speaker  48:02
are you what are the barriers to compare again, same data, same dependent

Speaker 1  48:12
variable, same number, which means, if he uses another data from you, the two of you cannot compare. Can you say, Oh, I get a square point nine, and you get post point seven. Your model might be a lot better than this one, because it's different data. Oh, if you said, you know, I have one regression model where I have y was like, I don't know, passenger minds on the left hand side, and the other model has no, believe me, go passenger line and you look at the output. So that's why I keep going back to that. I want you to remember his three bullet deal. Sleep, and so same day, the same dependent, if it's interesting, it's the same choices. And then same exact choices, same exact sample of data, and use adjusted R square or adjusted or square. Then you can compare models which have different numbers but same data. So you know, some of them estimate the model. You want to say, Oh, my God, you cannot say that based on this measure, because what are you comparing it to? But if you're comparing a satisfactory goal, say more than the mother with a larger about square fits better than mother B, so mother a better mother B, mother a, Mother B, of the same data, the same dependent variable, maybe different number of particles, then they can use robot square and say, oh, one model fits better. So this is if you use that, then you can say, among my models, because I'm using the same data as independent urban, one model fits better than the other. And so if you have two models and they both satisfies informal tests, both are acceptable, both are trustworthy, you can use them and that you want to make a choice, then this is a good criteria to make. Is

Speaker 5  51:00
this a criteria that is relative? There's, like, no range we should be looking at. It's all based on what the models here.

Speaker 1  51:08
Oh, I mean, if you say so, I have Model A, Model B, the total Mark sentiment and robot square for Model A is point five, one and four. Model B is point 512, instead. So Model B fits better. And the question is, you know, does it make a big difference? If I select Model B is Model A, it could make a difference. So, small differences, my experiences, personal experience, is small differences. Could, could be, could be meaningful, but, but that's what you can say. You can say Model B fits better than Model A. There's no absolute. This one fits well. Depends if you have, like, some target quantities. And if you say, I'm interested in predicting, you know, I have some prediction of revenue or some demand. Statistics, then you can say predicting, well, my smaller because designed to predict demand and demand at the level of daily ridership on this particular line, and I can have some data. And I can think, well, customized. That's when you can use the word predict, word, predicting words something. But this is just like fitting the data. It's not clear. What does it mean? So you can have a model square small numbers, point one and single so it's no quantitative if you see something like recommendation, if R square is point seven, then your model is good. That's stupid. It's really wrong. There's no such thing. But if, if the dependent variable is what you're trying to predict, right? So it's your prediction, right? So then, then you're using the model, you estimate the model and some data, then you're using the model to predict. The kind of example that I mentioned last time when we talk about prediction, then you have a target quantity. And the question was, how well do I predict this target quantity? And then calculating

Unknown Speaker  54:14
something

Speaker 1  54:21
to predict the ridership. I compare the difference, and I can calculate in Server predicting, or you can have a range and say, plus or minus 5% or 10% so to minus 5% or 10% so you can come up with a statistics of prediction error, which is, but it's not r square and it's not all square. That's just in model estimation. Okay, I'm done with goodness of fit. Any any question about goodness and faith and statistics, you know, which is, what do you do if you want to go test the regulation test that I described to you earlier. And after this regulation test is a general model against the model which is a restricted version, but suppose that you have so this is called nested hypothesis. The restricted model is a special case of the alternate model, so the new hypothesis is always some restricted model. So, so that's the case of lesson hypothesis. When h zero is two, the alternate model simplifies to the simple model and but the main is one situation when you want to compare non estimate. So like in the discrete choice example, suppose that I have a logic model and a probability model, two different formulas, the utility function may be the same specification, but they use a different distribution. In a regression case, they derive a model with a lot of variables, and in one of them, it's all linear. In the other case, all non linear expressions different for differently. And so this is called non nested hypothesis. Nasal modeling is a specialty, so I have you a single example just to demonstrate the idea. So the simple example using the screen choice, but it applies to regression as well. And so suppose that the variable enter linearly, like here, or in the log transform here. Now, in this case, it's kind of obvious what you would do. You would create a composite model, right? So, so that's when you can do that. You can create a composite model, and then the composite model, model one is a special case of the composite model, and Model Two is a special case of the composite model. So, so that's basically, excuse me. So this would be what you can do if you can come up with a composite model. And I'll talk about this. Two other tests, something called a J test in the appendix, nobody's using it. Mobile, square, and so that's that's kind of useful, but the best thing to do is to create a composite model. And since I need to take I need to select between these two examples, I can create this composite model. I include the model with include x and log x. Now there may be situations, and that's why I have these two other possibilities. But, if you can create a composite model, then you know what to do. So you have a choice between model one and model two, and you say, Well, let me first create a new model, which is a combination of one and two, and then do the test one twice, test a more composite model against model one and test it against model two. We know how to do that. So what could be the problem? The problem would be that you can reject model one and model two, or extract both model and one two. That's not very important, because you just calculate the p value, right? And you can take the model so you can say, Okay, which one essentially fixed that and but that's it. But you do you do? You can say, or you can see, if I have to choose between the two, again, you can see the one, the one that has the most significant so, so that's that's ideal to composite modeling. And so that's so if you have a situation where, like logit and probit, and you want to compare between the two, I didn't give you an equation of it, but it's really just another formula. Again, there's an integral that I didn't show it to you, but we'll come to it. But turns out that there is a solution. One can have a composite model the combined logit and model. That's a mixture model to talk about later in the semester. So you can click, so that's that's idea. Usually you come up with a composite model, and then you can compare, and then you can test

Speaker 3  1:00:59
previous slide. Just give a concrete definition of what's a restricted model, yeah,

Speaker 1  1:01:05
versus unrestricted, like examples, like the tablet example, the unrestricted I had three pies. The restricted model is only one pi. The selection was that pi one equal to pi two equal to pi three. That was a selection. The null hypothesis, the way we specify is usually in the form of inequality, like this, equality is a selection. The saying, I have a model with all kinds of parameters, and now testing a null hypothesis. The null hypothesis says beta five equal to beta four. That's a restriction, or beta four equal to beta five, equal to one, two restrictions, or pi one equal to pi, two, pi three, two restrictions. You count the level of equality in this sense of equal sign. So the null hypothesis is in an estimate the classical hypothesis testing that we, that I thought you, that you know now about, is in the form of the null hypothesis represents a restricted model. Okay, okay. Now suppose that the composite model is not feasible. For some reason. You still didn't study virtual model, and you can estimate the public model. You can estimate the logic model. And you ask yourself, How do I select between them? And the answer is, you can use all bar square. So that's a place where you can fly. Let's select the one that fits better. And turns out that it's possible to calculate the calculation of an upper bound of type one level. So you can say, I can essentially calculate some sort of a p value for a difference between or square. So if I say, more than two fits better than model one, and I have an example. There's an example in the textbook. I have a field. This an example. So in modern one and model two, different number of parameters, different value for square, and I can calculate the if I select model two and the probability of inter committing a type one zero is very small, even though the difference is essentially point, point, 002, right, which is very small. So that's it. So basically, using robot square has some application. Again, it's same data, same dependent variable, but two different specification. One is not a nested hypothesis of the other. Selecting the better fitting model using robot square is a good procedure to follow. Okay, that's next topic. So well done with testing, done with next is non linear specification. So what tools do we have to test non linear specification? So it's non linear transformation of independent variable if you have discrete or qualitative variable, category, value or categorical variable, you just define double variables. You don't have to do that and be careful. Sometimes people will use it. They give a variable in just 12345, and they use it as a variable. If it is continuous. That's wrong, because 12345, means say, level of indications doesn't mean you see that it's the difference between two and one is the same difference between three and one and two. You cannot use it. So you have to use you have to say, five categories need to have four damn drivers. And that's the way to custom so for categorical variable, you just don't make the usual mistake and don't include it if you have categories and the categories, if you get a discrete variable and it's 1234, 5,001% that's maybe, but, but it's better. In that case you suspect non linearity, just to define categories, and then you can capture any non linearity. And but then sometimes it's an issue, because if you have, for example, one situation that I encounter was supposed people important time which they perform active, taking me from 01, to 24, right? And then, if you put the damage you get coefficient determinant up and down, and this can become, kind of makes no sense, necessarily, the specific hour in which nothing is available. So but in general, you may have the situation where you expect some relationship. So, so if one do 345, level of indication, and so we expect monotonicity. So that's what we looked at. This algorithm. We looked at by one by two, by three the other. Essentially, that's, that's an example of dealing with non linearity with Antigo account, or discrete variable. Now sometimes people take a continuous variable, so that's now the only continuous variable. So when the primary is discrete Antigo account, you know what to do, right? But it's not automatic, because you have to look at it doesn't make sense. And so like in the case when the dimension about time of day, it doesn't make sense that we have this jump. So we said, Okay, let's look for some pattern of the day and have these coefficients following restricted to follow the smallest pattern. So there are solutions for that, and that's for dummy development, for continuous development. The last thing you want to do is represented by people sometimes thinking like age similar age cannot be linear. Typical function for ages COVID, you're on the rise, and then you're on the decline. And so using down the level means that there's a jump. If you say you're the category, take a continuous verb, then your discontinuities, they'll say, becomes a dangerous event, right? You follow me, right? You don't know jump. So therefore taking a continuous burden and replacing it with zombie burden is definitely something to be avoided. Sometimes it's a moment. This is where it was connected in the server, because to make it simple for people to fill out a questionnaire you have, like, say, age categories or income category, then, can I then use this categories of choice? Like, that's the data that you have. Or you may, you may take the midpoint of each category until it is continuous, but fine. I mean using in this particular case and using, okay, but again, don't use a cutting board as a variable. Okay, this is kind of common sense. But so what can you do with continuous variable? Three approaches, which are kind of simple to do, which are available in more software, and it can be specified. And then finally, there is, I talk about this one. You can then interactions, interaction again, it has to be placed on some COE somehow for your consideration, say, the effect of one variable depends on the effect of other variable, and then you can capture interaction. So that's that's beyond what I'm going to talk about today. So let's talk about these three options. The first one is a special case of spines, and it's called piecewise linear specification. And so suppose that I have called it x, and I believe that the effect of x on the model is non linear, but I don't have any particular non linear transformation that they have in mind, and I want to explore them. And so the idea is to select the break points to these all the B, b1, b2, we sleep, and we sleep, right? So what does it? What did we do here in this example? We took a number x and created four variables, x1 x2 x3 and x4 and then we estimate them linear model. So it's a linear utility function or linear regression right now, estimate beta one, beta two, beta three into four. The way these variables were created, they created such that we created a continuous, continuous function changes at this break point, but the function will make the same. So if you will, if you create four between these, from zero to b1, from B to b1, to b2, I'm sorry, from b1, to b2, from b2, to b3, and greater than b3, if you just separated this let them ever get different slopes at different ranges, then at the boundaries they don't connect, they will continue. And so the idea of this piecewise linear specification is to make them continuous at these boundaries. So this is a slope b beta one is a slope of zero to b1 then in b1 the function does not change value, but the slope changes to b2 so to beta two. At b2 the slope changed to b2 all the way to be three, and then v3 I'm sorry, so this is slope less than b1 that's the slope for b1 to be two, that's the slope between v2 to be three, and that's a slope for value of x greater than v3 so again, you select this break point, and then the way it looks looks like this. This was the break points, and you get different slots. And this may give you an idea of what the function may be, yes. So how

Speaker 5  1:13:31
do we can assume about the where the break points are? How can you get the idea about that? Yeah. And also, another point is that if a part of the line, the piece is not significant, then it doesn't mean that within this part there is no

Unknown Speaker  1:13:51
significant point zero. That's okay. So like new, this may be no significant difference. Okay, it's all of the doesn't mean you can take

Speaker 1  1:14:08
it out. You've discovered something. And so I have an example here where he says that negatively, is some complexity. Here, maybe some concavity later on. So it's an exploratory device, because the boycott usually, sometimes the boy knows I will die. I don't know, because this means this is age, then the particular birthday your gradient is changing. Maybe not a value function, but the gradient is changing. And it's it's possible to this is a piecewise linear. You can do it also piece wise quadratic, and make also the gradient continuous, so that the transition smooth at this breakpoint. But I think for the exploding device, this piece wise, linear is the best way possible to quantum form, if the break points, if arbitrary, then you make the file different break points, but eventually maybe find the function form that will fit this. And if, yeah, the break points are kind of arbitrary, but you need to select it such that you have enough data in each interval, right? So that's no recipe that you know how to select the breakpoint, but you have to look at the distribution of your data and select the breakpoint such that you have enough data in each interval. That's can answer your question. So that's piecewise mania. It's a special case of this blind idea. And as I said, you can have piecewise quadratic you can have a higher degree piecewise Angie. Next is box Cox. And so box Cox transformation is because sometimes we are not sure, is it linear, or is it like local electric and box Cox yellow. Box Cox is a transformation. It makes it non linear in the non linear parameter called lambda. So it's beta times the x of lambda. Instead of x, you have a function of x. This is a function of x at this function at the limit, when lambda goes to zero, it becomes log. And when lambda equal to one, then it's linear. So it includes as a special case linear and it includes an algorithmic as a special case. That's why it's kind of popular, because we always include linear, algorithmic both, well, that's a function that can be anywhere between logarithmic and linear. So that's x. So depending on the value of lambda, you get different kind of function from concavity, even complexity, which usually replicates.

Unknown Speaker  1:17:32
So it's available in the social it's available and available. So for addition, if you have linear, linear parameterization, then excuse me, then

Speaker 1  1:17:56
you can die, but you can fix the value of Rn and but if you remember the value from then you estimated. Estimating the value lambda, and you may have different variables with different lambda for different variables. You know this is no linear estimation, but it's available. So anyhow, let's say, and finally, the simplest possible idea. And you don't need the break points. You don't need non linearity. It's just a power series of polynomial, polynomial regression, linearly, square cube, and usually we stop the cube to the power four, whatever different is x1 and x square different. So be careful. Why you have to be careful with the problem that goes up and then it goes down, or it goes down and then it goes up, right? So I change the direction doesn't make usually we expect a sign like direction. So if you ever so, so this is very simple to do, but the bottom line is, then test it, immediately, plot it, whatever you got about it. Nowadays, it's easy to do just plot it and see whether that makes sense. And if it is a parabola, then it may be problem, right? Because So probably you want to stop after three. Don't want to stop after three is okay. Why is after three okay? Because after sleep is three, it can be monotonic. You know, why? No, why? After three, it can be monotonic. Come back, future, lecture, three. It's

Speaker 6  1:20:04
a

Speaker 1  1:20:08
positive. I mean, anyhow transforms. What does lambda signify? Should is number one is linear. Number zero is low. Okay, I did not talk about it in Section seven. I'll talk about it next time. But I'm not going to say much about it. I'm not going to use a slide. I just spoke about it, this mechanism for you to look at. Okay,

Unknown Speaker  1:20:57
Thank you. I

Unknown Speaker  1:21:25
see you. So robust word test is active. Vanessa, did you just compare the robust

Unknown Speaker  1:22:01
but in the robust word test, you are.


Transcribed by https://otter.ai